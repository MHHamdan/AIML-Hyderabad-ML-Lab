{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of Hackathon1 - Voice Food Ordering System.ipynb","provenance":[],"collapsed_sections":[]},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.0"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"SZIubkln0AI2"},"source":["# Advanced Certification in AIML\n","## A Program by IIIT-H and TalentSprint"]},{"cell_type":"code","metadata":{"id":"Alui-GbzEBtW","colab":{"base_uri":"https://localhost:8080/","height":521},"executionInfo":{"status":"ok","timestamp":1605898709343,"user_tz":300,"elapsed":473,"user":{"displayName":"Mohammed Hamdan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh7-0C4n1c_uDVPoywaYa91Jx17qP1YRlmJadqc=s64","userId":"00647759825092258022"}},"outputId":"704b60a9-804f-4b4e-fcfc-a088887a5d72"},"source":["#@title Explanation Video\n","from IPython.display import HTML\n","\n","HTML(\"\"\"<video width=\"800\" height=\"500\" controls>\n","  <source src=\"https://cdn.talentsprint.com/aiml/aiml_2020_b14_hyd/experiment_details_backup/Hackathon_Voice_based.mp4\" type=\"video/mp4\">\n","</video>\n","\"\"\")"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<video width=\"800\" height=\"500\" controls>\n","  <source src=\"https://cdn.talentsprint.com/aiml/aiml_2020_b14_hyd/experiment_details_backup/Hackathon_Voice_based.mp4\" type=\"video/mp4\">\n","</video>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"markdown","metadata":{"id":"4LNbxek40AI4"},"source":["# Hackathon: Voice commands based food ordering system\n","The goal of the hackathon is to train your model on different types of voice data (such as studio data, noisy data and finally on your own team data)"]},{"cell_type":"markdown","metadata":{"id":"Fms7Yt7byCuQ"},"source":["## Grading = 40 Marks"]},{"cell_type":"markdown","metadata":{"id":"zUtVl7cBHlIh"},"source":["### **Objectives:**\n","\n","Stage 0 - Obtain Features from Audio samples using Pre-trained Network\n","\n","Stage 1 (17 Marks) - Train a classifier on Studio data and deploy the model in the server \n","\n","Stage 2 (10 Marks) - Use 'Noisy_data' and 'Studio_data' together, train a classifier on the same, and deploy the model in the server.\n","\n","Stage 3 (13 Marks) - Collect your voice samples (team data) and refine the classifier trained on Studio_data and Noisy_data. Deploy the model in the server."]},{"cell_type":"markdown","metadata":{"id":"BYm_60PiPSsq"},"source":["## Dataset Description"]},{"cell_type":"markdown","metadata":{"id":"qiAu1XJx3lCJ"},"source":["The data contains voice samples of classes - Zero, One, Two, Three, Four, Five, Six, Seven, Eight, Nine, Yes and No. Each class is denoted by a numerical label from 0 to 11.\n","\n","The audio files collected in a Studio dataset contain very few noise samples whereas the audio files collected in a Noisy dataset contain more noise samples. In both datasets, noise and speech are mixed and are in wav format.\n","\n","The audio files recorded for the studio and noisy data are saved with the following naming convention: \n","\n","● Class Representation + user_id + sample_ID (or n + sample_ID)\n","\n","● For example: The voice sample by the user b2, which is “Yes”, is saved as 10_b2_35.wav. Here 35 is sample ID \n","\n","● The ‘10’ that you see above is the label of that sample \n"]},{"cell_type":"code","metadata":{"id":"Kv0xxq_d0Qb_","colab":{"base_uri":"https://localhost:8080/"},"cellView":"form","executionInfo":{"status":"ok","timestamp":1606016556959,"user_tz":300,"elapsed":810042,"user":{"displayName":"Mohammed Hamdan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh7-0C4n1c_uDVPoywaYa91Jx17qP1YRlmJadqc=s64","userId":"00647759825092258022"}},"outputId":"839d0a30-237f-45ba-8176-1231301a7257"},"source":["#@title Please run the setup to download the dataset\n","\n","from IPython import get_ipython\n","ipython = get_ipython()\n","  \n","notebook= \"Hackathon1 - Voice Food Ordering System\" #name of the notebook\n","\n","def setup():\n","    ipython.magic(\"sx wget https://cdn.talentsprint.com/aiml/Experiment_related_data/Week8/Hackathon2/Noisy_data.zip\")\n","    ipython.magic(\"sx wget https://cdn.iiith.talentsprint.com/aiml/Hackathon_data/studio_rev_data.zip\")\n","    ipython.magic(\"sx wget https://cdn.talentsprint.com/aiml/Experiment_related_data/Week8/Hackathon2/net_speech_89.pt\")\n","    ipython.magic(\"sx unzip studio_rev_data.zip\")\n","    ipython.magic(\"sx unzip Noisy_data.zip\")\n","    ipython.magic(\"sx pip install torch torchvision\")\n","    ipython.magic(\"sx pip install librosa\")\n","    print (\"Setup completed successfully\")\n","\n","setup()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Setup completed successfully\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"DqNBNvC25WNV"},"source":["import torch\n","from torch.autograd import Variable\n","import numpy as np\n","import librosa\n","import os\n","import warnings\n","from time import sleep\n","import sys\n","warnings.filterwarnings('ignore')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lEg2PYXrOjnZ"},"source":["## **Stage 0:** Obtain Features from Audio samples using Pre-trained Network\n","---"]},{"cell_type":"markdown","metadata":{"id":"wqMmxLR38vJ3"},"source":["### Pretrained Network for deep features\n","\n","\n","The following function contains code to load a pre-trained network to produce deep features of the audio sample. This network is trained with delta MFCC features of mono channel 8000 bit rate audio sample."]},{"cell_type":"code","metadata":{"id":"NDbuxUiL2zYL"},"source":["def get_network():\n","\n","    net = torch.nn.Sequential()\n","\n","    saved_net = torch.load(\"net_speech_89.pt\").cpu()\n","\n","    for index, module in enumerate(saved_net):\n","        net.add_module(\"layer\"+str(index),module)\n","        if (index+1)%17 == 0 :\n","            break\n","    return net"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dmoIgxTG5ZnF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606020080888,"user_tz":300,"elapsed":470,"user":{"displayName":"Mohammed Hamdan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh7-0C4n1c_uDVPoywaYa91Jx17qP1YRlmJadqc=s64","userId":"00647759825092258022"}},"outputId":"aba8febf-b396-4015-f24d-4a80d5f58fe1"},"source":["get_network()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Sequential(\n","  (layer0): Linear(in_features=900, out_features=800, bias=True)\n","  (layer1): ReLU()\n","  (layer2): Linear(in_features=800, out_features=700, bias=True)\n","  (layer3): ReLU()\n","  (layer4): Linear(in_features=700, out_features=600, bias=True)\n","  (layer5): ReLU()\n","  (layer6): Linear(in_features=600, out_features=500, bias=True)\n","  (layer7): ReLU()\n","  (layer8): Linear(in_features=500, out_features=400, bias=True)\n","  (layer9): ReLU()\n","  (layer10): Linear(in_features=400, out_features=300, bias=True)\n","  (layer11): ReLU()\n","  (layer12): Linear(in_features=300, out_features=200, bias=True)\n","  (layer13): ReLU()\n","  (layer14): Linear(in_features=200, out_features=100, bias=True)\n","  (layer15): ReLU()\n","  (layer16): Linear(in_features=100, out_features=50, bias=True)\n",")"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"sZS1NA1sATEf"},"source":["###Obtaining Features from Audio samples\n","Generate features from an audio sample of '.wav' format\n","* Generate Delta MFCC features of order 1 and 2 \n","* Pass them through the above mentioned deep neural net and obtain deep features.\n","\n","Parameters: filepath (path of audio sample),\n","                       sr (sampling rate, all the samples provided are of 8000 bitrate)\n","         \n","  Caution: Do not change the default parameters"]},{"cell_type":"markdown","metadata":{"id":"P2hPrflGPaPi"},"source":["\"\"\"\n","    extract MFCC feature\n","    :param y: np.ndarray [shape=(n,)], real-valued the input signal (audio time series)\n","    :param sr: sample rate of 'y'\n","    :param size: the length (seconds) of random crop from original audio, default as 3 seconds\n","    :return: MFCC feature\n","    \"\"\""]},{"cell_type":"code","metadata":{"id":"eTtb2zAj5k0-"},"source":["def get_features(filepath, sr=8000, n_mfcc=30, n_mels=128, frames = 15):\n","    \n","    #loads and decodes the audio as a time series y, \n","    #represented as a one-dimensional NumPy floating point array.\n","    #sr is sampling rate of y, the number of samples per second of audio. \n","    y, sr = librosa.load(filepath, sr=sr)\n","\n","    #Short-time Fourier transform (STFT).  Convert the audio file into mel-frequency cepstrum(MFC)\n","    #a representation of the short-term power spectrum of a sound, based on a linear cosine transform \n","    #of a log power spectrum on a nonlinear mel scale of frequency.\n","    # signal in time-frequency domain by computing(DFT)over short overlapping windows.\n","    D = np.abs(librosa.stft(y))**2\n","\n","    #Compute a mel-scaled spectrogram.If a spectrogram inputS is provided, \n","    # then it is mapped directly onto the mel basis by mel_f.dot(S). \n","    # Get the mel-spectrogram features using a precomuted power spectogram,\n","   \n","    S = librosa.feature.melspectrogram(S=D)\n","\n","    #If a time-series input y, sr is provided,then its magnitude spectrogram \n","    # S is first computed, and then mapped onto the mel scale by mel_f.dot(S**power).\n","    S = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=n_mels)\n","\n","    #transform the spectrogram output to a logarithmic scale by transforming\n","    # the amplitude to decibels. While doing so we will also normalize \n","    # the spectrogram so that its maximum represent the 0 dB point.\n","\n","    #* Convert a power spectrogram (amplitude squared) to decibel (dB) units\n","    log_S = librosa.power_to_db(S,ref=np.max)\n","    \n","\n","    # Mel-frequency cepstral coefficients (MFCCs) on log-power Mel spectrogram\n","    features = librosa.feature.mfcc(S=log_S, n_mfcc=n_mfcc)\n","\n","    if features.shape[1] < frames :\n","        features = np.hstack((features, np.zeros((n_mfcc, frames - features.shape[1]))))\n","    elif features.shape[1] > frames:\n","        features = features[:, :frames]\n","\n","    # Find 1st order delta_mfcc\n","    #Compute delta features: local estimate of the derivative of the input data \n","    #along the selected axis. Delta features are computed Savitsky-Golay filtering.\n","    delta1_mfcc = librosa.feature.delta(features, order=1)\n","\n","    # Find 2nd order delta_mfcc\n","    delta2_mfcc = librosa.feature.delta(features, order=2)\n","\n","    features = np.hstack((delta1_mfcc.flatten(), delta2_mfcc.flatten()))\n","    features = features.flatten()[np.newaxis, :]\n","    features = Variable(torch.from_numpy(features)).float()\n","    deep_net = get_network()\n","    deep_features = deep_net(features)\n","    #print(deep_features.shape)\n","    #print(audio_file)\n","    features.flatten()[np.newaxis, :]\n","    return deep_features.data.numpy().flatten()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NhLFY4n6BwIj"},"source":["### All the voice samples needed for training are present across the folders \"Noisy_data\" and \"studio_data\""]},{"cell_type":"code","metadata":{"id":"lMF1AqHZhl1h","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606020361147,"user_tz":300,"elapsed":787,"user":{"displayName":"Mohammed Hamdan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh7-0C4n1c_uDVPoywaYa91Jx17qP1YRlmJadqc=s64","userId":"00647759825092258022"}},"outputId":"973ad536-99ca-49e0-93a2-ae2c10e1f174"},"source":["%ls"],"execution_count":null,"outputs":[{"output_type":"stream","text":["net_speech_89.pt  Noisy_data.zip  \u001b[0m\u001b[01;34mstudio_data\u001b[0m/\n","\u001b[01;34mNoisy_data\u001b[0m/       \u001b[01;34msample_data\u001b[0m/    studio_rev_data.zip\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"a2AAbFp5KORl"},"source":["##**Stage 1**: Train a classifier on the Studio data and Deploy the model in the server\n","\n","---\n"]},{"cell_type":"markdown","metadata":{"id":"SB-LowDuCMUL"},"source":["### a) Extract features of Studio data (5 Marks)\n","\n"," Load 'Studio data' and extract deep features\n","\n"," **Evaluation Criteria:**\n","\n"," * Complete the code in the load_data function\n"," * The function should take path of the folder containing audio samples as input\n"," * It should return features of all the audio samples present in the specified folder into single array (list of lists or 2-d numpy array) and their respective labels should be returned too"]},{"cell_type":"code","metadata":{"id":"6sLoF5EccKyb"},"source":["def load_data(dirname):\n","    features = []  \n","    labels = []\n","    for root, directories, files in os.walk(dirname):\n","        filepath = ''\n","        for  filename in files:\n","            filepath = os.path.join(root, filename)\n","            features.append(get_features(filepath))\n","            r4 = filename.split('_', 1)\n","            r5 = r4[0]\n","            labels.append(int(r5))\n","    return features, labels "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"z5CG-d_yhpHX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1606020767696,"user_tz":300,"elapsed":425,"user":{"displayName":"Mohammed Hamdan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh7-0C4n1c_uDVPoywaYa91Jx17qP1YRlmJadqc=s64","userId":"00647759825092258022"}},"outputId":"a8a04ab9-7f03-4e65-e3f8-54d5e2fa5a86"},"source":["%ls"],"execution_count":null,"outputs":[{"output_type":"stream","text":["net_speech_89.pt  Noisy_data.zip  \u001b[0m\u001b[01;34mstudio_data\u001b[0m/\n","\u001b[01;34mNoisy_data\u001b[0m/       \u001b[01;34msample_data\u001b[0m/    studio_rev_data.zip\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"7673ezpxFEfM"},"source":["Load data from studio_data folder for extracting all features and labels"]},{"cell_type":"code","metadata":{"id":"u5CjrlPVPjNs"},"source":["studio_recorded_features, studio_recorded_labels = load_data('/content/studio_data')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vZ-YrV5UpAzP","executionInfo":{"status":"ok","timestamp":1606030068603,"user_tz":300,"elapsed":1004,"user":{"displayName":"Mohammed Hamdan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh7-0C4n1c_uDVPoywaYa91Jx17qP1YRlmJadqc=s64","userId":"00647759825092258022"}},"outputId":"6a26f88c-18c9-442d-e985-322af3251b02"},"source":["len(studio_recorded_features)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["8178"]},"metadata":{"tags":[]},"execution_count":110}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"I_3spHi4tNCw","executionInfo":{"status":"ok","timestamp":1606030185882,"user_tz":300,"elapsed":582,"user":{"displayName":"Mohammed Hamdan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh7-0C4n1c_uDVPoywaYa91Jx17qP1YRlmJadqc=s64","userId":"00647759825092258022"}},"outputId":"862d749c-579e-4fab-a2c1-a17660aff64a"},"source":["len(studio_recorded_labels)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["8178"]},"metadata":{"tags":[]},"execution_count":115}]},{"cell_type":"code","metadata":{"id":"sPJ1VksHpXeu"},"source":["# convert the list to numpy array\n","studio_recorded_features = np.array(studio_recorded_features)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"C60nhsTCtAzi","executionInfo":{"status":"ok","timestamp":1606047140413,"user_tz":300,"elapsed":457,"user":{"displayName":"Mohammed Hamdan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh7-0C4n1c_uDVPoywaYa91Jx17qP1YRlmJadqc=s64","userId":"00647759825092258022"}},"outputId":"22500c35-af53-49cf-a2aa-362a2fc767d6"},"source":["len(studio_recorded_features)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["8178"]},"metadata":{"tags":[]},"execution_count":140}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ogyDcNMtlejl","executionInfo":{"status":"ok","timestamp":1606042400383,"user_tz":300,"elapsed":634,"user":{"displayName":"Mohammed Hamdan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh7-0C4n1c_uDVPoywaYa91Jx17qP1YRlmJadqc=s64","userId":"00647759825092258022"}},"outputId":"5c543174-c523-43c4-8e88-f6afc27a4d1e"},"source":["studio_recorded_features.shape"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(8178, 50)"]},"metadata":{"tags":[]},"execution_count":132}]},{"cell_type":"code","metadata":{"id":"qfIf5uvHrTp7"},"source":["X = studio_recorded_features\n","y = studio_recorded_labels"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BGq6XpvhFynP"},"source":["### b) Train and classify on the studio_data (5 Marks)\n","\n","The goal here is to train and classify your model on voice samples collected in studio data\n","\n","**Evaluation Criteria:**\n","* Train the classifier\n","* Expected accuracy is above 85%"]},{"cell_type":"code","metadata":{"id":"VU5hdERsFw5o"},"source":["# YOUR CODE HERE\n","# Splitting the dataset into the Training set and Test set\n","from sklearn.model_selection import train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"v8FTpEXarx1X"},"source":["# Feature Scaling\n","from sklearn.preprocessing import StandardScaler\n","sc = StandardScaler()\n","X_train = sc.fit_transform(X_train)\n","X_test = sc.transform(X_test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1tBu_-ivr3vN","executionInfo":{"status":"ok","timestamp":1606047161430,"user_tz":300,"elapsed":1779,"user":{"displayName":"Mohammed Hamdan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh7-0C4n1c_uDVPoywaYa91Jx17qP1YRlmJadqc=s64","userId":"00647759825092258022"}},"outputId":"67096794-8f5e-4d27-bd59-db199e3fc070"},"source":["#Training the Decision Tree Classification model on the Training set\n","from sklearn.tree import DecisionTreeClassifier\n","classifier = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\n","classifier.fit(X_train, y_train)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='entropy',\n","                       max_depth=None, max_features=None, max_leaf_nodes=None,\n","                       min_impurity_decrease=0.0, min_impurity_split=None,\n","                       min_samples_leaf=1, min_samples_split=2,\n","                       min_weight_fraction_leaf=0.0, presort='deprecated',\n","                       random_state=0, splitter='best')"]},"metadata":{"tags":[]},"execution_count":142}]},{"cell_type":"code","metadata":{"id":"utOsUbbWsTP1"},"source":["# Predicting the Test set results\n","y_pred = classifier.predict(X_test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JgLGJyLPscBF","executionInfo":{"status":"ok","timestamp":1606047167612,"user_tz":300,"elapsed":710,"user":{"displayName":"Mohammed Hamdan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh7-0C4n1c_uDVPoywaYa91Jx17qP1YRlmJadqc=s64","userId":"00647759825092258022"}},"outputId":"ae5f0916-9aac-40bc-e82d-047f3dbc423f"},"source":["#Making the Confusion Matrix\n","from sklearn.metrics import confusion_matrix, accuracy_score\n","cm = confusion_matrix(y_test, y_pred)\n","print(cm)\n","accuracy_score(y_test, y_pred)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[[149   1   2   0   0   0   1   2   1   1   3   0]\n"," [  0 144   0   0   1   3   0   6   1   2   2   2]\n"," [  3   0 146   0   1   0   0   1   0   1   2   5]\n"," [  1   0   1 155   0   2   3   0   2   0   0   0]\n"," [  0   0   1   0 163   0   0   0   0   0   0   1]\n"," [  0   4   0   0   0 171   0   3   0   6   0   0]\n"," [  3   2   0   3   0   1 149   5  14   5  13   1]\n"," [  1   5   0   0   0   5   3 160   0   5   1   1]\n"," [  0   1   0   3   0   0  15   2 141   3   1   0]\n"," [  0   1   0   0   0   6   2   4   0 130   0   1]\n"," [  3   4   0   2   0   1   9   6   5   3 168   1]\n"," [  2   3   3   2   2   1   0   0   0   1   1 148]]\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["0.8919315403422983"]},"metadata":{"tags":[]},"execution_count":144}]},{"cell_type":"markdown","metadata":{"id":"qqIP3Y17byDq"},"source":["### c) Save and download your model (2 Marks)\n","\n","Save your trained model\n","\n","**Hint:** You can use joblib package to save the model."]},{"cell_type":"code","metadata":{"id":"ut8aQN5_G7bx"},"source":["# YOUR CODE HERE to save the trained model\n","# Save\n","from joblib import dump\n","dump(classifier, 'studio_data_DT.sav') \n","\n","# Load\n","from joblib import load\n","#clf = load('studio_data_DT.joblib') "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jsCHKXubHAJB"},"source":["Download your trained model using the code below\n","* Give the path of model file to download through the browser"]},{"cell_type":"code","metadata":{"id":"BDmWXfPaHJZG","colab":{"base_uri":"https://localhost:8080/","height":17},"executionInfo":{"status":"ok","timestamp":1606047188850,"user_tz":300,"elapsed":435,"user":{"displayName":"Mohammed Hamdan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh7-0C4n1c_uDVPoywaYa91Jx17qP1YRlmJadqc=s64","userId":"00647759825092258022"}},"outputId":"c604a580-a6ed-4eac-97da-c1f9b4f1198a"},"source":["from google.colab import files\n","files.download('/content/studio_data_DT.sav')"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"application/javascript":["download(\"download_893821c9-d57e-41df-9c21-13b860f277d2\", \"studio_data_DT.sav\", 145984)"],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"Hl3Ins3vSTQx"},"source":["### d) Deploy your model in the server (5 Marks).\n","\n","(This can be done on the day of the Hackathon once the login username and password provided by the mentors in the lab) \n","\n","Deploy your model on the server, check the hackathon document (2-Server Access and File transfer For Voice based food ordering) for details. \n","\n","To order food in user interface, go through the document (3-Hackathon 1 Application Interface Documentation) for details.\n","\n","\n","**Evaluation Criteria:**\n","\n","There are two stages in the food ordering application\n","        \n","* Ordering Item\n","* Providing the number of servings\n","    \n","If both the stages are cleared with correct predictions you will get\n","complete marks\n","Otherwise, no marks will be awarded\n","\n","\n","#### Now deploy the model trained on studio_data in the server to order food correctly. \n"]},{"cell_type":"markdown","metadata":{"id":"8VIgzdOyKU4s"},"source":["## **Stage 2**: Use 'Noisy_data' and 'Studio_data' together, train a Classifier on the same and deploy the model in the server \n","\n","---"]},{"cell_type":"markdown","metadata":{"id":"JLVpzTGycZS-"},"source":["### a) Extract features and classify the model (3 Marks)\n","\n","The goal here is to train your model on voice samples collected in both noisy and studio data\n","\n","**Evaluation Criteria:**\n","* Load 'Noisy_data' and extract features\n","* Combine noisy features with the studio features\n","* Train the classifier\n","\n","\n","Load data from Noisy_data folder for extracting all features and labels"]},{"cell_type":"code","metadata":{"id":"5LLejdkbCat2"},"source":["noisy_data, noisy_data_labels = load_data('Noisy_data')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"clnULqypfco9"},"source":["# Combine the features of Studio and Noisy data\n","# YOUR CODE HERE"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9LoGhipofqjY"},"source":["Train a classifier on the features obtained from noisy_data and studio_data"]},{"cell_type":"code","metadata":{"id":"GN6IymdFfisN"},"source":["# YOUR CODE HERE"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"E2jfdJigrTkr"},"source":["### b) Save and download your model (2 Marks)\n","Save your trained model\n","\n","**Hint:** You can use joblib package to save the model."]},{"cell_type":"code","metadata":{"id":"GJBkPlNUrhbz"},"source":["# YOUR CODE HERE"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7T3v5XQbron7"},"source":["Download your trained model using the code below\n","* Give the path of model file to download through the browser"]},{"cell_type":"code","metadata":{"id":"sNhSG65BrqAm"},"source":["from google.colab import files\n","files.download('<model_file_path>')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6K_1kK3cm09z"},"source":["### c) Deploy your model in the server (5 Marks).\n","\n","(This can be done on the day of the Hackathon once the login username and password given by the mentors in the lab) \n","\n","Deploy your model on the server, check the hackathon document (2-Server Access and File transfer For Voice based food ordering) for details.\n","\n","To order food in user interface, go through the document (3-Hackathon 1 Application Interface Documentation) for details.\n","\n","**Evaluation Criteria:**\n","\n","There are two stages in the food ordering application\n","        \n","* Ordering Item\n","* Providing the number of servings\n","    \n","If both the stages are cleared with correct predictions you will get\n","complete marks. \n","Otherwise, no marks will be awarded\n","\n","\n","#### Now deploy the model trained on studio_data and noisy_data in the server to order food correctly. "]},{"cell_type":"markdown","metadata":{"id":"GIXBC0aYKhKX"},"source":["## **Stage 3:** Collect your voice samples and refine the classifier trained on studio_data and Noisy_data\n","---"]},{"cell_type":"markdown","metadata":{"id":"pmSoJN11_kMR"},"source":["### a) Collect your Team Voice Samples and extract features (5 Marks)\n","\n","(This can be done on the day of the Hackathon once the login username and password is given by mentors in the lab)\n","\n","* In order to collect the team data, ensure the server is active (Refer document: 2-Server Access and File transfer For Voice based food ordering)\n","\n","* Refer document \"3-Hackathon_1 Application Interface Documentation\" for collecting your team voice samples. These will get stored in your server\n","\n","**Evaluation Criteria:**\n","* Load 'Team_data' and extract features\n","* Combine features of team data with the extracted features of studio and noisy data"]},{"cell_type":"code","metadata":{"id":"nv3I24flWlLq"},"source":["!mkdir team_data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gB_bSllKWJ5U"},"source":["# Replace <YOUR_GROUP_ID> with your Username given in the lab\n","!wget -r -A .wav https://aiml-sandbox1.talentsprint.com/audio_recorder/<YOUR_GROUP_ID>/team_data/ -nH --cut-dirs=100  -P ./team_data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2G17PFkgI02J"},"source":["# YOUR CODE HERE to Load data from teamdata folder for extracting all features and labels"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"G_sh6DCbJKhg"},"source":["# Combine the features of all voice samples (studio_data, noisy_data and teamdata)\n","# YOUR CODE HERE"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pSE9E9mDF7Az"},"source":["### b) Classify and download the model (3 Marks)\n","\n","The goal here is to train your model on all voice samples collected in noisy, studio and team data\n"]},{"cell_type":"code","metadata":{"id":"D1EtSEwDG-q4"},"source":["# YOUR CODE HERE for refining your classifier"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eQYzkuNHwhGs"},"source":["Save your trained model\n","\n","**Hint:** You can use joblib package to save the model"]},{"cell_type":"code","metadata":{"id":"Egk4hYGVw1-z"},"source":["# YOUR CODE HERE"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-DbUZ40Gw38-"},"source":["Download your trained model using the code below\n","* Give the path of model file to download through the browser"]},{"cell_type":"code","metadata":{"id":"lEtx7MTlw5Ap"},"source":["from google.colab import files\n","files.download('<model_file_path>')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pAiMx-auHCw9"},"source":["### c) Deploy your model in the server (5 Marks).\n","\n","(This can be done on the day of the Hackathon once the login username and password given by the mentors in the lab) \n","\n","Deploy your model on the server, check the hackathon document (2-Server Access and File transfer For Voice based food ordering) for details.\n","\n","To order food in user interface, go through the document (3-Hackathon 1 Application Interface Documentation) for details.\n","\n","\n","**Evaluation Criteria:**\n","\n","There are two stages in the food ordering application\n","        \n","* Ordering Item\n","* Providing the number of servings\n","    \n","If both the stages are cleared with correct predictions you will get\n","complete marks\n","Otherwise, no marks will be awarded\n","\n","\n","#### Now deploy the model trained on studio_data in the server to order food correctly. "]}]}