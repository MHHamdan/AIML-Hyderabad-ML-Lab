{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Hackathon3b_Expression_Recognition.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.0"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"SZIubkln0AI2"},"source":["# Advanced Certification in AIML\n","## A Program by IIIT-H and TalentSprint"]},{"cell_type":"markdown","metadata":{"id":"a8pCC9bE2blm"},"source":["Automated facial expression recognition provides an objective assessment of emotions. Human based assessment of emotions has many limitations and biases and automated facial expression technology has been found to deliver a better level of insight into behavior patterns. Emotion detection from facial expressions using AI is useful in automatically measuring consumers’ engagement with their content and brands, audience engagement for advertisements, customer satisfaction in the retail sector, psychological analyses, law enforcement etc."]},{"cell_type":"code","metadata":{"id":"RURW82wHOKnb","cellView":"form","colab":{"base_uri":"https://localhost:8080/","height":321},"executionInfo":{"status":"ok","timestamp":1613802796360,"user_tz":300,"elapsed":292,"user":{"displayName":"Hemanth Subramanyam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjBOXNDrWEHEHb9uFYixP7uXN29lrPcZEC9KjYL3KU=s64","userId":"09323849053732298462"}},"outputId":"bacbe60b-6f79-412f-90bc-1aea42c4b29b"},"source":["#@title Explanation Video\n","from IPython.display import HTML\n","\n","HTML(\"\"\"<video width=\"500\" height=\"300\" controls>\n","  <source src=\"https://cdn.iiith.talentsprint.com/aiml/Experiment_related_data/Hackathon3b_expression_recognition.mp4\" type=\"video/mp4\">\n","</video>\n","\"\"\")"],"execution_count":1,"outputs":[{"output_type":"execute_result","data":{"text/html":["<video width=\"500\" height=\"300\" controls>\n","  <source src=\"https://cdn.iiith.talentsprint.com/aiml/Experiment_related_data/Hackathon3b_expression_recognition.mp4\" type=\"video/mp4\">\n","</video>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]},"execution_count":1}]},{"cell_type":"markdown","metadata":{"id":"CrBFZZit7ES0"},"source":["**Objectives:** \n","\n","**Stage 4 (15 Marks):** Train a CNN Model and perform Expression Recognition in the EFR Mobile App.\n","\n","**Stage 5 (5 Marks):** Test for Anti-Face Spoofing on the EFR Mobile App."]},{"cell_type":"markdown","metadata":{"id":"BGq6XpvhFynP"},"source":["##**Stage 4 (15 Marks)**\n","\n","**(i) Train a CNN Model for Expression Recognition on given Expression data  \n","(ii) Deploy the Model and Perform Expression Recognition on Team Data through the EFR Mobile App**\n","\n","\n","---\n","\n","\n","* Define and train a CNN for expression recognition for the data under folder \"Expression_data\" which segregated on expression basis.\n","* Collect your team data using EFR application and test your model on the same and optimize the CNN architecture for predicting the respective labels of the images.\n","* Save and Download the trained expression model and upload them in the ftp server (refer to [Filezilla Installation and Configuration document](https://drive.google.com/file/d/1hnMXcwpCwAz94ljAhtsJdQSfe-JaOD5K/view?usp=sharing)).\n","\n","* Update the **“exp_recognition.py”** file in the server. Open the files in the terminal (Command prompt) and provide the code for predicting the expression on the face (Note: To define the architecture of your trained model, you'll need to define it in the file **\"exp_recognition_model.py\"**). \n","\n","* Test your model on the mobile app for Expression Recognition and Sequence Expression. Your team can also see your results in your terminal.\n","\n","\n","* Grading Scheme:\n","> * Expression Recognition (12M): If the functionality is returning expression class correctly for the face using the mobile app’s “Expression Recognition” functionality\n","> * Sequence Expression (3M): Get three consecutive correct Expressions using the mobile app’s “Sequence Expressions” functionality"]},{"cell_type":"markdown","metadata":{"id":"3e0e3sFh0JZJ"},"source":["**Download the dataset**"]},{"cell_type":"code","metadata":{"cellView":"form","id":"Kv0xxq_d0Qb_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1613807627340,"user_tz":300,"elapsed":152250,"user":{"displayName":"Hemanth Subramanyam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjBOXNDrWEHEHb9uFYixP7uXN29lrPcZEC9KjYL3KU=s64","userId":"09323849053732298462"}},"outputId":"e8a83acc-0dbb-4ec5-a743-a1e43bb89092"},"source":["#@title Run this cell to download the dataset\n","\n","from IPython import get_ipython\n","ipython = get_ipython()\n","  \n","notebook=\"M3_Hackathon\" #name of the notebook\n","\n","def setup():\n","#  ipython.magic(\"sx pip3 install torch\")\n","    ipython.magic(\"sx wget wget https://cdn.talentsprint.com/aiml/Experiment_related_data/Expression_data.zip\")\n","    \n","    ipython.magic(\"sx unzip Expression_data.zip\")\n","    \n","    ipython.magic(\"sx pip install torch==1.0.1 -f https://download.pytorch.org/whl/cu100/stable\")\n","    ipython.magic(\"sx pip install torchvision==0.2.1\")\n","    ipython.magic(\"sx pip install opencv-python\")\n","    print (\"Setup completed successfully\")\n","    return\n","setup()"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Setup completed successfully\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"NhLFY4n6BwIj"},"source":["**Dataset attributes:**\n","\n","During the setup you have downloaded the Expression data:\n","\n","* **Expression_data**: In this folder, the images are segregrated in terms of Expression\n","> * Expressions available: ANGER, DISGUST, FEAR, HAPPINESS, NEUTRAL, SADNESS, SURPRISE\n","> * Each class is organised as one folder\n","> * There are ~18000 total images in the training data and ~4500 total images in the testing data"]},{"cell_type":"code","metadata":{"id":"9llRIwuyNxMZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1613807627489,"user_tz":300,"elapsed":74739,"user":{"displayName":"Hemanth Subramanyam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjBOXNDrWEHEHb9uFYixP7uXN29lrPcZEC9KjYL3KU=s64","userId":"09323849053732298462"}},"outputId":"de308c22-cd4e-4441-cd2d-bb229880e11a"},"source":["%ls"],"execution_count":2,"outputs":[{"output_type":"stream","text":["\u001b[0m\u001b[01;34mExpression_data\u001b[0m/  Expression_data.zip  \u001b[01;34m__MACOSX\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"RC9IMSrJGTjK"},"source":["**Imports: All the imports are defined here**\n","\n","We are installing the following specific package versions -> torch 1.0.1, torchvision 0.2.1 and PIL 5.3.0 to maintain compatibility with the server \n","\n","* Firstly uninstall and downgrade the current PIL version. In the next cell, you will see a button \"Restart Runtime\" button appear below. \n","* Click on it and select 'Yes' to restart runtime and reset the PIL package. \n","* **DO NOT** go to the notebook's **RUNTIME  -> RESTART RUNTIME**. This will restart all packages and you will need to repeat all the steps from beginning.\n","\n","\n","* Simply continue with the next code cell"]},{"cell_type":"markdown","metadata":{"id":"IYzA3K2W8rSK"},"source":["PIL (Pillow) is the Python Image Library. Used to cut and resize images, or do simple manipulation.\n"]},{"cell_type":"code","metadata":{"id":"r9W3RfAdY3gt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1613807632283,"user_tz":300,"elapsed":964,"user":{"displayName":"Hemanth Subramanyam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjBOXNDrWEHEHb9uFYixP7uXN29lrPcZEC9KjYL3KU=s64","userId":"09323849053732298462"}},"outputId":"133ee5c6-2905-4476-88c1-4ec7a77fbda3"},"source":["!pip uninstall -y Pillow"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Uninstalling Pillow-7.0.0:\n","  Successfully uninstalled Pillow-7.0.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"k4rm-0TjGTNq","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1613807648405,"user_tz":300,"elapsed":2353,"user":{"displayName":"Hemanth Subramanyam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjBOXNDrWEHEHb9uFYixP7uXN29lrPcZEC9KjYL3KU=s64","userId":"09323849053732298462"}},"outputId":"ec767e29-6777-4e17-a03b-0d1de4da02c8"},"source":["# IGNORE ERROR. Click on Restart Runtime button and slect 'Yes' if prompts. Then proceed with the next code cell.\n","!pip install Pillow==5.3.0"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: Pillow==5.3.0 in /usr/local/lib/python3.6/dist-packages (5.3.0)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Nw2avdJbG0_4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1613807649417,"user_tz":300,"elapsed":248,"user":{"displayName":"Hemanth Subramanyam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjBOXNDrWEHEHb9uFYixP7uXN29lrPcZEC9KjYL3KU=s64","userId":"09323849053732298462"}},"outputId":"ebd78944-1801-4a4e-b17a-4f1625f330d1"},"source":["# When you run this, it should give you pil version = 5.3.0\n","import PIL\n","print(PIL.__version__)"],"execution_count":2,"outputs":[{"output_type":"stream","text":["5.3.0\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Lj0Yjxe8G46e","executionInfo":{"status":"ok","timestamp":1613807653073,"user_tz":300,"elapsed":608,"user":{"displayName":"Hemanth Subramanyam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjBOXNDrWEHEHb9uFYixP7uXN29lrPcZEC9KjYL3KU=s64","userId":"09323849053732298462"}}},"source":["import torchvision\n","import torchvision.datasets as dset\n","import torchvision.transforms as transforms\n","from torch.utils.data import DataLoader,Dataset\n","import matplotlib.pyplot as plt\n","import torchvision.utils\n","import numpy as np\n","import random\n","from PIL import Image\n","import torch\n","from torch.autograd import Variable\n","import PIL.ImageOps    \n","import torch.nn as nn\n","from torch import optim\n","import torch.nn.functional as F\n","import os\n","import warnings\n","from time import sleep\n","import sys\n","warnings.filterwarnings('ignore')"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QWe3lAlQk5U0"},"source":["For the following step, to obtain hints on building a CNN model for face expression, you may refer to this [article](https://drive.google.com/open?id=1P2rpaWW3tOtGGnw4dvtdZ4hjoc8iDNst)"]},{"cell_type":"markdown","metadata":{"id":"c86_KxLjmLT7"},"source":["**Define and train a CNN model for expression recognition**"]},{"cell_type":"code","metadata":{"id":"Bs38PWmmVg7k","executionInfo":{"status":"ok","timestamp":1613807660199,"user_tz":300,"elapsed":305,"user":{"displayName":"Hemanth Subramanyam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjBOXNDrWEHEHb9uFYixP7uXN29lrPcZEC9KjYL3KU=s64","userId":"09323849053732298462"}}},"source":["transform_train = transforms.Compose([transforms.Resize((224, 224)),\n","                                transforms.RandomHorizontalFlip(p=0.5),\n","                                transforms.RandomRotation(20),\n","                                transforms.ToTensor(),\n","                                transforms.Normalize(mean=[0.485], std=[0.229])])\n","\n","transform_validate = transforms.Compose([transforms.Resize((224, 224)),\n","                                transforms.ToTensor(),\n","                                transforms.Normalize(mean=[0.485], std=[0.229])])"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"jBavQt2QWQUd","executionInfo":{"status":"ok","timestamp":1613810918204,"user_tz":300,"elapsed":303,"user":{"displayName":"Hemanth Subramanyam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjBOXNDrWEHEHb9uFYixP7uXN29lrPcZEC9KjYL3KU=s64","userId":"09323849053732298462"}}},"source":["trainset = dset.ImageFolder(root='./Expression_data/Facial_expression_train/', transform=transform_train)\n","testset = dset.ImageFolder(root='./Expression_data/Facial_expression_test/', transform=transform_validate)"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"id":"fvniBR9LnsC1","executionInfo":{"status":"ok","timestamp":1613811562549,"user_tz":300,"elapsed":304,"user":{"displayName":"Hemanth Subramanyam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjBOXNDrWEHEHb9uFYixP7uXN29lrPcZEC9KjYL3KU=s64","userId":"09323849053732298462"}}},"source":["trainloader = DataLoader(trainset,batch_size=16,shuffle=True)\n","testloader = DataLoader(testset,batch_size=16,shuffle=False)"],"execution_count":25,"outputs":[]},{"cell_type":"code","metadata":{"id":"8ib5QkmeUv3E","executionInfo":{"status":"ok","timestamp":1613811564572,"user_tz":300,"elapsed":249,"user":{"displayName":"Hemanth Subramanyam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjBOXNDrWEHEHb9uFYixP7uXN29lrPcZEC9KjYL3KU=s64","userId":"09323849053732298462"}}},"source":["import torch\n","import torch.nn as nn\n","import torchvision.models as models\n"],"execution_count":26,"outputs":[]},{"cell_type":"code","metadata":{"id":"VU5hdERsFw5o","executionInfo":{"status":"ok","timestamp":1613812722625,"user_tz":300,"elapsed":877,"user":{"displayName":"Hemanth Subramanyam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjBOXNDrWEHEHb9uFYixP7uXN29lrPcZEC9KjYL3KU=s64","userId":"09323849053732298462"}}},"source":["# YOUR CODE HERE to define and train CNN model for Expression_Data.\n","\n","model = models.resnet50(pretrained=True)\n","for param in model.parameters():\n","    param.requires_grad = True\n","\n","model.fc = nn.Sequential(nn.Linear(2048, 256), \n","                        nn.ReLU(), \n","                        nn.Linear(256, 100), \n","                        nn.ReLU(),\n","                        nn.Linear(100, 7))\n","\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)\n"],"execution_count":39,"outputs":[]},{"cell_type":"code","metadata":{"id":"WeSQoT9oVOmC","executionInfo":{"status":"ok","timestamp":1613812725488,"user_tz":300,"elapsed":227,"user":{"displayName":"Hemanth Subramanyam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjBOXNDrWEHEHb9uFYixP7uXN29lrPcZEC9KjYL3KU=s64","userId":"09323849053732298462"}}},"source":["from torchsummary import summary"],"execution_count":40,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8snu6Q-FXCUL","executionInfo":{"status":"ok","timestamp":1613812726495,"user_tz":300,"elapsed":422,"user":{"displayName":"Hemanth Subramanyam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjBOXNDrWEHEHb9uFYixP7uXN29lrPcZEC9KjYL3KU=s64","userId":"09323849053732298462"}},"outputId":"de0d5449-98ae-4a84-d1ea-faa3bb09fc0f"},"source":["print(summary(model.cuda(),input_size=(3,224,224)))"],"execution_count":41,"outputs":[{"output_type":"stream","text":["----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1         [-1, 64, 112, 112]           9,408\n","       BatchNorm2d-2         [-1, 64, 112, 112]             128\n","              ReLU-3         [-1, 64, 112, 112]               0\n","         MaxPool2d-4           [-1, 64, 56, 56]               0\n","            Conv2d-5           [-1, 64, 56, 56]           4,096\n","       BatchNorm2d-6           [-1, 64, 56, 56]             128\n","              ReLU-7           [-1, 64, 56, 56]               0\n","            Conv2d-8           [-1, 64, 56, 56]          36,864\n","       BatchNorm2d-9           [-1, 64, 56, 56]             128\n","             ReLU-10           [-1, 64, 56, 56]               0\n","           Conv2d-11          [-1, 256, 56, 56]          16,384\n","      BatchNorm2d-12          [-1, 256, 56, 56]             512\n","           Conv2d-13          [-1, 256, 56, 56]          16,384\n","      BatchNorm2d-14          [-1, 256, 56, 56]             512\n","             ReLU-15          [-1, 256, 56, 56]               0\n","       Bottleneck-16          [-1, 256, 56, 56]               0\n","           Conv2d-17           [-1, 64, 56, 56]          16,384\n","      BatchNorm2d-18           [-1, 64, 56, 56]             128\n","             ReLU-19           [-1, 64, 56, 56]               0\n","           Conv2d-20           [-1, 64, 56, 56]          36,864\n","      BatchNorm2d-21           [-1, 64, 56, 56]             128\n","             ReLU-22           [-1, 64, 56, 56]               0\n","           Conv2d-23          [-1, 256, 56, 56]          16,384\n","      BatchNorm2d-24          [-1, 256, 56, 56]             512\n","             ReLU-25          [-1, 256, 56, 56]               0\n","       Bottleneck-26          [-1, 256, 56, 56]               0\n","           Conv2d-27           [-1, 64, 56, 56]          16,384\n","      BatchNorm2d-28           [-1, 64, 56, 56]             128\n","             ReLU-29           [-1, 64, 56, 56]               0\n","           Conv2d-30           [-1, 64, 56, 56]          36,864\n","      BatchNorm2d-31           [-1, 64, 56, 56]             128\n","             ReLU-32           [-1, 64, 56, 56]               0\n","           Conv2d-33          [-1, 256, 56, 56]          16,384\n","      BatchNorm2d-34          [-1, 256, 56, 56]             512\n","             ReLU-35          [-1, 256, 56, 56]               0\n","       Bottleneck-36          [-1, 256, 56, 56]               0\n","           Conv2d-37          [-1, 128, 56, 56]          32,768\n","      BatchNorm2d-38          [-1, 128, 56, 56]             256\n","             ReLU-39          [-1, 128, 56, 56]               0\n","           Conv2d-40          [-1, 128, 28, 28]         147,456\n","      BatchNorm2d-41          [-1, 128, 28, 28]             256\n","             ReLU-42          [-1, 128, 28, 28]               0\n","           Conv2d-43          [-1, 512, 28, 28]          65,536\n","      BatchNorm2d-44          [-1, 512, 28, 28]           1,024\n","           Conv2d-45          [-1, 512, 28, 28]         131,072\n","      BatchNorm2d-46          [-1, 512, 28, 28]           1,024\n","             ReLU-47          [-1, 512, 28, 28]               0\n","       Bottleneck-48          [-1, 512, 28, 28]               0\n","           Conv2d-49          [-1, 128, 28, 28]          65,536\n","      BatchNorm2d-50          [-1, 128, 28, 28]             256\n","             ReLU-51          [-1, 128, 28, 28]               0\n","           Conv2d-52          [-1, 128, 28, 28]         147,456\n","      BatchNorm2d-53          [-1, 128, 28, 28]             256\n","             ReLU-54          [-1, 128, 28, 28]               0\n","           Conv2d-55          [-1, 512, 28, 28]          65,536\n","      BatchNorm2d-56          [-1, 512, 28, 28]           1,024\n","             ReLU-57          [-1, 512, 28, 28]               0\n","       Bottleneck-58          [-1, 512, 28, 28]               0\n","           Conv2d-59          [-1, 128, 28, 28]          65,536\n","      BatchNorm2d-60          [-1, 128, 28, 28]             256\n","             ReLU-61          [-1, 128, 28, 28]               0\n","           Conv2d-62          [-1, 128, 28, 28]         147,456\n","      BatchNorm2d-63          [-1, 128, 28, 28]             256\n","             ReLU-64          [-1, 128, 28, 28]               0\n","           Conv2d-65          [-1, 512, 28, 28]          65,536\n","      BatchNorm2d-66          [-1, 512, 28, 28]           1,024\n","             ReLU-67          [-1, 512, 28, 28]               0\n","       Bottleneck-68          [-1, 512, 28, 28]               0\n","           Conv2d-69          [-1, 128, 28, 28]          65,536\n","      BatchNorm2d-70          [-1, 128, 28, 28]             256\n","             ReLU-71          [-1, 128, 28, 28]               0\n","           Conv2d-72          [-1, 128, 28, 28]         147,456\n","      BatchNorm2d-73          [-1, 128, 28, 28]             256\n","             ReLU-74          [-1, 128, 28, 28]               0\n","           Conv2d-75          [-1, 512, 28, 28]          65,536\n","      BatchNorm2d-76          [-1, 512, 28, 28]           1,024\n","             ReLU-77          [-1, 512, 28, 28]               0\n","       Bottleneck-78          [-1, 512, 28, 28]               0\n","           Conv2d-79          [-1, 256, 28, 28]         131,072\n","      BatchNorm2d-80          [-1, 256, 28, 28]             512\n","             ReLU-81          [-1, 256, 28, 28]               0\n","           Conv2d-82          [-1, 256, 14, 14]         589,824\n","      BatchNorm2d-83          [-1, 256, 14, 14]             512\n","             ReLU-84          [-1, 256, 14, 14]               0\n","           Conv2d-85         [-1, 1024, 14, 14]         262,144\n","      BatchNorm2d-86         [-1, 1024, 14, 14]           2,048\n","           Conv2d-87         [-1, 1024, 14, 14]         524,288\n","      BatchNorm2d-88         [-1, 1024, 14, 14]           2,048\n","             ReLU-89         [-1, 1024, 14, 14]               0\n","       Bottleneck-90         [-1, 1024, 14, 14]               0\n","           Conv2d-91          [-1, 256, 14, 14]         262,144\n","      BatchNorm2d-92          [-1, 256, 14, 14]             512\n","             ReLU-93          [-1, 256, 14, 14]               0\n","           Conv2d-94          [-1, 256, 14, 14]         589,824\n","      BatchNorm2d-95          [-1, 256, 14, 14]             512\n","             ReLU-96          [-1, 256, 14, 14]               0\n","           Conv2d-97         [-1, 1024, 14, 14]         262,144\n","      BatchNorm2d-98         [-1, 1024, 14, 14]           2,048\n","             ReLU-99         [-1, 1024, 14, 14]               0\n","      Bottleneck-100         [-1, 1024, 14, 14]               0\n","          Conv2d-101          [-1, 256, 14, 14]         262,144\n","     BatchNorm2d-102          [-1, 256, 14, 14]             512\n","            ReLU-103          [-1, 256, 14, 14]               0\n","          Conv2d-104          [-1, 256, 14, 14]         589,824\n","     BatchNorm2d-105          [-1, 256, 14, 14]             512\n","            ReLU-106          [-1, 256, 14, 14]               0\n","          Conv2d-107         [-1, 1024, 14, 14]         262,144\n","     BatchNorm2d-108         [-1, 1024, 14, 14]           2,048\n","            ReLU-109         [-1, 1024, 14, 14]               0\n","      Bottleneck-110         [-1, 1024, 14, 14]               0\n","          Conv2d-111          [-1, 256, 14, 14]         262,144\n","     BatchNorm2d-112          [-1, 256, 14, 14]             512\n","            ReLU-113          [-1, 256, 14, 14]               0\n","          Conv2d-114          [-1, 256, 14, 14]         589,824\n","     BatchNorm2d-115          [-1, 256, 14, 14]             512\n","            ReLU-116          [-1, 256, 14, 14]               0\n","          Conv2d-117         [-1, 1024, 14, 14]         262,144\n","     BatchNorm2d-118         [-1, 1024, 14, 14]           2,048\n","            ReLU-119         [-1, 1024, 14, 14]               0\n","      Bottleneck-120         [-1, 1024, 14, 14]               0\n","          Conv2d-121          [-1, 256, 14, 14]         262,144\n","     BatchNorm2d-122          [-1, 256, 14, 14]             512\n","            ReLU-123          [-1, 256, 14, 14]               0\n","          Conv2d-124          [-1, 256, 14, 14]         589,824\n","     BatchNorm2d-125          [-1, 256, 14, 14]             512\n","            ReLU-126          [-1, 256, 14, 14]               0\n","          Conv2d-127         [-1, 1024, 14, 14]         262,144\n","     BatchNorm2d-128         [-1, 1024, 14, 14]           2,048\n","            ReLU-129         [-1, 1024, 14, 14]               0\n","      Bottleneck-130         [-1, 1024, 14, 14]               0\n","          Conv2d-131          [-1, 256, 14, 14]         262,144\n","     BatchNorm2d-132          [-1, 256, 14, 14]             512\n","            ReLU-133          [-1, 256, 14, 14]               0\n","          Conv2d-134          [-1, 256, 14, 14]         589,824\n","     BatchNorm2d-135          [-1, 256, 14, 14]             512\n","            ReLU-136          [-1, 256, 14, 14]               0\n","          Conv2d-137         [-1, 1024, 14, 14]         262,144\n","     BatchNorm2d-138         [-1, 1024, 14, 14]           2,048\n","            ReLU-139         [-1, 1024, 14, 14]               0\n","      Bottleneck-140         [-1, 1024, 14, 14]               0\n","          Conv2d-141          [-1, 512, 14, 14]         524,288\n","     BatchNorm2d-142          [-1, 512, 14, 14]           1,024\n","            ReLU-143          [-1, 512, 14, 14]               0\n","          Conv2d-144            [-1, 512, 7, 7]       2,359,296\n","     BatchNorm2d-145            [-1, 512, 7, 7]           1,024\n","            ReLU-146            [-1, 512, 7, 7]               0\n","          Conv2d-147           [-1, 2048, 7, 7]       1,048,576\n","     BatchNorm2d-148           [-1, 2048, 7, 7]           4,096\n","          Conv2d-149           [-1, 2048, 7, 7]       2,097,152\n","     BatchNorm2d-150           [-1, 2048, 7, 7]           4,096\n","            ReLU-151           [-1, 2048, 7, 7]               0\n","      Bottleneck-152           [-1, 2048, 7, 7]               0\n","          Conv2d-153            [-1, 512, 7, 7]       1,048,576\n","     BatchNorm2d-154            [-1, 512, 7, 7]           1,024\n","            ReLU-155            [-1, 512, 7, 7]               0\n","          Conv2d-156            [-1, 512, 7, 7]       2,359,296\n","     BatchNorm2d-157            [-1, 512, 7, 7]           1,024\n","            ReLU-158            [-1, 512, 7, 7]               0\n","          Conv2d-159           [-1, 2048, 7, 7]       1,048,576\n","     BatchNorm2d-160           [-1, 2048, 7, 7]           4,096\n","            ReLU-161           [-1, 2048, 7, 7]               0\n","      Bottleneck-162           [-1, 2048, 7, 7]               0\n","          Conv2d-163            [-1, 512, 7, 7]       1,048,576\n","     BatchNorm2d-164            [-1, 512, 7, 7]           1,024\n","            ReLU-165            [-1, 512, 7, 7]               0\n","          Conv2d-166            [-1, 512, 7, 7]       2,359,296\n","     BatchNorm2d-167            [-1, 512, 7, 7]           1,024\n","            ReLU-168            [-1, 512, 7, 7]               0\n","          Conv2d-169           [-1, 2048, 7, 7]       1,048,576\n","     BatchNorm2d-170           [-1, 2048, 7, 7]           4,096\n","            ReLU-171           [-1, 2048, 7, 7]               0\n","      Bottleneck-172           [-1, 2048, 7, 7]               0\n","       AvgPool2d-173           [-1, 2048, 1, 1]               0\n","          Linear-174                  [-1, 256]         524,544\n","            ReLU-175                  [-1, 256]               0\n","          Linear-176                  [-1, 100]          25,700\n","            ReLU-177                  [-1, 100]               0\n","          Linear-178                    [-1, 7]             707\n","================================================================\n","Total params: 24,058,983\n","Trainable params: 24,058,983\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.57\n","Forward/backward pass size (MB): 286.56\n","Params size (MB): 91.78\n","Estimated Total Size (MB): 378.91\n","----------------------------------------------------------------\n","None\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IMI3SLzhXhE-","executionInfo":{"status":"ok","timestamp":1613814120174,"user_tz":300,"elapsed":1387012,"user":{"displayName":"Hemanth Subramanyam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjBOXNDrWEHEHb9uFYixP7uXN29lrPcZEC9KjYL3KU=s64","userId":"09323849053732298462"}},"outputId":"d5768a88-b4ea-4d11-8430-c335c94c10d0"},"source":["#training loop\n","epochs = 5\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","print(f\"Training on Device: {device}\")\n","for epoch in range(epochs):\n","    running_loss = []\n","    \n","    model.train()\n","    model.to(device)\n","    n=0\n","    for idx,(images,labels) in enumerate(trainloader):\n","        images = images.to(device)\n","        labels = labels.to(device)\n","        n+=1\n","        optimizer.zero_grad()\n","        #print(item)\n","        output = model(images)\n","        loss = criterion(output, labels)\n","        \n","        loss.backward()\n","        if idx%16 == 0: \n","          print(loss.item(), epoch) \n","        running_loss.append(loss.item())\n","        \n","        optimizer.step()\n","        \n","        #validation loop\n","    with torch.no_grad():\n","        model.eval()\n","        total = 0\n","        correct = 0\n","        for images,labels in testloader:\n","            images = images.to(device)\n","            labels = labels.to(device)\n","            output = model(images)\n","            _, pred = torch.max(output.data, 1)\n","            \n","            total+= labels.size(0)\n","            correct += (pred == labels).sum().item()\n","            \n","        print(f\"Epoch: {epoch}\")\n","        print(f\"Training Loss: {sum(running_loss)/len(running_loss)}\")\n","        print(f\"Validation Accuracy: {float(correct)/total}\")"],"execution_count":42,"outputs":[{"output_type":"stream","text":["Training on Device: cuda\n","1.9403088092803955 0\n","1.8676435947418213 0\n","1.792262315750122 0\n","1.7722524404525757 0\n","1.7476600408554077 0\n","1.496900200843811 0\n","1.8165913820266724 0\n","1.6312487125396729 0\n","1.6125746965408325 0\n","1.6983646154403687 0\n","1.9816495180130005 0\n","1.554158091545105 0\n","1.8170920610427856 0\n","1.8561561107635498 0\n","1.8653318881988525 0\n","1.4786951541900635 0\n","2.0035245418548584 0\n","1.6071925163269043 0\n","1.7542295455932617 0\n","1.4954873323440552 0\n","1.8878943920135498 0\n","1.6491085290908813 0\n","1.6459287405014038 0\n","1.7156982421875 0\n","1.598759412765503 0\n","1.648256540298462 0\n","2.0250775814056396 0\n","1.77474045753479 0\n","1.6115318536758423 0\n","1.4083822965621948 0\n","1.6528981924057007 0\n","1.5956058502197266 0\n","1.5850547552108765 0\n","1.5772109031677246 0\n","1.9190353155136108 0\n","1.7772668600082397 0\n","1.519682765007019 0\n","1.710207462310791 0\n","1.8436899185180664 0\n","1.9460381269454956 0\n","1.6476550102233887 0\n","1.9086713790893555 0\n","1.7630562782287598 0\n","1.847900152206421 0\n","1.6373794078826904 0\n","1.873079776763916 0\n","1.7054660320281982 0\n","1.8675557374954224 0\n","1.6409738063812256 0\n","1.7667369842529297 0\n","1.7671010494232178 0\n","1.7708401679992676 0\n","1.8185830116271973 0\n","1.7979799509048462 0\n","1.7367879152297974 0\n","1.6270133256912231 0\n","1.855088472366333 0\n","1.5680608749389648 0\n","1.4249675273895264 0\n","1.8230865001678467 0\n","1.5627036094665527 0\n","1.5861291885375977 0\n","1.7943004369735718 0\n","1.751523494720459 0\n","1.8579469919204712 0\n","1.5370001792907715 0\n","1.6676512956619263 0\n","1.806605339050293 0\n","1.5273102521896362 0\n","1.9950449466705322 0\n","1.6184444427490234 0\n","1.8362438678741455 0\n","Epoch: 0\n","Training Loss: 1.7636445134799525\n","Validation Accuracy: 0.2891380826737027\n","1.8378467559814453 1\n","1.69913911819458 1\n","1.7258809804916382 1\n","1.540591835975647 1\n","1.6612615585327148 1\n","1.8503273725509644 1\n","1.693767786026001 1\n","1.8207993507385254 1\n","1.9045227766036987 1\n","1.5905261039733887 1\n","1.7941477298736572 1\n","1.6303263902664185 1\n","1.8382525444030762 1\n","1.8005784749984741 1\n","1.6719549894332886 1\n","1.7891218662261963 1\n","1.8157678842544556 1\n","1.9209599494934082 1\n","1.8410539627075195 1\n","1.7327214479446411 1\n","1.9748914241790771 1\n","1.694188117980957 1\n","1.948049545288086 1\n","1.6799440383911133 1\n","1.6474162340164185 1\n","1.662016749382019 1\n","1.8534678220748901 1\n","1.5933009386062622 1\n","1.611818552017212 1\n","1.5994294881820679 1\n","1.7086435556411743 1\n","1.5090354681015015 1\n","2.030306339263916 1\n","1.6667481660842896 1\n","1.713432788848877 1\n","1.8026189804077148 1\n","1.7072501182556152 1\n","1.6807701587677002 1\n","1.770808219909668 1\n","1.8341096639633179 1\n","1.6488465070724487 1\n","1.617222547531128 1\n","1.7223542928695679 1\n","1.8020085096359253 1\n","2.033377170562744 1\n","2.0017693042755127 1\n","1.6600748300552368 1\n","1.573951244354248 1\n","1.6389384269714355 1\n","1.7530947923660278 1\n","1.8148810863494873 1\n","1.972720742225647 1\n","1.8068735599517822 1\n","1.417955756187439 1\n","1.7140477895736694 1\n","1.8857319355010986 1\n","1.9495906829833984 1\n","1.689962387084961 1\n","1.8878755569458008 1\n","1.655592679977417 1\n","1.7452281713485718 1\n","1.7915148735046387 1\n","1.7310104370117188 1\n","1.7188143730163574 1\n","1.8683613538742065 1\n","1.7055128812789917 1\n","1.6571582555770874 1\n","1.7880960702896118 1\n","1.6752278804779053 1\n","1.7310088872909546 1\n","1.780626893043518 1\n","1.6620491743087769 1\n","Epoch: 1\n","Training Loss: 1.7524849136565586\n","Validation Accuracy: 0.2926561125769569\n","1.689520001411438 2\n","1.976391077041626 2\n","1.7029098272323608 2\n","1.7009406089782715 2\n","1.5835031270980835 2\n","1.8751775026321411 2\n","1.6689739227294922 2\n","1.7977495193481445 2\n","1.7066285610198975 2\n","1.8611390590667725 2\n","1.77898108959198 2\n","1.8757054805755615 2\n","1.8833246231079102 2\n","1.715449333190918 2\n","1.6746119260787964 2\n","1.541185736656189 2\n","1.5447416305541992 2\n","1.760360836982727 2\n","1.781921148300171 2\n","1.821155071258545 2\n","2.0351059436798096 2\n","1.978957176208496 2\n","1.9753377437591553 2\n","2.0124030113220215 2\n","1.784672737121582 2\n","1.6078567504882812 2\n","1.9219475984573364 2\n","1.7764770984649658 2\n","1.6795345544815063 2\n","1.870462417602539 2\n","1.8597311973571777 2\n","1.6943669319152832 2\n","1.8584232330322266 2\n","1.879661202430725 2\n","2.035961389541626 2\n","1.981396198272705 2\n","1.6473329067230225 2\n","1.6346406936645508 2\n","1.7882450819015503 2\n","1.552201509475708 2\n","1.684213399887085 2\n","1.7336312532424927 2\n","1.8383469581604004 2\n","1.7607645988464355 2\n","1.518661379814148 2\n","1.82402765750885 2\n","1.6984292268753052 2\n","1.8346117734909058 2\n","1.631486177444458 2\n","1.863811731338501 2\n","1.7348722219467163 2\n","1.7156217098236084 2\n","1.8031097650527954 2\n","1.760643482208252 2\n","1.794392466545105 2\n","1.5918574333190918 2\n","2.0234954357147217 2\n","1.7592313289642334 2\n","1.6961486339569092 2\n","1.9167487621307373 2\n","1.7956664562225342 2\n","1.6546748876571655 2\n","1.9691479206085205 2\n","1.8143584728240967 2\n","1.6444584131240845 2\n","2.014843702316284 2\n","1.5507899522781372 2\n","1.8723503351211548 2\n","1.8226901292800903 2\n","1.6127928495407104 2\n","1.765356183052063 2\n","1.6098930835723877 2\n","Epoch: 2\n","Training Loss: 1.7506934968441752\n","Validation Accuracy: 0.3089270008795075\n","1.8011808395385742 3\n","1.7431256771087646 3\n","1.7627078294754028 3\n","1.7522788047790527 3\n","1.8796463012695312 3\n","1.8172270059585571 3\n","1.5596007108688354 3\n","1.7848907709121704 3\n","1.81769859790802 3\n","1.984161376953125 3\n","1.632424235343933 3\n","1.6321171522140503 3\n","1.7784672975540161 3\n","1.6552236080169678 3\n","1.674358606338501 3\n","1.7552947998046875 3\n","1.815298318862915 3\n","1.7549090385437012 3\n","2.039083242416382 3\n","1.7911858558654785 3\n","1.642308235168457 3\n","1.76100754737854 3\n","1.8761727809906006 3\n","1.838238000869751 3\n","1.7537318468093872 3\n","1.7860653400421143 3\n","1.803743600845337 3\n","1.658225655555725 3\n","1.8015692234039307 3\n","1.9217702150344849 3\n","1.868927240371704 3\n","1.9175633192062378 3\n","1.777124285697937 3\n","1.6571727991104126 3\n","1.4838329553604126 3\n","1.3928978443145752 3\n","1.724027395248413 3\n","1.667576551437378 3\n","2.008591890335083 3\n","1.755509614944458 3\n","1.6151484251022339 3\n","1.7110779285430908 3\n","1.529665231704712 3\n","1.4662237167358398 3\n","1.7469779253005981 3\n","1.6352137327194214 3\n","1.7636078596115112 3\n","1.6411105394363403 3\n","1.62278413772583 3\n","1.7096465826034546 3\n","1.7170629501342773 3\n","1.767293930053711 3\n","1.5659704208374023 3\n","1.7140917778015137 3\n","2.0163164138793945 3\n","1.7759578227996826 3\n","1.7767462730407715 3\n","1.6586692333221436 3\n","1.8866157531738281 3\n","1.7180252075195312 3\n","1.541313648223877 3\n","1.7255162000656128 3\n","1.8601710796356201 3\n","1.6962125301361084 3\n","1.4853219985961914 3\n","1.620932936668396 3\n","1.6751599311828613 3\n","1.6979100704193115 3\n","1.9223048686981201 3\n","1.9572198390960693 3\n","1.7687593698501587 3\n","1.4709417819976807 3\n","Epoch: 3\n","Training Loss: 1.7470081395608035\n","Validation Accuracy: 0.3073878627968338\n","1.6823618412017822 4\n","1.496746301651001 4\n","1.6199145317077637 4\n","1.6991982460021973 4\n","1.7611194849014282 4\n","1.7833125591278076 4\n","1.9795916080474854 4\n","1.4627408981323242 4\n","1.591239333152771 4\n","1.834642767906189 4\n","1.9043043851852417 4\n","1.7272017002105713 4\n","1.7742857933044434 4\n","1.6272236108779907 4\n","1.7401509284973145 4\n","1.67269766330719 4\n","1.7668368816375732 4\n","2.0952281951904297 4\n","1.878566026687622 4\n","1.5463461875915527 4\n","1.5828399658203125 4\n","1.8802852630615234 4\n","1.781722903251648 4\n","1.5965900421142578 4\n","1.7222939729690552 4\n","1.8313664197921753 4\n","1.7886197566986084 4\n","1.630923867225647 4\n","1.6728763580322266 4\n","1.68619966506958 4\n","1.5469849109649658 4\n","1.7710777521133423 4\n","1.8002630472183228 4\n","1.9187968969345093 4\n","1.8505276441574097 4\n","1.616542100906372 4\n","1.466487169265747 4\n","1.6888389587402344 4\n","1.8785008192062378 4\n","1.8041366338729858 4\n","1.8048710823059082 4\n","1.869337558746338 4\n","1.6680008172988892 4\n","1.6856211423873901 4\n","1.9155209064483643 4\n","1.9210575819015503 4\n","1.8828636407852173 4\n","1.7601686716079712 4\n","1.6943717002868652 4\n","1.7441133260726929 4\n","1.91203773021698 4\n","1.772639513015747 4\n","1.5372743606567383 4\n","1.763146162033081 4\n","1.7382826805114746 4\n","1.570615291595459 4\n","1.6797254085540771 4\n","1.517917275428772 4\n","1.7478848695755005 4\n","1.516582727432251 4\n","1.5629268884658813 4\n","1.8198877573013306 4\n","1.867301344871521 4\n","1.7932645082473755 4\n","2.077634572982788 4\n","1.7113393545150757 4\n","1.5336028337478638 4\n","1.6581734418869019 4\n","1.9243606328964233 4\n","1.5016530752182007 4\n","1.875099778175354 4\n","1.5719187259674072 4\n","Epoch: 4\n","Training Loss: 1.7435304853301774\n","Validation Accuracy: 0.30936675461741425\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"gNZCxw0wmRlI"},"source":["**Test your model and optimize CNN architecture for predicting the labels correctly**"]},{"cell_type":"code","metadata":{"id":"SzuYlK_8mbYy"},"source":["# YOUR CODE HERE for test evaluation"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pvw3vZ7kR9sR"},"source":["**Team Data Collection (activate the server first)** \n","\n","  - (This can be done on the day of the Hackathon once the login username and password are given)"]},{"cell_type":"markdown","metadata":{"id":"1ki9ee8YUFQr"},"source":["Activate the Server Access\n","* Open the terminal (Command Prompt)\n","* Login to SSH by typing **ssh (username)@aiml-sandbox1.talentsprint.com**. Give the login username which is given to you. \n","\n","Eg: `ssh b15h3gxx@aiml-sandbox1.talentsprint.com`\n","\n","  (If it is your first time connecting to the server from this computer, accept the connection by typing \"yes\".)\n","* After logging into SSH, please activate your virtual environment using the\n","command **source venv/bin/activate** and then press enter\n","* You can start the server by giving the command **sh runserver.sh** and then press enter.\n","* In order to collect team data in mobile app, ensure the server is active\n"]},{"cell_type":"markdown","metadata":{"id":"bHpqJDuFHVmL"},"source":["**Collect your team data using the EFR Mobile App and fine-tune the CNN for expression data on your team**\n","\n","Team Data Collection\n","\n","* Follow the \"Mobile_APP_Documentation\" to collect the Expression photos of your team. These will be stored in the server to which login is provided to you.\n","\n","[Mobile_APP_Documentation](https://drive.google.com/file/d/1tpr8_U0Ll_TexN4s-0pmPPg23J7Usok2/view?usp=sharing)\n","\n","\n","**Download your team expression data from the EFR app into your colab notebook using the links provided below**\n","\n","NOTE: Replace the string \"username\" with your login username (such as b15h3gxx) in the below cell for expression images. \n","\n","This data will be useful for testing the above trained cnn networks."]},{"cell_type":"code","metadata":{"id":"I8U0F_CDIhmh","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1613814123403,"user_tz":300,"elapsed":3216,"user":{"displayName":"Hemanth Subramanyam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjBOXNDrWEHEHb9uFYixP7uXN29lrPcZEC9KjYL3KU=s64","userId":"09323849053732298462"}},"outputId":"b56effa6-5676-4e75-ff07-c1f6d1328359"},"source":["!wget -nH --recursive --no-parent --reject 'index.*' https://aiml-sandbox.talentsprint.com/expression_detection/b15h3g12/captured_images_with_Expression/ --cut-dirs=3  -P ./captured_images_with_Expression"],"execution_count":43,"outputs":[{"output_type":"stream","text":["--2021-02-20 09:42:00--  https://aiml-sandbox.talentsprint.com/expression_detection/b15h3g12/captured_images_with_Expression/\n","Resolving aiml-sandbox.talentsprint.com (aiml-sandbox.talentsprint.com)... 139.162.203.12\n","Connecting to aiml-sandbox.talentsprint.com (aiml-sandbox.talentsprint.com)|139.162.203.12|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘./captured_images_with_Expression/index.html.tmp’\n","\n","index.html.tmp          [ <=>                ]   1.03K  --.-KB/s    in 0s      \n","\n","2021-02-20 09:42:01 (117 MB/s) - ‘./captured_images_with_Expression/index.html.tmp’ saved [1054]\n","\n","Loading robots.txt; please ignore errors.\n","--2021-02-20 09:42:01--  https://aiml-sandbox.talentsprint.com/robots.txt\n","Reusing existing connection to aiml-sandbox.talentsprint.com:443.\n","HTTP request sent, awaiting response... 404 Not Found\n","2021-02-20 09:42:01 ERROR 404: Not Found.\n","\n","Removing ./captured_images_with_Expression/index.html.tmp since it should be rejected.\n","\n","--2021-02-20 09:42:01--  https://aiml-sandbox.talentsprint.com/expression_detection/b15h3g12/captured_images_with_Expression/ANGER/\n","Reusing existing connection to aiml-sandbox.talentsprint.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘./captured_images_with_Expression/ANGER/index.html.tmp’\n","\n","ANGER/index.html.tm     [ <=>                ]     527  --.-KB/s    in 0s      \n","\n","2021-02-20 09:42:01 (67.2 MB/s) - ‘./captured_images_with_Expression/ANGER/index.html.tmp’ saved [527]\n","\n","Removing ./captured_images_with_Expression/ANGER/index.html.tmp since it should be rejected.\n","\n","--2021-02-20 09:42:01--  https://aiml-sandbox.talentsprint.com/expression_detection/b15h3g12/captured_images_with_Expression/DISGUST/\n","Reusing existing connection to aiml-sandbox.talentsprint.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘./captured_images_with_Expression/DISGUST/index.html.tmp’\n","\n","DISGUST/index.html.     [ <=>                ]     408  --.-KB/s    in 0s      \n","\n","2021-02-20 09:42:01 (50.6 MB/s) - ‘./captured_images_with_Expression/DISGUST/index.html.tmp’ saved [408]\n","\n","Removing ./captured_images_with_Expression/DISGUST/index.html.tmp since it should be rejected.\n","\n","--2021-02-20 09:42:01--  https://aiml-sandbox.talentsprint.com/expression_detection/b15h3g12/captured_images_with_Expression/FEAR/\n","Reusing existing connection to aiml-sandbox.talentsprint.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘./captured_images_with_Expression/FEAR/index.html.tmp’\n","\n","FEAR/index.html.tmp     [ <=>                ]     399  --.-KB/s    in 0s      \n","\n","2021-02-20 09:42:01 (49.5 MB/s) - ‘./captured_images_with_Expression/FEAR/index.html.tmp’ saved [399]\n","\n","Removing ./captured_images_with_Expression/FEAR/index.html.tmp since it should be rejected.\n","\n","--2021-02-20 09:42:01--  https://aiml-sandbox.talentsprint.com/expression_detection/b15h3g12/captured_images_with_Expression/HAPPINESS/\n","Reusing existing connection to aiml-sandbox.talentsprint.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘./captured_images_with_Expression/HAPPINESS/index.html.tmp’\n","\n","HAPPINESS/index.htm     [ <=>                ]     414  --.-KB/s    in 0s      \n","\n","2021-02-20 09:42:01 (51.4 MB/s) - ‘./captured_images_with_Expression/HAPPINESS/index.html.tmp’ saved [414]\n","\n","Removing ./captured_images_with_Expression/HAPPINESS/index.html.tmp since it should be rejected.\n","\n","--2021-02-20 09:42:01--  https://aiml-sandbox.talentsprint.com/expression_detection/b15h3g12/captured_images_with_Expression/NEUTRAL/\n","Reusing existing connection to aiml-sandbox.talentsprint.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘./captured_images_with_Expression/NEUTRAL/index.html.tmp’\n","\n","NEUTRAL/index.html.     [ <=>                ]     408  --.-KB/s    in 0s      \n","\n","2021-02-20 09:42:01 (49.9 MB/s) - ‘./captured_images_with_Expression/NEUTRAL/index.html.tmp’ saved [408]\n","\n","Removing ./captured_images_with_Expression/NEUTRAL/index.html.tmp since it should be rejected.\n","\n","--2021-02-20 09:42:01--  https://aiml-sandbox.talentsprint.com/expression_detection/b15h3g12/captured_images_with_Expression/SADNESS/\n","Reusing existing connection to aiml-sandbox.talentsprint.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘./captured_images_with_Expression/SADNESS/index.html.tmp’\n","\n","SADNESS/index.html.     [ <=>                ]     408  --.-KB/s    in 0s      \n","\n","2021-02-20 09:42:02 (49.3 MB/s) - ‘./captured_images_with_Expression/SADNESS/index.html.tmp’ saved [408]\n","\n","Removing ./captured_images_with_Expression/SADNESS/index.html.tmp since it should be rejected.\n","\n","--2021-02-20 09:42:02--  https://aiml-sandbox.talentsprint.com/expression_detection/b15h3g12/captured_images_with_Expression/SURPRISE/\n","Reusing existing connection to aiml-sandbox.talentsprint.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: unspecified [text/html]\n","Saving to: ‘./captured_images_with_Expression/SURPRISE/index.html.tmp’\n","\n","SURPRISE/index.html     [ <=>                ]     411  --.-KB/s    in 0s      \n","\n","2021-02-20 09:42:02 (55.2 MB/s) - ‘./captured_images_with_Expression/SURPRISE/index.html.tmp’ saved [411]\n","\n","Removing ./captured_images_with_Expression/SURPRISE/index.html.tmp since it should be rejected.\n","\n","--2021-02-20 09:42:02--  https://aiml-sandbox.talentsprint.com/expression_detection/b15h3g12/captured_images_with_Expression/ANGER/ANGER_1613813640.jpg\n","Reusing existing connection to aiml-sandbox.talentsprint.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 5748 (5.6K) [image/jpeg]\n","Saving to: ‘./captured_images_with_Expression/ANGER/ANGER_1613813640.jpg’\n","\n","ANGER/ANGER_1613813 100%[===================>]   5.61K  --.-KB/s    in 0s      \n","\n","2021-02-20 09:42:02 (637 MB/s) - ‘./captured_images_with_Expression/ANGER/ANGER_1613813640.jpg’ saved [5748/5748]\n","\n","--2021-02-20 09:42:02--  https://aiml-sandbox.talentsprint.com/expression_detection/b15h3g12/captured_images_with_Expression/ANGER/ANGER_1613813672.jpg\n","Reusing existing connection to aiml-sandbox.talentsprint.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 3469 (3.4K) [image/jpeg]\n","Saving to: ‘./captured_images_with_Expression/ANGER/ANGER_1613813672.jpg’\n","\n","ANGER/ANGER_1613813 100%[===================>]   3.39K  --.-KB/s    in 0s      \n","\n","2021-02-20 09:42:02 (733 MB/s) - ‘./captured_images_with_Expression/ANGER/ANGER_1613813672.jpg’ saved [3469/3469]\n","\n","--2021-02-20 09:42:02--  https://aiml-sandbox.talentsprint.com/expression_detection/b15h3g12/captured_images_with_Expression/DISGUST/DISGUST_1613813660.jpg\n","Reusing existing connection to aiml-sandbox.talentsprint.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 2229 (2.2K) [image/jpeg]\n","Saving to: ‘./captured_images_with_Expression/DISGUST/DISGUST_1613813660.jpg’\n","\n","DISGUST/DISGUST_161 100%[===================>]   2.18K  --.-KB/s    in 0s      \n","\n","2021-02-20 09:42:02 (526 MB/s) - ‘./captured_images_with_Expression/DISGUST/DISGUST_1613813660.jpg’ saved [2229/2229]\n","\n","--2021-02-20 09:42:02--  https://aiml-sandbox.talentsprint.com/expression_detection/b15h3g12/captured_images_with_Expression/FEAR/FEAR_1613813683.jpg\n","Reusing existing connection to aiml-sandbox.talentsprint.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 3993 (3.9K) [image/jpeg]\n","Saving to: ‘./captured_images_with_Expression/FEAR/FEAR_1613813683.jpg’\n","\n","FEAR/FEAR_161381368 100%[===================>]   3.90K  --.-KB/s    in 0s      \n","\n","2021-02-20 09:42:02 (1003 MB/s) - ‘./captured_images_with_Expression/FEAR/FEAR_1613813683.jpg’ saved [3993/3993]\n","\n","--2021-02-20 09:42:02--  https://aiml-sandbox.talentsprint.com/expression_detection/b15h3g12/captured_images_with_Expression/HAPPINESS/HAPPINESS_1613813696.jpg\n","Reusing existing connection to aiml-sandbox.talentsprint.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 3594 (3.5K) [image/jpeg]\n","Saving to: ‘./captured_images_with_Expression/HAPPINESS/HAPPINESS_1613813696.jpg’\n","\n","HAPPINESS/HAPPINESS 100%[===================>]   3.51K  --.-KB/s    in 0s      \n","\n","2021-02-20 09:42:02 (709 MB/s) - ‘./captured_images_with_Expression/HAPPINESS/HAPPINESS_1613813696.jpg’ saved [3594/3594]\n","\n","--2021-02-20 09:42:02--  https://aiml-sandbox.talentsprint.com/expression_detection/b15h3g12/captured_images_with_Expression/NEUTRAL/NEUTRAL_1613813712.jpg\n","Reusing existing connection to aiml-sandbox.talentsprint.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 2897 (2.8K) [image/jpeg]\n","Saving to: ‘./captured_images_with_Expression/NEUTRAL/NEUTRAL_1613813712.jpg’\n","\n","NEUTRAL/NEUTRAL_161 100%[===================>]   2.83K  --.-KB/s    in 0s      \n","\n","2021-02-20 09:42:02 (677 MB/s) - ‘./captured_images_with_Expression/NEUTRAL/NEUTRAL_1613813712.jpg’ saved [2897/2897]\n","\n","--2021-02-20 09:42:02--  https://aiml-sandbox.talentsprint.com/expression_detection/b15h3g12/captured_images_with_Expression/SADNESS/SADNESS_1613813728.jpg\n","Reusing existing connection to aiml-sandbox.talentsprint.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 5217 (5.1K) [image/jpeg]\n","Saving to: ‘./captured_images_with_Expression/SADNESS/SADNESS_1613813728.jpg’\n","\n","SADNESS/SADNESS_161 100%[===================>]   5.09K  --.-KB/s    in 0s      \n","\n","2021-02-20 09:42:03 (1.01 GB/s) - ‘./captured_images_with_Expression/SADNESS/SADNESS_1613813728.jpg’ saved [5217/5217]\n","\n","--2021-02-20 09:42:03--  https://aiml-sandbox.talentsprint.com/expression_detection/b15h3g12/captured_images_with_Expression/SURPRISE/SURPRISE_1613813745.jpg\n","Reusing existing connection to aiml-sandbox.talentsprint.com:443.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 3705 (3.6K) [image/jpeg]\n","Saving to: ‘./captured_images_with_Expression/SURPRISE/SURPRISE_1613813745.jpg’\n","\n","SURPRISE/SURPRISE_1 100%[===================>]   3.62K  --.-KB/s    in 0s      \n","\n","2021-02-20 09:42:03 (840 MB/s) - ‘./captured_images_with_Expression/SURPRISE/SURPRISE_1613813745.jpg’ saved [3705/3705]\n","\n","FINISHED --2021-02-20 09:42:03--\n","Total wall clock time: 2.7s\n","Downloaded: 16 files, 34K in 0s (330 MB/s)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"B4-clpEl-1RF"},"source":["%ls"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rfZ0omsOJsp-"},"source":["# YOUR CODE HERE for loading the team expression data. Note: Use the same transform which used for Expression_Data.\n","# YOU CODE HERE for Dataloader\n","finalClassifierDset = dset.ImageFolder(root='./captured_images_with_Expression',\n","                                       transform = transforms.Compose([transforms.Grayscale(num_output_channels = 1), transforms.Resize((100,100)), transforms.ToTensor()]))\n","representation_dataloader = DataLoader(finalClassifierDset, shuffle=False, num_workers=8, batch_size=100)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"P9VE2o32KO4y"},"source":["# YOUR CODE HERE for getting the CNN representation of your team data with expression. Optimize the CNN model for predicting the labels of expressions correctly\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OGixO_z6Gf-Y"},"source":["**Save your trained model**\n","\n","* Save the state dictionary of the classifier (use pytorch only), It will be useful in\n","integrating model to the mobile app\n","\n"," [Hint](https://pytorch.org/tutorials/beginner/saving_loading_models.html)"]},{"cell_type":"code","metadata":{"id":"A7KAIpLsI4Uj","executionInfo":{"status":"ok","timestamp":1613815175981,"user_tz":300,"elapsed":470,"user":{"displayName":"Hemanth Subramanyam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjBOXNDrWEHEHb9uFYixP7uXN29lrPcZEC9KjYL3KU=s64","userId":"09323849053732298462"}}},"source":["### YOUR CODE HERE for saving the CNN model\n","state = {'net_dict': model.state_dict()}\n","torch.save(state, './resnet50_model.t7')"],"execution_count":44,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jsCHKXubHAJB"},"source":["**Download your trained model**\n","* Given the path of model file the following code downloads it through the browser"]},{"cell_type":"code","metadata":{"id":"BDmWXfPaHJZG","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1613815201454,"user_tz":300,"elapsed":282,"user":{"displayName":"Hemanth Subramanyam","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjBOXNDrWEHEHb9uFYixP7uXN29lrPcZEC9KjYL3KU=s64","userId":"09323849053732298462"}},"outputId":"e97cf2b6-9147-474b-9855-6b1218e9c2c2"},"source":["from google.colab import files\n","files.download('./resnet50_model.t7')"],"execution_count":45,"outputs":[{"output_type":"display_data","data":{"application/javascript":["\n","    async function download(id, filename, size) {\n","      if (!google.colab.kernel.accessAllowed) {\n","        return;\n","      }\n","      const div = document.createElement('div');\n","      const label = document.createElement('label');\n","      label.textContent = `Downloading \"${filename}\": `;\n","      div.appendChild(label);\n","      const progress = document.createElement('progress');\n","      progress.max = size;\n","      div.appendChild(progress);\n","      document.body.appendChild(div);\n","\n","      const buffers = [];\n","      let downloaded = 0;\n","\n","      const channel = await google.colab.kernel.comms.open(id);\n","      // Send a message to notify the kernel that we're ready.\n","      channel.send({})\n","\n","      for await (const message of channel.messages) {\n","        // Send a message to notify the kernel that we're ready.\n","        channel.send({})\n","        if (message.buffers) {\n","          for (const buffer of message.buffers) {\n","            buffers.push(buffer);\n","            downloaded += buffer.byteLength;\n","            progress.value = downloaded;\n","          }\n","        }\n","      }\n","      const blob = new Blob(buffers, {type: 'application/binary'});\n","      const a = document.createElement('a');\n","      a.href = window.URL.createObjectURL(blob);\n","      a.download = filename;\n","      div.appendChild(a);\n","      a.click();\n","      div.remove();\n","    }\n","  "],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{"tags":[]}},{"output_type":"display_data","data":{"application/javascript":["download(\"download_81577c7c-5aaa-4fda-a428-c607495c9580\", \"resnet50_model.t7\", 96503621)"],"text/plain":["<IPython.core.display.Javascript object>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"R7ccsM_ZISWj"},"source":["##**Stage 5 (Anti Face Spoofing): (5 marks)**\n","\n","\n","---\n","\n","\n","\n","The objective of anti face spoofing is to be able to unlock (say) a screen not just by your image\n","(which can be easily be spoofed with a photograph of yours) but by a switch in the expression\n","demanded by the Mobile App (which is much less probable to mimic)\n","* **Grading scheme**:\n","> * **Anti Face Spoofing**: (5M Only if both the cases mentioned below are achieved)\n",">>* **Unlock**: Correct face + Correct Demanded Expression\n",">>* **Stay Locked**: Correct face + Incorrect Demanded Expression (as you might imagine there are multiple other such possibilities, which you are free to explore)"]},{"cell_type":"code","metadata":{"id":"E9Wl6ZCEJ8gr"},"source":["# Test in your mobile app and see if it gets unlock."],"execution_count":null,"outputs":[]}]}