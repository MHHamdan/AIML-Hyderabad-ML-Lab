{"nbformat": 4, "nbformat_minor": 0, "metadata": {"colab": {"name": "U2W7_04_Newsgroup_PerformanceMetrics_C.ipynb", "provenance": [], "collapsed_sections": []}, "kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}}, "cells": [{"cell_type": "markdown", "metadata": {"id": "UH62CWxTDTpf"}, "source": ["# Advanced Certification in AIML\n", "## A Program by IIIT-H and TalentSprint"]}, {"cell_type": "markdown", "metadata": {"id": "qu26Vq9jDTpj"}, "source": ["### Learning Objectives:\n", "\n", "The focus of this experiment is to check the performance metrics under section 3  below. Section 1 and 2 is a repeat of Word2Vec and 'Bag of words' models you saw earlier. You may skim through them."]}, {"cell_type": "code", "metadata": {"id": "2rsqN-usrMtj", "cellView": "form"}, "source": ["#@title Experiment Walkthrough\n", "#@markdown 20 Newsgroup PerformanceMetrics\n", "from IPython.display import HTML\n", "\n", "HTML(\"\"\"<video width=\"420\" height=\"240\" controls>\n", "  <source src=\"https://cdn.talentsprint.com/aiml/AIML_BATCH_HYD_7/JAN20/performance_metrics.mp4\" type=\"video/mp4\">\n", "</video>\n", "\"\"\")"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "7pobf8O4bBRJ"}, "source": ["### Dataset\n", "In this experiment we use the 20 newsgroup dataset\n", "\n", "**Description**\n", "\n", "This dataset is a collection of approximately 20,000 newsgroup documents, partitioned across 20 different newsgroups. That is there are approximately one thousand documents taken from each of the following newsgroups:\n", "\n", "    alt.athesim\n", "    comp.graphics   \n", "    comp.os.ms-windows.misc\n", "    comp.sys.ibm.pc.hardware\n", "    comp.sys.mac.hardware\n", "    comp.windows.x\n", "    misc.forsale\n", "    rec.autos\n", "    rec.motorcycles\n", "    rec.sport.baseball\n", "    rec.sport.hockey\n", "    sci.crypt\n", "    sci.electronics\n", "    sci.med\n", "    sci.space\n", "    soc.religion.christian\n", "    talk.politics.guns\n", "    talk.politics.mideast\n", "    talk.politics.misc\n", "    talk.religion.misc\n", "\n", "The dataset consists of **Usenet** posts--essentially an email sent by subscribers to that newsgroup. They typically contain quotes from previous posts as well as cross posts i.e. a few posts may be sent to more than once in a newsgroup.\n", "\n", "Each newsgroup is stored in a subdirectory, with each post stored as a separate file.\n", "\n", "Data source to this experiment : http://archive.ics.uci.edu/ml/datasets/Twenty+Newsgroups"]}, {"cell_type": "markdown", "metadata": {"id": "-ZYYZDRUDZQe"}, "source": ["### Setup Steps"]}, {"cell_type": "code", "metadata": {"id": "pl3kyR-_DYJG"}, "source": ["#@title Please enter your registration id to start: { run: \"auto\", display-mode: \"form\" }\n", "Id = \"\" #@param {type:\"string\"}\n"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"id": "d5pX0FDNDfCz"}, "source": ["#@title Please enter your password (normally your phone number) to continue: { run: \"auto\", display-mode: \"form\" }\n", "password = \"\" #@param {type:\"string\"}\n"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"cellView": "form", "id": "GZbag6jFbYYX"}, "source": ["#@title Run this cell to complete the setup for this Notebook\n", "from IPython import get_ipython\n", "\n", "ipython = get_ipython()\n", "  \n", "notebook= \"U2W7_04_Newsgroup_PerformanceMetrics_C\" #name of the notebook\n", "\n", "def setup():\n", "\n", "    ipython.magic(\"sx pip3 install gensim\")\n", "    ipython.magic(\"sx wget https://cdn.talentsprint.com/aiml/Experiment_related_data/AIML_DS_NEWSGROUPS_PICKELFILE.pkl\")\n", "    #ipython.magic(\"sx unzip AIML_DS_NEWSGROUPS_PICKELFILE.pkl.zip?dl=1\")\n", "    ipython.magic(\"sx wget https://cdn.talentsprint.com/aiml/Experiment_related_data/AIML_DS_WORD2VEC2D_STD.pkl.zip?dl=1\")\n", "    ipython.magic(\"sx unzip AIML_DS_WORD2VEC2D_STD.pkl.zip?dl=1\")\n", "    ipython.magic(\"sx wget https://cdn.talentsprint.com/talentsprint1/archives/sc/aiml/experiment_related_data/AIML_DS_GOOGLENEWS-VECTORS-NEGATIVE-300_STD.rar\")\n", "    ipython.magic(\"sx unrar e /content/AIML_DS_GOOGLENEWS-VECTORS-NEGATIVE-300_STD.rar\")\n", "    from IPython.display import HTML, display\n", "    display(HTML('<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId={0}&recordId={1}\"></script>'.format(getId(),submission_id)))\n", "    print(\"Setup completed successfully\")\n", "    return\n", "\n", "def submit_notebook():\n", "    ipython.magic(\"notebook -e \"+ notebook + \".ipynb\")\n", "    \n", "    import requests, json, base64, datetime\n", "\n", "    url = \"https://dashboard.talentsprint.com/xp/app/save_notebook_attempts\"\n", "    if not submission_id:\n", "      data = {\"id\" : getId(), \"notebook\" : notebook, \"mobile\" : getPassword()}\n", "      r = requests.post(url, data = data)\n", "      r = json.loads(r.text)\n", "\n", "      if r[\"status\"] == \"Success\":\n", "          return r[\"record_id\"]\n", "      elif \"err\" in r:        \n", "        print(r[\"err\"])\n", "        return None        \n", "      else:\n", "        print (\"Something is wrong, the notebook will not be submitted for grading\")\n", "        return None\n", "    \n", "    elif getAnswer() and getComplexity() and getAdditional() and getConcepts() and getWalkthrough() and getComments() and getMentorSupport():\n", "      f = open(notebook + \".ipynb\", \"rb\")\n", "      file_hash = base64.b64encode(f.read())\n", "\n", "      data = {\"complexity\" : Complexity, \"additional\" :Additional, \n", "              \"concepts\" : Concepts, \"record_id\" : submission_id, \n", "              \"answer\" : Answer, \"id\" : Id, \"file_hash\" : file_hash,\n", "              \"notebook\" : notebook, \"feedback_walkthrough\":Walkthrough ,\n", "              \"feedback_experiments_input\" : Comments,\n", "              \"feedback_mentor_support\": Mentor_support}\n", "\n", "      r = requests.post(url, data = data)\n", "      r = json.loads(r.text)\n", "      if \"err\" in r:        \n", "        print(r[\"err\"])\n", "        return None   \n", "      else:\n", "        print(\"Your submission is successful.\")\n", "        print(\"Ref Id:\", submission_id)\n", "        print(\"Date of submission: \", r[\"date\"])\n", "        print(\"Time of submission: \", r[\"time\"])\n", "        print(\"View your submissions: https://aiml.iiith.talentsprint.com/notebook_submissions\")\n", "        #print(\"For any queries/discrepancies, please connect with mentors through the chat icon in LMS dashboard.\")\n", "        return submission_id\n", "    else: submission_id\n", "    \n", "\n", "def getAdditional():\n", "  try:\n", "    if not Additional: \n", "      raise NameError\n", "    else:\n", "      return Additional  \n", "  except NameError:\n", "    print (\"Please answer Additional Question\")\n", "    return None\n", "\n", "def getComplexity():\n", "  try:\n", "    if not Complexity:\n", "      raise NameError\n", "    else:\n", "      return Complexity\n", "  except NameError:\n", "    print (\"Please answer Complexity Question\")\n", "    return None\n", "  \n", "def getConcepts():\n", "  try:\n", "    if not Concepts:\n", "      raise NameError\n", "    else:\n", "      return Concepts\n", "  except NameError:\n", "    print (\"Please answer Concepts Question\")\n", "    return None\n", "  \n", "  \n", "def getWalkthrough():\n", "  try:\n", "    if not Walkthrough:\n", "      raise NameError\n", "    else:\n", "      return Walkthrough\n", "  except NameError:\n", "    print (\"Please answer Walkthrough Question\")\n", "    return None\n", "  \n", "def getComments():\n", "  try:\n", "    if not Comments:\n", "      raise NameError\n", "    else:\n", "      return Comments\n", "  except NameError:\n", "    print (\"Please answer Comments Question\")\n", "    return None\n", "  \n", "\n", "def getMentorSupport():\n", "  try:\n", "    if not Mentor_support:\n", "      raise NameError\n", "    else:\n", "      return Mentor_support\n", "  except NameError:\n", "    print (\"Please answer Mentor support Question\")\n", "    return None\n", "\n", "def getAnswer():\n", "  try:\n", "    if not Answer:\n", "      raise NameError \n", "    else: \n", "      return Answer\n", "  except NameError:\n", "    print (\"Please answer Question\")\n", "    return None\n", "  \n", "\n", "def getId():\n", "  try: \n", "    return Id if Id else None\n", "  except NameError:\n", "    return None\n", "\n", "def getPassword():\n", "  try:\n", "    return password if password else None\n", "  except NameError:\n", "    return None\n", "\n", "submission_id = None\n", "### Setup \n", "if getPassword() and getId():\n", "  submission_id = submit_notebook()\n", "  if submission_id:\n", "    setup() \n", "else:\n", "  print (\"Please complete Id and Password cells before running setup\")\n", "\n"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"id": "NGUe0eXtcl0K"}, "source": ["!ls"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "EoOfWUvct6OD"}, "source": ["#### Import required packages"]}, {"cell_type": "code", "metadata": {"id": "iCwSJmXNDTpq"}, "source": ["# Importing required Packages\n", "import pickle\n", "import re\n", "import operator\n", "from collections import defaultdict\n", "import matplotlib.pyplot as plt\n", "import numpy as np\n", "import math\n", "import collections\n", "import gensim"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "LkOdbWcbDTpm"}, "source": ["\n", "\n", "To get a sense of our data, let us first start by counting the frequencies of the target classes in our news articles in the training set."]}, {"cell_type": "code", "metadata": {"id": "urLEOmkbDTpv"}, "source": ["dataset = pickle.load(open('AIML_DS_NEWSGROUPS_PICKELFILE.pkl','rb'))\n", "print(dataset.keys())\n", "\n", "# Print frequencies of dataset\n", "print(\"Class : count\")\n", "print(\"--------------\")\n", "number_of_documents = 0\n", "for key in dataset:\n", "    print(key, ':', len(dataset[key]))"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "3E1md10WDTpy"}, "source": ["## Splitting into train and test sets"]}, {"cell_type": "markdown", "metadata": {"id": "R0AYtzi-DTpz"}, "source": ["Next, let us split our dataset which consists of 1000 samples per class, into training and test sets. We use 950 samples from each class in the training set, and the remaining 50 in the test set. \n", "\n", "As a mental exercise you should try reasoning about why is it important to ensure a nearly equal distribution of classes in your training and test sets. "]}, {"cell_type": "code", "metadata": {"id": "SjDQqmbMDTp1"}, "source": ["train_set = {}\n", "test_set = {}\n", "new_dataset = {}\n", "# Clean dataset for text encoding issues :- Very useful when dealing with non-unicode characters\n", "for key in dataset:\n", "    new_dataset[key] = [[i.decode('utf-8', errors='replace').lower() for i in f] for f in dataset[key]]\n", "    \n", "# Break dataset into 95-5 split for training and testing\n", "n_train = 0\n", "n_test = 0\n", "for k in new_dataset:\n", "    split = int(0.95*len(new_dataset[k]))\n", "    train_set[k] = new_dataset[k][0:split]\n", "    test_set[k] = new_dataset[k][split:]\n", "    n_train += len(train_set[k])\n", "    n_test += len(test_set[k])"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "ob_suw8Vnwfm"}, "source": ["In the following cell/block, we are finding the frequency of the words and storing it in a dictionary. By doing this we have the vocabulary built for the corpus (in this case 20 news groups)"]}, {"cell_type": "code", "metadata": {"id": "BcXuwTGWDTp5"}, "source": ["# Initialize a dictionary to store frequencies of words.\n", "# Key:Value === Word:Count\n", "frequency = defaultdict(int)\n", "    \n", "for key in train_set:\n", "    for f in train_set[key]:\n", "        \n", "        # Find all words which consist only of capital and lowercase characters and are between length of 2-9.\n", "        # We ignore all special characters such as !.$ and words containing numbers\n", "        words = re.findall(r'(\\b[A-Za-z][a-z]{2,9}\\b)', ' '.join(f))\n", "    \n", "        for word in words:\n", "            frequency[word] += 1\n", "\n", "sorted_words = sorted(frequency.items(), key=operator.itemgetter(1), reverse=True)\n", "print(\"Top-10 most frequent words:\")\n", "for word in sorted_words[:10]:\n", "    print(word)\n", "\n", "print('----------------------------')\n", "print(\"10 least frequent words:\")\n", "for word in sorted_words[-10:]:\n", "    print(word)"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "Sa7p82ILn2Is"}, "source": ["### Ignore the 25 most frequent words, and the words which appear less than 100 times\n", "\n", "Please observe the bar chart of word vs its frequency. Generally the stop words, the words that appear on most of the documents and the words that are too rare come in this list. These words don't add much value in the classification task. So, we are removing them to decrease the vocabulary. Removing the words in vocabulary helps in saving memory and speeds up the processing (i.e. it decreases the space complexity to large extent)"]}, {"cell_type": "code", "metadata": {"id": "VJpqahqtDTp-"}, "source": ["valid_words = defaultdict(int)\n", "\n", "print('Number of words before preprocessing:', len(sorted_words))\n", "\n", "# Ignore the 25 most frequent words, and the words which appear less than 100 times\n", "ignore_most_frequent = 25\n", "freq_thresh = 100\n", "feature_number = 0\n", "for word, word_frequency in sorted_words[ignore_most_frequent:]:\n", "    if word_frequency > freq_thresh:\n", "        valid_words[word] = feature_number\n", "        feature_number += 1\n", "        \n", "print('Number of words after preprocessing:', len(valid_words))\n", "\n", "word_vector_size = len(valid_words)"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "bXqK59gGDTqE"}, "source": ["# Section 1: Word2Vec"]}, {"cell_type": "markdown", "metadata": {"id": "csm5a0hrDTqI"}, "source": ["In the previous section we saw a naive way to represent words as dense vectors which can leverage the semantics of the words.\n", "\n", "The problem with count-based word representations is that they are costly in terms of memory to compute large co-occurrence matrices. Let us see another method to find representations of words without explicitly counting words.\n", "\n", "Here, we aim to predict the next word given the context in which the word appears. (For example, given the last $n$ words, predict the next word). A very smart way to do this is by using a feature representation called \"Word2Vec\" with transforms each word into 300-dimensional vectors.\n", "\n", "Link to pretrained 300 dimensional word2vec: https://drive.google.com/uc?id=0B7XkCwpI5KDYNlNUTTlSS21pQmM&export=download"]}, {"cell_type": "markdown", "metadata": {"id": "0QBnlIZxDTqK"}, "source": ["## 1.1 Visualizations\n", "\n", "Before we go to the actual 300 dimensional vectors, let's try to explore some of the more intriguing properties of word2vec.\n", "\n", "You have been provided with a sample of word vectors. **We have reduced the dimensionality of the 300-dimensional vectors to 2 dimensions, so that we can plot them in matplotlib.**"]}, {"cell_type": "code", "metadata": {"id": "yg0FXnBhDTqL"}, "source": ["def plot_values(values, labels, figsize = (8,4), c = []):\n", "    x = []\n", "    y = []\n", "    for value in values:\n", "        x.append(value[0])\n", "        y.append(value[1])\n", "        \n", "    plt.figure(figsize=figsize) \n", "    for i in range(len(labels)):\n", "        plt.scatter(x[i],y[i], color=c[i])\n", "        plt.annotate(labels[i],\n", "                     xy=(x[i], y[i]),\n", "                     xytext=(5, 2),\n", "                     textcoords='offset points',\n", "                     ha='right',\n", "                     va='bottom')\n", "    plt.show()\n", "\n", "\n", "import pickle\n", "two_dim_model = pickle.load(open('AIML_DS_WORD2VEC2D_STD.pkl', 'rb'))\n", "\n", "wv_labels = {}\n", "for vec, word in two_dim_model:\n", "    wv_labels[word] = vec\n", "    \n", "colors = ['blue' for i in range(len(wv_labels))]\n", "\n", "plot_values(wv_labels.values(), list(wv_labels.keys()), figsize = (16, 9), c = colors)"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "W2Yxw_7FDTqP"}, "source": ["Now, we have given you the 2D representation of different word vectors. Plot the word vectors for the words 'King', 'Queen', 'man', 'women', 'Germany', 'France', 'Berlin', 'Paris', 'best', 'good', 'strong', 'strongest' in green color:"]}, {"cell_type": "code", "metadata": {"id": "JMF6vO05DTqQ"}, "source": ["wv_list = ['king', 'queen', 'man', 'woman', 'Germany', 'France', 'Berlin', 'Paris', 'best', 'good', 'strong', 'strongest']\n", "wv_new_labels = {}\n", "for word in wv_list:\n", "    wv_new_labels[word] = wv_labels[word]\n", "\n", "colors = ['green' for i in range(len(wv_new_labels))]\n", "plot_values(wv_new_labels.values(), list(wv_new_labels.keys()), c = colors)"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "2PbuN6OaDTqU"}, "source": ["Consider the word analogy question: \"**Queen is to King, as Woman is to what?**\"\n", "\n", "To answer this question, we aim to find what the difference between a \"King\" and \"Queen\" is, and apply that difference to a \"Woman\". If we try to put this mathematically, we can write:- \n", "\n", "$$\n", " Answer = Queen - King + Woman\n", "$$\n", "\n", "Compute the value of the vector on the right hand side of the above equation and plot the resulting vector in red in the same plot as before."]}, {"cell_type": "code", "metadata": {"id": "GHer2vrbDTqV"}, "source": ["answer = wv_new_labels['queen']  - wv_new_labels['king'] + wv_new_labels['woman']\n", "\n", "wv_new_labels['answer1'] = answer\n", "\n", "colors = ['green' if word not in ['answer1'] else 'red' for word in wv_new_labels]\n", "\n", "plot_values(wv_new_labels.values(), list(wv_new_labels.keys()), c = colors)"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "5kCh_NClDTqY"}, "source": ["Notice how the answer vector is very close to the vector of the word \"Man\"? Incidentally, \"Man\" is the right answer to the word analogy question! This is the power of Word2Vec representations."]}, {"cell_type": "markdown", "metadata": {"id": "mOcxKzuvDTql"}, "source": ["## 1.2 Load pre-trained Word2Vec\n", "\n", "Let us now proceed to load the complete pretrained vectors."]}, {"cell_type": "code", "metadata": {"id": "yXoawLkSDTqn"}, "source": ["model = gensim.models.KeyedVectors.load_word2vec_format('AIML_DS_GOOGLENEWS-VECTORS-NEGATIVE-300_STD.bin', binary=True, limit=500000)"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "wFdjtmZLDTqq"}, "source": ["## 1.3 Word2Vec representation\n", "\n", " This method converts documents to word vectors. It first checks if the word is valid according to our initial frequency threshold. Next, if it is, we add the precomputed word vectors together. If the word is valid, but we do not have a valid vector to represent the word, we add a random gaussian noise instead. Since we do not want to induce new noise each time, we store the same noise vector for training and test time in substitute_word_vecs variable."]}, {"cell_type": "markdown", "metadata": {"id": "hcf20pVAoC_S"}, "source": ["###  Now, let us think about what we mean by \"valid vector for representing a word\" and why is doesn't it exist in some cases?\n", "\n", "* It happens when we have  misspelt word in the vocabulary\n", "* If the vocabulary for corpus you are classifying and the corpus used to find the vector embedding are different."]}, {"cell_type": "code", "metadata": {"id": "DiZxIihmDTqr"}, "source": ["word2vec_vector_size = 300\n", "\n", "'''\n", " This method converts documents to word vectors. It first checks if the word is valid according to our initial frequency \n", " threshold. Next, if it is, we add the precomputed word vectors together. If the word is valid, but we do not have a valid \n", " vector to represent the word, we add a random gaussian noise instead. Since we do not want to induce new noise each time,\n", " we store the same noise vector for training and test time in substitute_word_vecs variable.\n", "'''\n", "def convert_to_w2v(dataset, number_of_documents, substitute_word_vecs={}):\n", "    labels = np.zeros((number_of_documents, 1))\n", "    w2v_rep = np.zeros((number_of_documents, word2vec_vector_size))\n", "    \n", "    # Iterate over the dataset and split into words\n", "    i = 0\n", "    for label, class_name in enumerate(dataset):\n", "        for f in dataset[class_name]:\n", "            text = ' '.join(f).split(' ')\n", "            valid_count = 1\n", "            for word in text:\n", "                \n", "                # Check if word is valid or not according to original dataset pruning\n", "                if word in valid_words:\n", "                    try:\n", "                        w2v_rep[i] += model[word]\n", "                    except:\n", "                        '''The word isn't in our pretrained word-vectors, hence we add a random gaussian noise\n", "                         to account for this. We store the random vector we assigned to the word, and reuse \n", "                         the same vector during test time to ensure consistency.'''\n", "                        \n", "                        if word not in substitute_word_vecs.keys():\n", "                            substitute_word_vecs[word] = np.random.normal(-0.25, 0.25, word2vec_vector_size)\n", "                            \n", "                        w2v_rep[i] += substitute_word_vecs[word]\n", "                    \n", "                    valid_count += 1\n", "            \n", "            # Average\n", "            w2v_rep[i] = w2v_rep[i] / valid_count\n", "            \n", "            # Save label\n", "            labels[i] = label\n", "            \n", "            i += 1\n", "    \n", "    return w2v_rep, labels, substitute_word_vecs\n", "\n", "# Convert the train and test datasets into their word2vec representations\n", "train_w2v_set, train_w2v_labels, substitute_word_vecs = convert_to_w2v(train_set, n_train)\n", "test_w2v_set, test_w2v_labels,_ = convert_to_w2v(test_set, n_test, substitute_word_vecs)"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "tmaYxNu-hIc5"}, "source": ["# Section 2:  Bag-of-Words representation\n", "\n", "The simplest way to represent a document $D$ as a vector $V$ would be to now count the relevant words in the document. \n", "\n", "For each document, make a vector of the count of each of the words in the vocabulary (excluding the words removed in the previous step - the \"stopwords\")."]}, {"cell_type": "code", "metadata": {"id": "-nsxZHLZdF3z"}, "source": ["def convert_to_BoW(dataset, number_of_documents):\n", "    bow_representation = np.zeros((number_of_documents, word_vector_size))\n", "    labels = np.zeros((number_of_documents, 1))\n", "    \n", "    i = 0\n", "    for label, class_name in enumerate(dataset):\n", "        \n", "        # For each file\n", "        for f in dataset[class_name]:\n", "            \n", "            # Read all text in file\n", "            text = ' '.join(f).split(' ')\n", "            \n", "            # For each word\n", "            for word in text:\n", "                if word in valid_words:\n", "                    bow_representation[i, valid_words[word]] += 1\n", "            \n", "            # Label of document\n", "            labels[i] = label\n", "            \n", "            # Increment document counter\n", "            i += 1\n", "    \n", "    return bow_representation, labels\n", "\n", "# Convert the dataset into their bag of words representation treating train and test separately\n", "train_bow_set, train_bow_labels = convert_to_BoW(train_set, n_train)\n", "test_bow_set, test_bow_labels = convert_to_BoW(test_set, n_test)"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "tJAVDaPtiFCv"}, "source": ["# 3. Using performance metrics"]}, {"cell_type": "markdown", "metadata": {"id": "hrs_bVWhDTqy"}, "source": ["## 3.1 Document classification using Word2Vec\n", "\n", "For the test documents, use your favorite distance metric (Cosine, Eucilidean, etc.) to find similar news articles from your training set and classify using kNN."]}, {"cell_type": "code", "metadata": {"id": "lhGbqmGYWX3k"}, "source": ["from sklearn.neighbors import KNeighborsClassifier"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"id": "VaUD0G5QWmls"}, "source": ["neigh = KNeighborsClassifier(n_neighbors=3)\n", "neigh.fit(train_w2v_set, train_w2v_labels)"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"id": "7Wlk0vKXW3Q_"}, "source": ["predicted_value = neigh.predict(test_w2v_set)"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"id": "fpsGiz45XQ6X"}, "source": ["from sklearn.metrics import accuracy_score"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"id": "Dyk-FTCiXSBX"}, "source": ["accuracy_score(test_w2v_labels,predicted_value)"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "QAfYqPo7kqmq"}, "source": ["#### Precision"]}, {"cell_type": "code", "metadata": {"id": "XR_QBRVHkqmv"}, "source": ["from sklearn.metrics import precision_score\n", "precision_score(test_w2v_labels, predicted_value, average='macro') "], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "kYQ2kSqzkqm3"}, "source": ["### Recall\n"]}, {"cell_type": "code", "metadata": {"id": "cJxThHQVkqm9"}, "source": ["from sklearn.metrics import recall_score\n", "recall_score(test_w2v_labels, predicted_value, average='macro') "], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "8ktTImr3kqnW"}, "source": ["#### Classification Report"]}, {"cell_type": "code", "metadata": {"id": "9J6qTdOpkqnY"}, "source": ["from sklearn.metrics import classification_report"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"id": "jAZQGL7Jkqnc"}, "source": ["print(classification_report(test_w2v_labels,predicted_value))"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"id": "lpdiOhd7kqng"}, "source": ["import seaborn as sns; sns.set()\n", "from sklearn.metrics import confusion_matrix\n", "matW2V = confusion_matrix(test_w2v_labels, predicted_value)"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"id": "oE0qibAqkqnj"}, "source": ["dist_labelsW2V=np.asarray(list(set(test_w2v_labels.flatten().astype(int))))"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "99aAmFWckqn3"}, "source": ["#### Confusion matrix"]}, {"cell_type": "code", "metadata": {"id": "OLZVBSRCkqn4"}, "source": ["sns.heatmap(matW2V.T, square=True, annot=True, fmt='d', cbar=False,\n", "            xticklabels=dist_labelsW2V,\n", "            yticklabels=dist_labelsW2V)\n", "plt.xlabel('true label')\n", "plt.ylabel('predicted label');"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "7Z_FYG9rdNPY"}, "source": ["## 3.2 Document classification using Bag-of-Words\n", "\n"]}, {"cell_type": "code", "metadata": {"id": "FP_MCK14gPqo"}, "source": ["from sklearn.neighbors import KNeighborsClassifier\n", "model = KNeighborsClassifier(n_neighbors=5, metric='euclidean')\n", "model.fit(train_bow_set, train_bow_labels)"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"id": "5hI_sNMZgUkD"}, "source": ["pred = model.predict(test_bow_set) # this cell takes some time to execute"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "ozuXRWk9gVE1"}, "source": ["Computing accuracy for the bag-of-words features on the full test set:\n"]}, {"cell_type": "code", "metadata": {"id": "EY0ofgf4gdeH"}, "source": ["from sklearn.metrics import accuracy_score\n", "accuracy_score(test_bow_labels, pred)"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "YDc84enUiqmn"}, "source": ["#### Precision"]}, {"cell_type": "code", "metadata": {"id": "jeUJBGIhbA5b"}, "source": ["from sklearn.metrics import precision_score\n", "precision_score(test_w2v_labels, pred,average='macro') "], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "CnhhydtOi1dc"}, "source": ["### Recall\n"]}, {"cell_type": "code", "metadata": {"id": "pqxhoKfTi416"}, "source": ["from sklearn.metrics import recall_score\n", "recall_score(test_bow_labels, pred,average='macro') "], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "TBn2Ken3i8ZN"}, "source": ["#### Classification Report"]}, {"cell_type": "code", "metadata": {"id": "U32PV6yNi_gv"}, "source": ["from sklearn.metrics import classification_report"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"id": "oMuACNMZjCQ0"}, "source": ["print(classification_report(test_bow_labels,pred))"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"id": "nX8DVOj-jFXw"}, "source": ["import seaborn as sns; sns.set()\n", "from sklearn.metrics import confusion_matrix\n", "mat = confusion_matrix(test_bow_labels, pred)"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"id": "0EbCIMpYjLu0"}, "source": ["dist_labels=np.asarray(list(set(test_bow_labels.flatten().astype(int))))"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "oCQum_DKjXua"}, "source": ["#### Confusion matrix"]}, {"cell_type": "code", "metadata": {"id": "nFkQTqm8jPL3"}, "source": ["sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False,\n", "            xticklabels=dist_labels,\n", "            yticklabels=dist_labels)\n", "plt.xlabel('true label')\n", "plt.ylabel('predicted label');"], "execution_count": null, "outputs": []}, {"cell_type": "markdown", "metadata": {"id": "8YIha6yMtyuu"}, "source": ["### Please answer the questions below to complete the experiment:"]}, {"cell_type": "code", "metadata": {"id": "MdwEI4hQ1rws"}, "source": ["#@title Say I have a precision of 80% and a recall of 15%. if I could modify my system so the precision is 70% but the recall is 20%, in the latter case F-score would be higher { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n", "Answer = \"\" #@param [\"\",\"True\",\"False\"]\n"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"id": "NMzKSbLIgFzQ"}, "source": ["#@title How was the experiment? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n", "Complexity = \"\" #@param [\"\",\"Too Simple, I am wasting time\", \"Good, But Not Challenging for me\", \"Good and Challenging for me\", \"Was Tough, but I did it\", \"Too Difficult for me\"]\n"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"id": "DjcH1VWSFI2l"}, "source": ["#@title If it was too easy, what more would you have liked to be added? If it was very difficult, what would you have liked to have been removed? { run: \"auto\", display-mode: \"form\" }\n", "Additional = \"\" #@param {type:\"string\"}\n"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"id": "4VBk_4VTAxCM"}, "source": ["#@title Can you identify the concepts from the lecture which this experiment covered? { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n", "Concepts = \"\" #@param [\"\",\"Yes\", \"No\"]\n"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"id": "r35isHfTVGKc"}, "source": ["#@title  Experiment walkthrough video? { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n", "Walkthrough = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"id": "XH91cL1JWH7m"}, "source": ["#@title  Text and image description/explanation and code comments within the experiment: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n", "Comments = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"id": "z8xLqj7VWIKW"}, "source": ["#@title Mentor Support: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n", "Mentor_support = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"id": "FzAZHt1zw-Y-", "cellView": "form"}, "source": ["#@title Run this cell to submit your notebook for grading { vertical-output: true }\n", "try:\n", "  if submission_id:\n", "      return_id = submit_notebook()\n", "      if return_id : submission_id = return_id\n", "  else:\n", "      print(\"Please complete the setup first.\")\n", "except NameError:\n", "  print (\"Please complete the setup first.\")"], "execution_count": null, "outputs": []}, {"cell_type": "code", "metadata": {"id": "-aR_Fn50-Pd0"}, "source": [""], "execution_count": null, "outputs": []}]}