{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Demo_Gradient_Descent.ipynb","provenance":[],"private_outputs":true,"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"uYE7gRUzFgR5"},"source":["# Advanced Certification in AIML\n","## A Program by IIIT-H and TalentSprint"]},{"cell_type":"markdown","metadata":{"id":"a3-AYqMRqHOD"},"source":["## Learning Objectives"]},{"cell_type":"markdown","metadata":{"id":"myNyvBNXVgkX"},"source":["At the end of the experiment,  you will be able to :\n","\n","* Understand the intution of Gradient Descent"]},{"cell_type":"markdown","metadata":{"id":"XsuH046p_d1N"},"source":["## Overview \n","\n","In general terms Gradient means slope or slant of a surface. So gradient descent means descending a slope to reach the lowest point on that surface. Gradient Descent aims to minimize the cost function, a function reaches its minimum value when the slope is equal to 0. Gradient descent is an iterative algorithm, that starts from a random point on a function and travels down its slope in steps until it reaches the minimum point of that function."]},{"cell_type":"markdown","metadata":{"id":"FhA-4D7SrIpo"},"source":["## Setup Steps"]},{"cell_type":"code","metadata":{"id":"4QIG-f-NZbQv"},"source":["#@title Please enter your registration id to start: { run: \"auto\", display-mode: \"form\" }\n","Id = \"2100121\" #@param {type:\"string\"}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5G1jNi-YZbQ1"},"source":["#@title Please enter your password (normally your phone number) to continue: { run: \"auto\", display-mode: \"form\" }\n","password = \"5142192291\" #@param {type:\"string\"}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QRvlfkw5rV_Q","cellView":"form"},"source":["#@title Run this cell to complete the setup for this Notebook\n","from IPython import get_ipython\n","\n","ipython = get_ipython()\n","  \n","notebook= \"Demo_Gradient_Descent\" #name of the notebook\n","Answer = \"Ungraded\"\n","def setup():\n","    from IPython.display import HTML, display\n","    display(HTML('<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId={0}&recordId={1}\"></script>'.format(getId(),submission_id)))\n","    print(\"Setup completed successfully\")\n","    return\n","\n","def submit_notebook():\n","    \n","    ipython.magic(\"notebook -e \"+ notebook + \".ipynb\")\n","    \n","    import requests, json, base64, datetime\n","\n","    url = \"https://dashboard.talentsprint.com/xp/app/save_notebook_attempts\"\n","    if not submission_id:\n","      data = {\"id\" : getId(), \"notebook\" : notebook, \"mobile\" : getPassword()}\n","      r = requests.post(url, data = data)\n","      r = json.loads(r.text)\n","\n","      if r[\"status\"] == \"Success\":\n","          return r[\"record_id\"]\n","      elif \"err\" in r:        \n","        print(r[\"err\"])\n","        return None        \n","      else:\n","        print (\"Something is wrong, the notebook will not be submitted for grading\")\n","        return None\n","\n","    elif getAnswer() and getComplexity() and getAdditional() and getConcepts():\n","      f = open(notebook + \".ipynb\", \"rb\")\n","      file_hash = base64.b64encode(f.read())\n","\n","      data = {\"complexity\" : Complexity, \"additional\" :Additional, \n","              \"concepts\" : Concepts, \"record_id\" : submission_id, \n","              \"answer\" : Answer, \"id\" : Id, \"file_hash\" : file_hash,\n","              \"feedback_experiments_input\" : Comments, \"notebook\" : notebook}\n","\n","      r = requests.post(url, data = data)\n","      r = json.loads(r.text)\n","      if \"err\" in r:        \n","        print(r[\"err\"])\n","        return None   \n","      else:\n","        print(\"Your submission is successful.\")\n","        print(\"Ref Id:\", submission_id)\n","        print(\"Date of submission: \", r[\"date\"])\n","        print(\"Time of submission: \", r[\"time\"])\n","        print(\"View your submissions: https://aiml.iiith.talentsprint.com/notebook_submissions\")\n","        # print(\"For any queries/discrepancies, please connect with mentors through the chat icon in LMS dashboard.\")\n","      return submission_id\n","    else: submission_id\n","    \n","\n","def getAdditional():\n","  try:\n","    if not Additional: \n","      raise NameError\n","    else:\n","      return Additional  \n","  except NameError:\n","    print (\"Please answer Additional Question\")\n","    return None\n","def getComments():\n","  try:\n","    if not Comments:\n","      raise NameError\n","    else:\n","      return Comments\n","  except NameError:\n","    print (\"Please answer Comments Question\")\n","    return None\n","\n","def getComplexity():\n","  try:\n","    if not Complexity:\n","      raise NameError\n","    else:\n","      return Complexity\n","  except NameError:\n","    print (\"Please answer Complexity Question\")\n","    return None\n","  \n","def getConcepts():\n","  try:\n","    if not Concepts:\n","      raise NameError\n","    else:\n","      return Concepts\n","  except NameError:\n","    print (\"Please answer Concepts Question\")\n","    return None\n","\n","def getAnswer():\n","  try:\n","    if not Answer:\n","      raise NameError \n","    else: \n","      return Answer\n","  except NameError:\n","    print (\"Please answer Question\")\n","    return None\n","\n","def getId():\n","  try: \n","    return Id if Id else None\n","  except NameError:\n","    return None\n","\n","def getPassword():\n","  try:\n","    return password if password else None\n","  except NameError:\n","    return None\n","\n","submission_id = None\n","### Setup \n","if getPassword() and getId():\n","  submission_id = submit_notebook()\n","  if submission_id:\n","    setup()\n","    \n","else:\n","  print (\"Please complete Id and Password cells before running setup\")\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UQmzuXIChorp"},"source":["#### Import required packages"]},{"cell_type":"code","metadata":{"id":"238GaEpo0MlR"},"source":["%matplotlib inline\n","import matplotlib.pyplot as plt\n","import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GuYgUJJy6GeU"},"source":["### Intution behind Gradient Descent\n","\n","Mathematical concepts before understanding the gradient descent\n","\n","* Let's take a function,  $f(x) = x^{2} + 5$ \n","\n","* Task to find the minimum value of the function i.e. to find the X value where the value of Y is minimum."]},{"cell_type":"code","metadata":{"id":"Bufkr_n2yrKe"},"source":["# Keep all the given values of x into the function and see the output values of y.\n","def func(x):\n","    return x**2 + 5\n","\n","X = np.array([-6, -5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5, 6]) # Input values\n","Y = func(X) # Call the function to get the y value\n","\n","print(\"X values: \" , X)\n","print(\"f(x) values: \", Y)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-RnaLCsM1Qtz"},"source":["Now let's plot the function $f(x) = x^{2} + 5$ for the given values of X"]},{"cell_type":"code","metadata":{"id":"JZ_5xJTR1m9h"},"source":["# Plotting x and y values\n","plt.plot(X, Y, marker='o',color='b',linestyle='-');\n","plt.plot(X, Y, 'o', color='r');"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Lp5no7O9A0zI"},"source":["### Gradient Descent \n","\n","It is an iterative optimization algorithm that finds the minimum value of a function.\n","\n","* To find the gradient of above function $f(x) = x^{2} + 5$ , differentiate the function with respect to  x \n","\n","To find the derivative of the function, we use Sympy package to make symbol of x.  \n","SymPy is a Python library for symbolic mathematics\n","\n","To know more about Sympy refer [link](https://docs.sympy.org/latest/tutorial/gotchas.html#symbols)"]},{"cell_type":"code","metadata":{"id":"eDXKJTeq1qAU"},"source":["# Use sympy package from python to find the derivative of a function\n","import sympy as sp\n","\n","x_sym = sp.Symbol('x')\n","\n","f = x_sym**2 + 5\n","f_derivative = f.diff(x_sym)\n","\n","print(\"Derivative of f(x): \", f_derivative)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9MGhWQICrK4h"},"source":["### How to converge the gradient ?\n","\n","* Where to start ?\n","\n","* In which direction to move?\n","\n","* How to reach the minimum point?\n","\n","\n","The general idea is to start with a random starting point X, the Gradient of a function gives always the direction of greatest rate of increase, and move towards the negative direction which minimizes the value of the function.\n"]},{"cell_type":"markdown","metadata":{"id":"DdTIiiflH6iX"},"source":["### The steps of the Gradient Descent algorithm \n","\n","1. Compute the gradient of the function (first order derivative of the function)\n","\n","2. Start from the random point by choosing learning rate **eta** and the no.of iterations\n","\n","3. Update the gradient and move towards negative slope to reach the minimum point."]},{"cell_type":"markdown","metadata":{"id":"hMUYPmgSSNlh"},"source":["The **learning rate** mentioned above is a parameter which  influences the convergence of the algorithm. Larger learning rates make the algorithm take huge steps down the slope and it might jump across the minimum point thereby missing it. So, it is always good to stick to low learning rates\n"]},{"cell_type":"code","metadata":{"id":"11-C1DsL1-io"},"source":["# Below function to converge the gradient\n","def gradient_converge(eta , gradient_x, iterations):\n","  for i in range(0,iterations):\n","      \n","      # The derivative is the rate of change or the slope of a function at a given point\n","      deriv_x = 2 * gradient_x  \n","\n","      # Calculating the gradient    \n","      gradient_x = gradient_x - eta * deriv_x \n","  return gradient_x"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DEw2JGD91TBd"},"source":["Set the parameters for the gradient converge with iterations = 100, eta = 0.001"]},{"cell_type":"code","metadata":{"id":"2nYtA_Dr0-xo"},"source":["# Specify the number of iterations for the process to repeat.\n","iterations = 100    \n","\n","# Set an initial value, to start with\n","Initial_point = 6 \n","\n","# Learning rate  \n","eta = 0.001         "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SeXO4jUM1GRq"},"source":["# Calling the gradient_converge() function\n","gradient_x = gradient_converge(eta, Initial_point, iterations)\n","print(\"iterations = \",iterations,\"\\ngradient_x = \",gradient_x)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"v3U-9ge4sMDX"},"source":["### Visualization of the Gradient Convergence for the above parameters"]},{"cell_type":"code","metadata":{"id":"-jvqRt6Tq31f"},"source":["plt.plot(X, Y, marker='o',color='b',linestyle='-');\n","plt.plot(X, Y, 'o', color='r');\n","y_gradient = func(gradient_x)\n","plt.plot(gradient_x, y_gradient,'ko',  markersize = 10)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"c-dzU8Nu-Nv0"},"source":["From the above graph, with iterations = 100, eta = 0.001 the gradient has moved down to point 5, which is not the minimum point."]},{"cell_type":"markdown","metadata":{"id":"u-6ystdJ9BPk"},"source":["Now let's try changing the parameters for the gradient converge with, iterations = 5000, eta = 0.001 "]},{"cell_type":"code","metadata":{"id":"FIhjpzH19BPn"},"source":["# Change the no.of iterations to 5000\n","iterations = 5000    \n","\n","# Start point\n","Initial_point = 6 \n","\n","# Learning rate\n","eta = 0.001         "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dysw7KeH9BPu"},"source":["# Calling the gradient_converge() function\n","gradient_x = gradient_converge(eta, Initial_point, iterations)\n","print(\"iterations = \",iterations,\"\\ngradient_x = \",gradient_x)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eVmIPLWc97tE"},"source":["### Visualize the Gradient Convergence for the changed parameters"]},{"cell_type":"code","metadata":{"id":"nL9OP4pq9fpk"},"source":["plt.plot(X, Y, marker='o',color='b',linestyle='-');\n","plt.plot(X, Y, 'o', color='r');\n","y_gradient = func(gradient_x)\n","plt.plot(gradient_x, y_gradient,'ko',  markersize = 10)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3gb6jKI6Br1M"},"source":["From the above graph, observe that after 5000 iterations the gradient reaches the minimum point in the plot which is very close to  0 "]},{"cell_type":"markdown","metadata":{"id":"4YlpByIaDqcf"},"source":["Also try with larger learning rates and observe the change in how the gradient converges quickly with large steps and reaches the minimum point"]},{"cell_type":"markdown","metadata":{"id":"tC2rt1ZxrgC7"},"source":["## Please answer the questions below to complete the experiment:"]},{"cell_type":"code","metadata":{"id":"NMzKSbLIgFzQ"},"source":["#@title How was the experiment? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n","Complexity = \"Good, But Not Challenging for me\" #@param [\"\",\"Too Simple, I am wasting time\", \"Good, But Not Challenging for me\", \"Good and Challenging for me\", \"Was Tough, but I did it\", \"Too Difficult for me\"]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DjcH1VWSFI2l"},"source":["#@title If it was too easy, what more would you have liked to be added? If it was very difficult, what would you have liked to have been removed? { run: \"auto\", display-mode: \"form\" }\n","Additional = \"none\" #@param {type:\"string\"}\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_cTetkuegP7d"},"source":["#@title Can you identify the concepts from the lecture which this experiment covered? { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n","Concepts = \"Yes\" #@param [\"\",\"Yes\", \"No\"]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QFQw0ddId_Ej"},"source":["#@title  Text and image description/explanation and code comments within the experiment: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n","Comments = \"Very Useful\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-CXztFuygSBG","cellView":"form"},"source":["#@title Run this cell to submit your notebook  { vertical-output: true }\n","try:\n","  if submission_id:\n","      return_id = submit_notebook()\n","      if return_id : submission_id =return_id\n","  else:\n","      print(\"Please complete the setup first.\")\n","except NameError:\n","  print (\"Please complete the setup first.\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kdhTawG0rMEq"},"source":[""],"execution_count":null,"outputs":[]}]}