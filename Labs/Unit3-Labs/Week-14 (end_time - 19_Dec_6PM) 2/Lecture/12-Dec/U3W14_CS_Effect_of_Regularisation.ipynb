{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"U3W14_CS_Effect_of_Regularisation.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"-zSWyuZTlYS1"},"source":["\n","# Advanced Certification in AIML\n","## A Program by IIIT-H and TalentSprint"]},{"cell_type":"markdown","metadata":{"id":"kRUhG-LzJux_"},"source":["### Not for Grading"]},{"cell_type":"markdown","metadata":{"id":"Iex1YUs9JyoD"},"source":["## Effect of Regularization"]},{"cell_type":"code","metadata":{"id":"MbPI1X5gJ3fH","cellView":"form"},"source":["#@title Case Study Walkthrough\n","#@markdown  Effect of Regularization\n","from IPython.display import HTML\n","\n","HTML(\"\"\"<video width=\"320\" height=\"240\" controls>\n","  <source src=\"https://cdn.talentsprint.com/talentsprint/archives/sc/aiml/aiml_2018_b7_hyd/preview_videos/effects_of_regularization.mp4\">\n","</video>\n","\"\"\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oU2MBx7JtTyP"},"source":["## Learning Objectives"]},{"cell_type":"markdown","metadata":{"id":"Ot3pfe3AtWaN"},"source":["At the end of the experiment, you will be able to:\n","\n","\n","* Understand Bias and Variance    \n","* Know how does the degree of the polynomial affect the bias and variance"]},{"cell_type":"markdown","metadata":{"id":"35RBpSDUtYUm"},"source":["## Dataset"]},{"cell_type":"markdown","metadata":{"id":"SVIILnxWtcKA"},"source":["### Description\n","\n","In this experiment, we have chosen sine curve real data.  As the real-world is never perfectly clean however, we add noise to the curve to show that the real-world data is noisy. This is done by adding a small random number to each value. \n"]},{"cell_type":"markdown","metadata":{"id":"fi4X52VouBL4"},"source":["## AI /ML Technique"]},{"cell_type":"markdown","metadata":{"id":"lgAparrouDiQ"},"source":["In this experiment, we use the sine curve to understand how the change in bias and variance affects the degree of a polynomial.\n","\n","\n","### Bias and Variance:\n","\n","The **bias** is an error from erroneous assumptions in the learning algorithm. High bias can cause an algorithm to miss the relevant relations between features and target outputs (underfitting). \n","\n","The **variance** is an error from sensitivity to small fluctuations in the training set. High variance can cause an algorithm to model the random noise in the training data, rather than the intended outputs (overfitting). The below image shows how the overfitting and underfitting looks.\n","\n","![alt text](https://cdn.talentsprint.com/aiml/Experiment_related_data/IMAGES/Overfitting.png)\n","\n","\n","\n","\n","You can use any polynomial of higher order to project the idea of bias and variance. However, the sine values suits our experiment better because, it is a curve which is complex enough not to fit with simple linear or quadratic equations (to show bias) however it will overfit our data with higher order polynomials (6th order).\n","\n","We want to try and capture the data using a polynomial function. A polynomial is defined by the degree, or the highest power for the x-values. A line has a degree of 1 because it is of the form $y = m_1*x + c$ where $m$ is the slope and $c$ is the intercept. A third degree polynomial would have the form $y = m_3 * x^3 + m_2 * x^2 + m_1* x + c$ and so on. The higher the degree of the polynomial, the more flexible the model.\n","\n","\n","We use fit_poly() to create a polynomial function with the specified number of degrees and plots the results. We can use these results to determine the optimal degrees to achieve the right balance between overfitting and underfitting. \n","\n","\n","In this experiment we generate a  dataset to observe how the model changes with changing degrees.\n","  1. We will see how the model influences the performance. \n","  2. Estimate the errors."]},{"cell_type":"markdown","metadata":{"id":"iiXPGTW_lgPr"},"source":["### Setup Steps"]},{"cell_type":"code","metadata":{"id":"8RvR2Y2DFk6w"},"source":["#@title Please enter your registration id to start:  { run: \"auto\", display-mode: \"form\" }\n","Id = \"\" #@param {type:\"string\"}\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_9A39Um0FlCl"},"source":["#@title Please enter your password (normally your phone number) to continue: { run: \"auto\", display-mode: \"form\" }\n","password = \"\" #@param {type:\"string\"}\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QRvlfkw5rV_Q","cellView":"form"},"source":["#@title Run this cell to complete the setup for this Notebook\n","from IPython import get_ipython\n","\n","ipython = get_ipython()\n","  \n","notebook= \"U3W14_CS_Effect_of_Regularisation\" #name of the notebook\n","Answer = \"Ungraded\"\n","def setup():\n","   \n","    from IPython.display import HTML, display\n","    display(HTML('<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId={0}&recordId={1}\"></script>'.format(getId(),submission_id)))\n","    print(\"Setup completed successfully\")\n","    return\n","\n","def submit_notebook():\n","    \n","    ipython.magic(\"notebook -e \"+ notebook + \".ipynb\")\n","    \n","    import requests, json, base64, datetime\n","\n","    url = \"https://dashboard.talentsprint.com/xp/app/save_notebook_attempts\"\n","    if not submission_id:\n","      data = {\"id\" : getId(), \"notebook\" : notebook, \"mobile\" : getPassword()}\n","      r = requests.post(url, data = data)\n","      r = json.loads(r.text)\n","\n","      if r[\"status\"] == \"Success\":\n","          return r[\"record_id\"]\n","      elif \"err\" in r:        \n","        print(r[\"err\"])\n","        return None        \n","      else:\n","        print (\"Something is wrong, the notebook will not be submitted for grading\")\n","        return None\n","\n","    elif getAnswer() and getComplexity() and getAdditional() and getConcepts():\n","      f = open(notebook + \".ipynb\", \"rb\")\n","      file_hash = base64.b64encode(f.read())\n","\n","      data = {\"complexity\" : Complexity, \"additional\" :Additional, \n","              \"concepts\" : Concepts, \"record_id\" : submission_id, \n","              \"answer\" : Answer, \"id\" : Id, \"file_hash\" : file_hash,\n","              \"feedback_experiments_input\" : Comments, \"notebook\" : notebook}\n","\n","      r = requests.post(url, data = data)\n","      r = json.loads(r.text)\n","      if \"err\" in r:        \n","        print(r[\"err\"])\n","        return None   \n","      else:\n","        print(\"Your submission is successful.\")\n","        print(\"Ref Id:\",submission_id)\n","        print(\"Date of submission: \",r[\"date\"])\n","        print(\"Time of submission: \", r[\"time\"])\n","        print(\"View your submissions: https://aiml.iiith.talentsprint.com/notebook_submissions\")\n","        # print(\"For any queries/discrepancies, please connect with mentors through the chat icon in LMS dashboard.\")\n","      return submission_id\n","    else: submission_id\n","    \n","\n","def getAdditional():\n","  try:\n","    if not Additional: \n","      raise NameError\n","    else:\n","      return Additional  \n","  except NameError:\n","    print (\"Please answer Additional Question\")\n","    return None\n","def getComments():\n","  try:\n","    if not Comments:\n","      raise NameError\n","    else:\n","      return Comments\n","  except NameError:\n","    print (\"Please answer Comments Question\")\n","    return None\n","\n","def getComplexity():\n","  try:\n","    if not Complexity:\n","      raise NameError\n","    else:\n","      return Complexity\n","  except NameError:\n","    print (\"Please answer Complexity Question\")\n","    return None\n","  \n","def getConcepts():\n","  try:\n","    if not Concepts:\n","      raise NameError\n","    else:\n","      return Concepts\n","  except NameError:\n","    print (\"Please answer Concepts Question\")\n","    return None\n","\n","def getAnswer():\n","  try:\n","    if not Answer:\n","      raise NameError \n","    else: \n","      return Answer\n","  except NameError:\n","    print (\"Please answer Question\")\n","    return None\n","\n","def getId():\n","  try: \n","    return Id if Id else None\n","  except NameError:\n","    return None\n","\n","def getPassword():\n","  try:\n","    return password if password else None\n","  except NameError:\n","    return None\n","\n","submission_id = None\n","### Setup \n","if getPassword() and getId():\n","  submission_id = submit_notebook()\n","  if submission_id:\n","    setup()\n","    \n","else:\n","  print (\"Please complete Id and Password cells before running setup\")\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GEeo3n0ElYS_"},"source":["**Importing the required packages**"]},{"cell_type":"code","metadata":{"id":"UFmT3W9flYTD"},"source":["import numpy as np\n","import pandas as pd\n","# Scikit-Learn for fitting models\n","from sklearn.preprocessing import PolynomialFeatures\n","from sklearn.metrics import mean_squared_error\n","from sklearn.linear_model import LinearRegression\n","from sklearn.linear_model import Ridge\n","from sklearn.linear_model import Lasso\n","\n","# For plotting \n","import matplotlib\n","import matplotlib.pyplot as plt"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"k-Ds7YIulYTI"},"source":["**Generating the Data**"]},{"cell_type":"markdown","metadata":{"id":"68Gxwp_TlYTK"},"source":["We define a curve, in this case a sine curve to serve as our process that generates the data. As the real-world is never perfectly clean however, we also need to add some noise into the observations. This is done by adding a small random number to each value."]},{"cell_type":"code","metadata":{"id":"zs-y6GqqlYTK"},"source":["#Set the random seed for reproducible results\n","np.random.seed(42)\n","\n","#generating function representing a process in real life\n","def true_gen(x):\n","    y = np.sin(1.2 * x * np.pi) \n","    return(y)\n","\n","# x values and y value with a small amount of random noise\n","x = np.sort(np.random.rand(120))\n","y = true_gen(x) + 0.1 * np.random.randn(len(x))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SwMjhoOflYTO"},"source":["**Training and Testing**"]},{"cell_type":"code","metadata":{"id":"z_HykToalYTQ"},"source":["# Random indices for creating training and testing sets\n","random_ind = np.random.choice(list(range(120)), size = 120, replace=False)\n","x_t = x[random_ind]\n","y_t = y[random_ind]\n","\n","# Training and testing observations\n","train = x_t[:int(0.7 * len(x))]\n","test = x_t[int(0.7 * len(x)):]\n","\n","y_train = y_t[:int(0.7 * len(y))]\n","y_test = y_t[int(0.7 * len(y)):]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ze9xVifrlYTT"},"source":["**Visualization**"]},{"cell_type":"code","metadata":{"id":"IkvhKsRwlYTV"},"source":["# Visualize observations of train data\n","plt.plot(train, y_train, 'ko', label = 'Train'); \n","plt.legend()\n","plt.xlabel('x'); plt.ylabel('y'); plt.title('Data');\n","plt.savefig('just_data.png')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ulD5-Y31lYTc"},"source":["**Polynomial Model**"]},{"cell_type":"markdown","metadata":{"id":"VRxbemV4lYTd"},"source":["We want to try and capture the data using a polynomial function. A polynomial is defined by the degree, or the highest power for the x-values. A line has a degree of 1 because it is of the form $y = m_1*x + c$ where $m$ is the slope and $c$ is the intercept. A third degree polynomial would have the form $y = m_3 * x^3 + m_2 * x^2 + m_1* x + c$ and so on. The higher the degree of the polynomial, the more flexible the model.\n","\n","The following function creates a polynomial with the specified number of degrees and plots the results. We can use these results to determine the optimal degrees to achieve the right balance between over and underfitting."]},{"cell_type":"code","metadata":{"id":"kNMZTgcwlYTe"},"source":["def fit_poly(train, y_train, test, y_test, degrees, plot='train', regularization='L', return_scores=False):\n","    \n","    # Create a polynomial transformation of features\n","    features = PolynomialFeatures(degree=degrees, include_bias=False)\n","    \n","    # Reshape training features for use in scikit-learn and transform features\n","    train = train.reshape((-1, 1))\n","    train_trans = features.fit_transform(train)\n","    \n","    # Create the linear regression model and train\n","    if regularization=='L':\n","      model = LinearRegression()\n","      model.fit(train_trans, y_train)\n","    \n","    elif regularization=='L1':\n","      model = Lasso(alpha=0.00125, fit_intercept=True)\n","      model.fit(train_trans, y_train)\n","    \n","    elif regularization=='L2':\n","      model = Ridge(alpha=0.00125, fit_intercept=True)\n","      model.fit(train_trans, y_train)\n","    \n","    # Train set predictions and error\n","    train_predictions = model.predict(train_trans)\n","    training_error = mean_squared_error(y_train, train_predictions) # Format test features\n","    test = test.reshape((-1, 1))\n","    test_trans = features.fit_transform(test)\n","    \n","    # Test set predictions and error\n","    test_predictions = model.predict(test_trans)\n","    testing_error = mean_squared_error(y_test, test_predictions)\n","    \n","    # Find the model curve and the true curve\n","    x_curve = np.linspace(0, 1, 100)\n","    x_curve = x_curve.reshape((-1, 1))\n","    x_curve_trans = features.fit_transform(x_curve)\n","    \n","    # Model curve\n","    model_curve = model.predict(x_curve_trans)\n","    \n","    # True curve\n","    y_true_curve = true_gen(x_curve[:, 0])\n","    \n","    # Plot observations, true function, and model predicted function\n","    if plot == 'train':\n","        plt.plot(train[:, 0], y_train, 'ko', label = 'Observations')\n","        plt.plot(x_curve[:, 0], y_true_curve, linewidth = 4, label = 'True Function')\n","        plt.plot(x_curve[:, 0], model_curve, linewidth = 4, label = 'Model Function')\n","        plt.xlabel('x'); plt.ylabel('y')\n","        plt.legend()\n","        plt.ylim(-1, 1.5); plt.xlim(0, 1)\n","        plt.title('{} Degree Model on Training Data'.format(degrees))\n","        plt.savefig('{}_regularization_{}_degrees.png'.format(regularization, str(degrees)))\n","        plt.show()\n","        \n","    elif plot == 'test':\n","        # Plot the test observations and test predictions\n","        plt.plot(test, y_test, 'o', label = 'Test Observations')\n","        plt.plot(x_curve[:, 0], y_true_curve, 'b-', linewidth = 2, label = 'True Function')\n","        plt.plot(test, test_predictions, 'ro', label = 'Test Predictions')\n","        plt.ylim(-1, 1.5); plt.xlim(0, 1)\n","        plt.legend(), plt.xlabel('x'), plt.ylabel('y'); plt.title('{} Degree Model on Testing Data'.format(degrees)), plt.show();\n","    \n","     # Return the metrics\n","    if return_scores:\n","        return training_error, testing_error\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tAD-3BK3lYTj"},"source":["**Model with Different Degrees** "]},{"cell_type":"markdown","metadata":{"id":"t7j_cmzwlYTk"},"source":["Degrees = 1 -> Underfitting"]},{"cell_type":"markdown","metadata":{"id":"B2UE_bJ4lYTl"},"source":["For example, a degree-1 polynomial fits a straight line to the data. In this case a linear model cannot accurately learn the relationship between x and y so it will underfit the data. This is because an underfit model has low variance and high bias. Variance refers to how much the model is dependent on the training data."]},{"cell_type":"code","metadata":{"id":"0rbtAKQxlYTn"},"source":["fit_poly(train, y_train, test, y_test, degrees = 1, regularization='L', plot='train')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VTuUY31BlYT0"},"source":["**Degrees = 25 -> Overfitting**\n"," An overfit model will have extremely low training error but a high testing error."]},{"cell_type":"markdown","metadata":{"id":"oxXVQ4cYlYT0"},"source":["We can go in the completely opposite direction and create a model that overfits the data. This model has too much flexibility and learns the training data too closely. As the training data has some amount of noise, it will end up capturing that noise and will be misled by that noise when it tries to make predictions on the test data."]},{"cell_type":"markdown","metadata":{"id":"_ifkNSyElYT2"},"source":["This is a model with a high variance, because it will change significantly depending on the training data."]},{"cell_type":"markdown","metadata":{"id":"tdi-a0ZQkW0X"},"source":["### Fitting the polynomial features without any regularization"]},{"cell_type":"code","metadata":{"id":"bkf8acYGlYT3"},"source":["fit_poly(train, y_train, test, y_test, degrees = 25, regularization='L', plot='train')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hrVaT6Bfkg5E"},"source":["### Fitting the polynomial features with L1 regularization"]},{"cell_type":"code","metadata":{"id":"pRohDz2Tjmgj"},"source":["fit_poly(train, y_train, test, y_test, degrees = 25, regularization='L1', plot='train')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8JgBnXEQkkx2"},"source":["### Fitting the polynomial features with L2 regularization"]},{"cell_type":"code","metadata":{"id":"OUh0FBGzju3p"},"source":["fit_poly(train, y_train, test, y_test, degrees = 25, regularization='L2', plot='train')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tC2rt1ZxrgC7"},"source":["## Please answer the questions below to complete the experiment:"]},{"cell_type":"code","metadata":{"id":"NMzKSbLIgFzQ"},"source":["#@title How was the experiment? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n","Complexity = \"\" #@param [\"\",\"Too Simple, I am wasting time\", \"Good, But Not Challenging for me\", \"Good and Challenging for me\", \"Was Tough, but I did it\", \"Too Difficult for me\"]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DjcH1VWSFI2l"},"source":["#@title If it was too easy, what more would you have liked to be added? If it was very difficult, what would you have liked to have been removed? { run: \"auto\", display-mode: \"form\" }\n","Additional = \"\" #@param {type:\"string\"}\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_cTetkuegP7d"},"source":["#@title Can you identify the concepts from the lecture which this experiment covered? { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n","Concepts = \"\" #@param [\"\",\"Yes\", \"No\"]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QFQw0ddId_Ej"},"source":["#@title  Text and image description/explanation and code comments within the experiment: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n","Comments = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-CXztFuygSBG","cellView":"form"},"source":["#@title Run this cell to submit your notebook  { vertical-output: true }\n","try:\n","  if submission_id:\n","      return_id = submit_notebook()\n","      if return_id : submission_id =return_id\n","  else:\n","      print(\"Please complete the setup first.\")\n","except NameError:\n","  print (\"Please complete the setup first.\")"],"execution_count":null,"outputs":[]}]}