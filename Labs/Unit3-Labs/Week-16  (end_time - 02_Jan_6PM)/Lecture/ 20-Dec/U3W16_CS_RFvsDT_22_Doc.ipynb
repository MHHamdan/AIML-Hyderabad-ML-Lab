{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"U3W16_CS_RFvsDT_22_Doc.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"y9NNhMb5wbUK"},"source":["# Advanced Certification in AIML\n","## A Program by IIIT-H and TalentSprint"]},{"cell_type":"markdown","metadata":{"id":"rWVhm_c8OdlE"},"source":["###Not for grading"]},{"cell_type":"markdown","metadata":{"id":"Ttg_PKFibkkQ"},"source":["## Random Forests vs Decision Trees on 20 class Documents data"]},{"cell_type":"code","metadata":{"id":"YFiSrzJDbvXM","cellView":"form","executionInfo":{"status":"ok","timestamp":1608452510345,"user_tz":300,"elapsed":422,"user":{"displayName":"Mohammed Hamdan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh7-0C4n1c_uDVPoywaYa91Jx17qP1YRlmJadqc=s64","userId":"00647759825092258022"}},"outputId":"0a100014-5cc8-4de4-e9f8-5c50d41d7295","colab":{"base_uri":"https://localhost:8080/","height":262}},"source":["#@title Case Study Walkthrough\n","#@markdown  Random Forests vs Decision Trees on 20 class Documents data\n","from IPython.display import HTML\n","\n","HTML(\"\"\"<video width=\"320\" height=\"240\" controls>\n","  <source src=\"https://cdn.talentsprint.com/talentsprint/archives/sc/aiml/aiml_2018_b7_hyd/preview_videos/rf_vs_dt.mp4\">\n","</video>\n","\"\"\")"],"execution_count":1,"outputs":[{"output_type":"execute_result","data":{"text/html":["<video width=\"320\" height=\"240\" controls>\n","  <source src=\"https://cdn.talentsprint.com/talentsprint/archives/sc/aiml/aiml_2018_b7_hyd/preview_videos/rf_vs_dt.mp4\">\n","</video>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]},"execution_count":1}]},{"cell_type":"markdown","metadata":{"id":"xhhKkt8fIu0K"},"source":["### Setup Steps:\n"]},{"cell_type":"code","metadata":{"id":"tCXazX5-OJsx"},"source":["#@title Please enter your registration id to start: (e.g. P181900101) { run: \"auto\", display-mode: \"form\" }\n","Id = \"\" #@param {type:\"string\"}\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tVXPzEYkO1z2"},"source":["#@title Please enter your password (normally your phone number) to continue: { run: \"auto\", display-mode: \"form\" }\n","password = \"\" #@param {type:\"string\"}\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QRvlfkw5rV_Q","cellView":"form"},"source":["#@title Run this cell to complete the setup for this Notebook\n","from IPython import get_ipython\n","\n","ipython = get_ipython()\n","  \n","notebook= \"U3W16_CS_RFvsDT_22_Doc\" #name of the notebook\n","Answer = \"Ungraded\"\n","def setup():\n","    ipython.magic(\"sx wget https://cdn.talentsprint.com/aiml/Experiment_related_data/AIML_DS_NEWSGROUPS_PICKELFILE.pkl\")\n","    from IPython.display import HTML, display\n","    display(HTML('<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId={0}&recordId={1}\"></script>'.format(getId(),submission_id)))\n","    print(\"Setup completed successfully\")\n","    return\n","\n","def submit_notebook():\n","    \n","    ipython.magic(\"notebook -e \"+ notebook + \".ipynb\")\n","    \n","    import requests, json, base64, datetime\n","\n","    url = \"https://dashboard.talentsprint.com/xp/app/save_notebook_attempts\"\n","    if not submission_id:\n","      data = {\"id\" : getId(), \"notebook\" : notebook, \"mobile\" : getPassword()}\n","      r = requests.post(url, data = data)\n","      r = json.loads(r.text)\n","\n","      if r[\"status\"] == \"Success\":\n","          return r[\"record_id\"]\n","      elif \"err\" in r:        \n","        print(r[\"err\"])\n","        return None        \n","      else:\n","        print (\"Something is wrong, the notebook will not be submitted for grading\")\n","        return None\n","\n","    elif getAnswer() and getComplexity() and getAdditional() and getConcepts():\n","      f = open(notebook + \".ipynb\", \"rb\")\n","      file_hash = base64.b64encode(f.read())\n","\n","      data = {\"complexity\" : Complexity, \"additional\" :Additional, \n","              \"concepts\" : Concepts, \"record_id\" : submission_id, \n","              \"answer\" : Answer, \"id\" : Id, \"file_hash\" : file_hash,\n","              \"feedback_experiments_input\" : Comments, \"notebook\" : notebook}\n","\n","      r = requests.post(url, data = data)\n","      r = json.loads(r.text)\n","      if \"err\" in r:        \n","        print(r[\"err\"])\n","        return None   \n","      else:\n","        print(\"Your submission is successful.\")\n","        print(\"Ref Id:\",submission_id)\n","        print(\"Date of submission: \",r[\"date\"])\n","        print(\"Time of submission: \", r[\"time\"])\n","        print(\"View your submissions: https://aiml.iiith.talentsprint.com/notebook_submissions\")\n","        # print(\"For any queries/discrepancies, please connect with mentors through the chat icon in LMS dashboard.\")\n","      return submission_id\n","    else: submission_id\n","    \n","\n","def getAdditional():\n","  try:\n","    if not Additional: \n","      raise NameError\n","    else:\n","      return Additional  \n","  except NameError:\n","    print (\"Please answer Additional Question\")\n","    return None\n","def getComments():\n","  try:\n","    if not Comments:\n","      raise NameError\n","    else:\n","      return Comments\n","  except NameError:\n","    print (\"Please answer Comments Question\")\n","    return None\n","\n","def getComplexity():\n","  try:\n","    if not Complexity:\n","      raise NameError\n","    else:\n","      return Complexity\n","  except NameError:\n","    print (\"Please answer Complexity Question\")\n","    return None\n","  \n","def getConcepts():\n","  try:\n","    if not Concepts:\n","      raise NameError\n","    else:\n","      return Concepts\n","  except NameError:\n","    print (\"Please answer Concepts Question\")\n","    return None\n","\n","def getAnswer():\n","  try:\n","    if not Answer:\n","      raise NameError \n","    else: \n","      return Answer\n","  except NameError:\n","    print (\"Please answer Question\")\n","    return None\n","\n","def getId():\n","  try: \n","    return Id if Id else None\n","  except NameError:\n","    return None\n","\n","def getPassword():\n","  try:\n","    return password if password else None\n","  except NameError:\n","    return None\n","\n","submission_id = None\n","### Setup \n","if getPassword() and getId():\n","  submission_id = submit_notebook()\n","  if submission_id:\n","    setup()\n","    \n","else:\n","  print (\"Please complete Id and Password cells before running setup\")\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Jw6cAnS4OW2U"},"source":["##Importing packages"]},{"cell_type":"code","metadata":{"id":"H9sC9--9wbUq"},"source":["# Importing required Packages\n","import pickle\n","import re\n","import operator\n","from collections import defaultdict\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import math\n","import collections\n","import gensim\n","from nltk import ngrams\n","import warnings\n","warnings.filterwarnings('ignore')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JEhKu_rjwbUt"},"source":["# Loading the dataset\n","\n","dataset = pickle.load(open('AIML_DS_NEWSGROUPS_PICKELFILE.pkl','rb'))\n","print(dataset.keys())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZF9S3GYSwbUx"},"source":["# Print frequencies of dataset\n","print(\"Class : count\")\n","print(\"--------------\")\n","number_of_documents = 0\n","for key in dataset:\n","    print(key, ':', len(dataset[key]))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gJDnkxkmwbU1"},"source":["Next, let us split our dataset which consists of 1000 samples per class, into training and test sets. We use 950 samples from each class in the training set, and the remaining 50 in the test set. \n","\n","As a mental exercise you should try reasoning about why is it important to ensure a nearly equal distribution of classes in your training and test sets. "]},{"cell_type":"code","metadata":{"id":"u6GK60NEwbU2"},"source":["train_set = {}\n","test_set = {}\n","\n","# Clean dataset for text encoding issues :- Very useful when dealing with non-unicode characters\n","for key in dataset:\n","    dataset[key] = [[i.decode('utf-8', errors='replace').lower() for i in f] for f in dataset[key]]\n","    \n","# Break dataset into 95-5 split for training and testing\n","n_train = 0\n","n_test = 0\n","for k in dataset:\n","    split = int(0.95*len(dataset[k]))\n","    train_set[k] = dataset[k][0:split]\n","    test_set[k] = dataset[k][split:-1]\n","    n_train += len(train_set[k])\n","    n_test += len(test_set[k])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"afd4qfAEwbU6"},"source":["## 1. Bag-of-Words\n","\n","Let us begin our journey into text classification with one of the simplest but most commonly used feature representations for news documents - Bag-of-Words.\n","\n","As you might have realized, machine learning algorithms need good feature representations of different inputs.  Concretely, we would like to represent each news article $D$ in terms of a feature vector $V$, which can be used for classification. Feature vector $V$ is made up of the number of occurrences of each word in the vocabulary.\n","\n","Let us begin by counting the number of occurrences of every word in the news documents in the training set."]},{"cell_type":"markdown","metadata":{"id":"OACynazXwbU7"},"source":["### 1.1 Word frequency"]},{"cell_type":"markdown","metadata":{"id":"zWgzZVqYwbU8"},"source":["Let us try understanding the kind of words that appear frequently, and those that occur rarely. We now count the frequencies of words:"]},{"cell_type":"code","metadata":{"id":"9kyqVjvBWLdK"},"source":["def frequency_words(train_set):\n","    frequency = defaultdict(int)\n","\n","    for key in train_set:\n","        for f in train_set[key]:\n","            # Find all words which consist only of capital and lowercase characters and are between the length of 2-9.\n","            # We ignore all special characters such as !.$ and words containing numbers\n","            words = re.findall(r'(\\b[A-Za-z][a-z]{2,9}\\b)', ' '.join(f))\n","            for word in words:\n","                frequency[word] += 1\n","    return frequency"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AdILkwjHwbU9"},"source":["frequency_of_words = frequency_words(train_set)\n","\n","sorted_words = sorted(frequency_of_words.items(), key=operator.itemgetter(1), reverse=True)\n","print(\"Top-10 most frequent words:\")\n","for word in sorted_words[:10]:\n","    print(word)\n","\n","print('----------------------------')\n","print(\"10 least frequent words:\")\n","for word in sorted_words[-10:-1]:\n","    print(word)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YxZqnO8GwbVB"},"source":["Next, we attempt to plot a histogram of the counts of various words in descending order. \n","\n","Could you comment on the relationship between the frequency of the most frequent word to the second frequent word? \n","And what about the third most frequent word?\n","\n","(Hint - Check the relative frequencies of the first, second and third most frequent words)\n","\n","(After answering, you can visit https://en.wikipedia.org/wiki/Zipf%27s_law for further Reading)"]},{"cell_type":"code","metadata":{"id":"kyWfgN4qwbVC"},"source":["%matplotlib inline  \n","\n","fig = plt.figure()\n","fig.set_size_inches(20,10)\n","\n","plt.bar(range(len(sorted_words[:100])), [v for k, v in sorted_words[:100]] , align='center')\n","plt.xticks(range(len(sorted_words[:100])), [k for k, v in sorted_words[:100]])\n","locs, labels = plt.xticks()\n","plt.setp(labels, rotation=90)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yvcU4VDkwbVE"},"source":["### 1.2 Pre-processing to remove most and least frequent words\n","\n","We can see that different words appear at different frequencies.\n","\n","The most common words appear in almost all documents. Hence, for a classification task, having information about those words' frequencies does not matter much since they appear frequently in every type of document. To get a good feature representation, we eliminate them since they do not add too much value.\n","\n","Additionally, notice how the least frequent words appear so rarely that they might not be useful either.\n","\n","Let us pre-process our news articles now to remove the most frequent and least frequent words by thresholding their counts: "]},{"cell_type":"code","metadata":{"id":"TrBX6IOSaofH"},"source":["def cleaning_vocabulary_words(list_of_grams):\n","    valid_words = defaultdict(int)\n","\n","    print('Number of words before preprocessing:', len(list_of_grams))\n","\n","      # Ignore the 25 most frequent words, and the words which appear less than 100 times\n","    ignore_most_frequent = 25\n","    freq_thresh = 100\n","    feature_number = 0\n","    for word, word_frequency in list_of_grams[ignore_most_frequent:]:\n","        if word_frequency > freq_thresh:\n","            valid_words[word] = feature_number\n","            feature_number += 1\n","        elif '_' in word:\n","            valid_words[word] = feature_number\n","            feature_number += 1\n","\n","    print('Number of words after preprocessing:', len(valid_words))\n","\n","    vector_size = len(valid_words)\n","    return valid_words, vector_size"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ztxnNsH8bVQV"},"source":["valid_words, number_of_words = cleaning_vocabulary_words(sorted_words)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OL7KDD5nwbVG"},"source":["### 1.3 Bag-of-Words representation\n","\n","The simplest way to represent a document $D$ as a vector $V$ would be to now count the relevant words in the document. \n","\n","For each document, make a vector of the count of each of the words in the vocabulary (excluding the words removed in the previous step - the \"stopwords\")."]},{"cell_type":"code","metadata":{"id":"hmPFrRWfwbVH"},"source":["def convert_to_BoW(dataset, number_of_documents):\n","    bow_representation = np.zeros((number_of_documents, number_of_words))\n","    labels = np.zeros((number_of_documents, 1))\n","    \n","    i = 0\n","    for label, class_name in enumerate(dataset):\n","        \n","        # For each file\n","        for f in dataset[class_name]:\n","            \n","            # Read all text in file\n","            text = ' '.join(f).split(' ')\n","            \n","            # For each word\n","            for word in text:\n","                if word in valid_words:\n","                    bow_representation[i, valid_words[word]] += 1\n","            \n","            # Label of document\n","            labels[i] = label\n","            \n","            # Increment document counter\n","            i += 1\n","    \n","    return bow_representation, labels\n","\n","# Convert the dataset into their bag of words representation treating train and test separately\n","train_bow_set, train_bow_labels = convert_to_BoW(train_set, n_train)\n","test_bow_set, test_bow_labels = convert_to_BoW(test_set, n_test)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4SRJ9uJAF1jm"},"source":["## Classification with Decision tree"]},{"cell_type":"code","metadata":{"id":"hAmD2iXJCSST"},"source":["from sklearn.metrics import accuracy_score\n","from sklearn.tree import DecisionTreeClassifier"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BWYdsz5pDXZS"},"source":["tree = DecisionTreeClassifier()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CUhZgCEctQm0"},"source":["%%time\n","\n","tree.fit(train_bow_set, train_bow_labels)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_O6-rlirGHLO"},"source":["##Accuracy on Training data"]},{"cell_type":"code","metadata":{"id":"fPR1nQA4DZLs"},"source":["accuracy_score(tree.predict(train_bow_set), train_bow_labels)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2sLRNeE42QBg"},"source":["%%time\n","y_pred = tree.predict(test_bow_set)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Z4Oj0v0wGMwO"},"source":["## Accuracy on Test data"]},{"cell_type":"code","metadata":{"id":"CLkDZkmUDa9a"},"source":["accuracy_score(y_pred, test_bow_labels)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HWWDv28_F97h"},"source":["## Classification with Random Forest"]},{"cell_type":"code","metadata":{"id":"Hb0AKbhLEcuI"},"source":["from sklearn.ensemble import RandomForestClassifier\n","rf = RandomForestClassifier(n_estimators=100, random_state=0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bMPCQ1rQEeUR"},"source":["%%time\n","rf.fit(train_bow_set, train_bow_labels)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dbaTCakaGShA"},"source":["##Accuracy on Train data"]},{"cell_type":"code","metadata":{"id":"QUzxV8CVEj9b"},"source":["accuracy_score(rf.predict(train_bow_set), train_bow_labels)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SLP1zdRjGaRt"},"source":["## Accuracy on Test data"]},{"cell_type":"code","metadata":{"id":"7kc8QyGQ2C21"},"source":["\n","%%time\n","y_pred = rf.predict(test_bow_set)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xJj0o6JPElvI"},"source":["accuracy_score(y_pred, test_bow_labels)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gHODGhEdHgo_"},"source":["## Change in Random Forest Accuracy with an increase in Estimators"]},{"cell_type":"code","metadata":{"id":"O-thza2vGimT"},"source":["training_acc = []\n","test_acc = []\n","for i in range(1,101, 10):\n","    rf = RandomForestClassifier(n_estimators=i, random_state=0)\n","    rf.fit(train_bow_set, train_bow_labels)\n","    training_acc.append(accuracy_score(rf.predict(train_bow_set), train_bow_labels))\n","    test_acc.append(accuracy_score(rf.predict(test_bow_set), test_bow_labels))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5_n_3FR9G-s7"},"source":["plt.plot(range(1,101, 10), training_acc, 'r', label='Training Accuracy')\n","plt.plot(range(1,101, 10), test_acc, 'b', label='Test Accuracy')\n","plt.xlabel('#Estimators')\n","plt.ylabel('Accuracy')\n","plt.legend()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SKPQtbUpYBgn"},"source":["##Change in the accuracy of Random Forest with depth\r\n","\r\n","### Below code block will take some time to complete the execution  "]},{"cell_type":"code","metadata":{"id":"AJt0PlSlYEtA"},"source":["d_training_acc = []\n","d_test_acc = []\n","for i in range(1,3001, 300):\n","    rf = RandomForestClassifier(n_estimators = 100, max_depth=i, random_state=0)\n","    rf.fit(train_bow_set, train_bow_labels)\n","    d_training_acc.append(accuracy_score(rf.predict(train_bow_set), train_bow_labels))\n","    d_test_acc.append(accuracy_score(rf.predict(test_bow_set), test_bow_labels))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xwpKfMROYRVk"},"source":["plt.plot(range(1,3001, 300), d_training_acc, 'r', label='Training Accuracy')\n","plt.plot(range(1,3001, 300), d_test_acc, 'b', label='Test Accuracy')\n","plt.xlabel('Depth of tree')\n","plt.ylabel('Accuracy')\n","plt.legend()\n","plt.title('Random Forest')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"o62HFoB5DGcN"},"source":["##Change in the accuracy of  Decision Tree with depth"]},{"cell_type":"code","metadata":{"id":"S4OsofM6o26x"},"source":["\n","dt_training_acc = []\n","dt_test_acc = []\n","for i in range(1,3001, 300):\n","    tree = DecisionTreeClassifier(max_depth=i)\n","    tree.fit(train_bow_set, train_bow_labels)\n","    dt_training_acc.append(accuracy_score(tree.predict(train_bow_set), train_bow_labels))\n","    dt_test_acc.append(accuracy_score(tree.predict(test_bow_set), test_bow_labels))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zabQWg4AqBkL"},"source":["plt.plot(range(1,3001, 300), dt_training_acc, 'r', label='Training Accuracy')\n","plt.plot(range(1,3001, 300), dt_test_acc, 'b', label='Test Accuracy')\n","plt.xlabel('Depth of tree')\n","plt.ylabel('Accuracy')\n","plt.legend()\n","plt.title('Decision Tree')\n","plt.savefig('DT_depthvsAcc.png')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tC2rt1ZxrgC7"},"source":["## Please answer the questions below to complete the experiment:"]},{"cell_type":"code","metadata":{"id":"NMzKSbLIgFzQ"},"source":["#@title How was the experiment? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n","Complexity = \"\" #@param [\"\",\"Too Simple, I am wasting time\", \"Good, But Not Challenging for me\", \"Good and Challenging for me\", \"Was Tough, but I did it\", \"Too Difficult for me\"]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DjcH1VWSFI2l"},"source":["#@title If it was too easy, what more would you have liked to be added? If it was very difficult, what would you have liked to have been removed? { run: \"auto\", display-mode: \"form\" }\n","Additional = \"\" #@param {type:\"string\"}\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_cTetkuegP7d"},"source":["#@title Can you identify the concepts from the lecture which this experiment covered? { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n","Concepts = \"\" #@param [\"\",\"Yes\", \"No\"]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QFQw0ddId_Ej"},"source":["#@title  Text and image description/explanation and code comments within the experiment: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n","Comments = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"-CXztFuygSBG"},"source":["#@title Run this cell to submit your notebook  { vertical-output: true }\n","try:\n","  if submission_id:\n","      return_id = submit_notebook()\n","      if return_id : submission_id =return_id\n","  else:\n","      print(\"Please complete the setup first.\")\n","except NameError:\n","  print (\"Please complete the setup first.\")"],"execution_count":null,"outputs":[]}]}