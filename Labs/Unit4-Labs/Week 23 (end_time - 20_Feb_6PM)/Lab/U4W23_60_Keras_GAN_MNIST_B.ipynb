{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.0"},"colab":{"name":"U4W23_60_Keras_GAN_MNIST_B.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"lfiU7vYXTUxH"},"source":["# Advanced Certification in AIML\n","## A Program by IIIT-H and TalentSprint"]},{"cell_type":"markdown","metadata":{"id":"CL2dp3Y3DLdV"},"source":["## Learning Objective"]},{"cell_type":"markdown","metadata":{"id":"OTdi8IF4DPBK"},"source":["At the end of the experiment, you will be able to :\n","\n","* understand GAN using keras framework"]},{"cell_type":"code","metadata":{"id":"OvDJsz_mnH9j","cellView":"form"},"source":["#@title Experiment Walkthrough Video\n","from IPython.display import HTML\n","\n","HTML(\"\"\"<video width=\"420\" height=\"540\" controls>\n","  <source src=\"https://cdn.talentsprint.com/talentsprint1/archives/sc/Keras_GANs.mp4\" type=\"video/mp4\">\n","</video>\n","\"\"\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9NuRcSZydkL_"},"source":["## Dataset"]},{"cell_type":"markdown","metadata":{"id":"5KqbKyGSdnuq"},"source":["###Description\n","\n","We use the MNIST dataset for this experiment. Below are the details:\n","\n","1. The dataset contains 60,000 Handwritten digits as training samples and 10,000 Test samples, \n","which means each digit occurs 6000 times in the training set and 1000 times in the testing set. (approximately). \n","2. Each image is Size Normalized and Centered \n","3. Each image is 28 X 28 Pixel with 0-255 Gray Scale Value. \n","4. That means each image is represented as 784 (28 X28) dimension vector where each value is in the range 0- 255.\n","\n","### History\n","\n","Yann LeCun (Director of AI Research, Facebook, Courant Institute, NYU) was given the task of identifying the cheque numbers (in the 90’s) and the amount associated with that cheque without manual intervention. That is when this dataset was created which raised the bars and became a benchmark.\n","\n","Yann LeCun and Corinna Cortes (Google Labs, New York) hold the copyright of MNIST dataset, which is a subset of the original NIST datasets. This dataset is made available under the terms of the Creative Commons Attribution-Share Alike 3.0 license. \n","\n","It is the handwritten digits dataset in which half of them are written by the Census Bureau employees and remaining by the high school students. The digits collected among the Census Bureau employees are easier and cleaner to recognize than the digits collected among the students.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"MJu4cGUCdwTr"},"source":["## Domain Information"]},{"cell_type":"markdown","metadata":{"id":"vMtA8wDddypU"},"source":["\n","Handwriting changes person to person. Some of us have neat handwriting and some have illegible handwriting such as doctors. However, if you think about it even a child who recognizes alphabets and numerics can identify the characters of a text even written by a stranger. But even a technically knowledgeable adult cannot describe the process by which he or she recognizes the text/letters. As you know this is an excellent challenge for Machine Learning.\n","\n","![altxt](https://i.pinimg.com/originals/f2/7a/ac/f27aac4542c0090872110836d65f4c99.jpg)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"X4F3RvUAd4Hw"},"source":["## AI / ML Technique"]},{"cell_type":"markdown","metadata":{"id":"L1qJVLV4f2tw"},"source":["### Generative Adversary Networks (GAN)\n","\n","\n","GANs are generative models devised by Goodfellow et al. in 2014. GAN is about creating, like drawing a portrait or composing a symphony. This is hard compared to other deep learning fields. For instance, it is much easier to identify a Monet painting than painting one.\n","\n","\n","The main focus of GAN is to generate data from scratch, mostly images but other domains including music have been done.\n","\n","GAN composes of two deep networks :\n","\n","* Generator\n","* Discriminator\n","\n","\n","#### Generator \n","\n","The generator tries to produce data that come from some probability distribution. For example, that would be you trying to reproduce the party’s tickets.\n","\n","#### Discriminator\n","\n","The discriminator acts like a judge. It gets to decide if the input comes from the generator or from the true training set. For example, that would be the party’s security comparing your fake ticket with the true ticket to find flaws in your design.\n","\n","In summary, we can say that :\n","\n","* The generator trying to maximize the probability of making the discriminator mistake its inputs as real.\n","\n","* And the discriminator guiding the generator to produce more realistic images.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"IfpAgIPxr1Em"},"source":["![alt text](https://cdn.talentsprint.com/aiml/Experiment_related_data/IMAGES/gan.png)"]},{"cell_type":"markdown","metadata":{"id":"e8ktuH54eQo8"},"source":["### Setup Steps"]},{"cell_type":"code","metadata":{"id":"9Y_UxOzPeVkW","executionInfo":{"status":"ok","timestamp":1613260672829,"user_tz":300,"elapsed":335,"user":{"displayName":"Mohammed Hamdan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh7-0C4n1c_uDVPoywaYa91Jx17qP1YRlmJadqc=s64","userId":"00647759825092258022"}}},"source":["#@title Please enter your registration id to start: (e.g. P181900101) { run: \"auto\", display-mode: \"form\" }\n","Id = \"2100121\" #@param {type:\"string\"}\n"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"2NKdNiU3eWap","executionInfo":{"status":"ok","timestamp":1613260677744,"user_tz":300,"elapsed":1721,"user":{"displayName":"Mohammed Hamdan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh7-0C4n1c_uDVPoywaYa91Jx17qP1YRlmJadqc=s64","userId":"00647759825092258022"}}},"source":["#@title Please enter your password (normally your phone number) to continue: { run: \"auto\", display-mode: \"form\" }\n","password = \"5142192291\" #@param {type:\"string\"}\n"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"WBPPuGmBlDIN","cellView":"form","colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"status":"ok","timestamp":1613260677744,"user_tz":300,"elapsed":2225,"user":{"displayName":"Mohammed Hamdan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh7-0C4n1c_uDVPoywaYa91Jx17qP1YRlmJadqc=s64","userId":"00647759825092258022"}},"outputId":"868d8930-a66c-4810-c393-6dbabf1a56c1"},"source":["#@title Run this cell to complete the setup for this Notebook\n","from IPython import get_ipython\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","ipython = get_ipython()\n","\n","notebook= \"U4W23_60_Keras_GAN_MNIST_B\" #name of the notebook\n","\n","def setup():\n","    from IPython.display import HTML, display\n","    display(HTML('<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId={0}&recordId={1}\"></script>'.format(getId(),submission_id)))\n","    print(\"Setup completed successfully\")\n","    return\n","\n","def submit_notebook():\n","    ipython.magic(\"notebook -e \"+ notebook + \".ipynb\")\n","    \n","    import requests, json, base64, datetime\n","\n","    url = \"https://dashboard.talentsprint.com/xp/app/save_notebook_attempts\"\n","    if not submission_id:\n","      data = {\"id\" : getId(), \"notebook\" : notebook, \"mobile\" : getPassword()}\n","      r = requests.post(url, data = data)\n","      r = json.loads(r.text)\n","\n","      if r[\"status\"] == \"Success\":\n","          return r[\"record_id\"]\n","      elif \"err\" in r:        \n","        print(r[\"err\"])\n","        return None        \n","      else:\n","        print (\"Something is wrong, the notebook will not be submitted for grading\")\n","        return None\n","    \n","    elif getAnswer() and getComplexity() and getAdditional() and getConcepts() and getWalkthrough() and getComments() and getMentorSupport():\n","      f = open(notebook + \".ipynb\", \"rb\")\n","      file_hash = base64.b64encode(f.read())\n","\n","      data = {\"complexity\" : Complexity, \"additional\" :Additional, \n","              \"concepts\" : Concepts, \"record_id\" : submission_id, \n","              \"answer\" : Answer, \"id\" : Id, \"file_hash\" : file_hash,\n","              \"notebook\" : notebook, \"feedback_walkthrough\":Walkthrough ,\n","              \"feedback_experiments_input\" : Comments,\n","              \"feedback_mentor_support\": Mentor_support}\n","\n","      r = requests.post(url, data = data)\n","      r = json.loads(r.text)\n","      if \"err\" in r:        \n","        print(r[\"err\"])\n","        return None   \n","      else:\n","        print(\"Your submission is successful.\")\n","        print(\"Ref Id:\", submission_id)\n","        print(\"Date of submission: \", r[\"date\"])\n","        print(\"Time of submission: \", r[\"time\"])\n","        print(\"View your submissions: https://aiml.iiith.talentsprint.com/notebook_submissions\")\n","        #print(\"For any queries/discrepancies, please connect with mentors through the chat icon in LMS dashboard.\")\n","        return submission_id\n","    else: submission_id\n","    \n","\n","def getAdditional():\n","  try:\n","    if not Additional: \n","      raise NameError\n","    else:\n","      return Additional  \n","  except NameError:\n","    print (\"Please answer Additional Question\")\n","    return None\n","\n","def getComplexity():\n","  try:\n","    if not Complexity:\n","      raise NameError\n","    else:\n","      return Complexity\n","  except NameError:\n","    print (\"Please answer Complexity Question\")\n","    return None\n","  \n","def getConcepts():\n","  try:\n","    if not Concepts:\n","      raise NameError\n","    else:\n","      return Concepts\n","  except NameError:\n","    print (\"Please answer Concepts Question\")\n","    return None\n","  \n","  \n","def getWalkthrough():\n","  try:\n","    if not Walkthrough:\n","      raise NameError\n","    else:\n","      return Walkthrough\n","  except NameError:\n","    print (\"Please answer Walkthrough Question\")\n","    return None\n","  \n","def getComments():\n","  try:\n","    if not Comments:\n","      raise NameError\n","    else:\n","      return Comments\n","  except NameError:\n","    print (\"Please answer Comments Question\")\n","    return None\n","  \n","\n","def getMentorSupport():\n","  try:\n","    if not Mentor_support:\n","      raise NameError\n","    else:\n","      return Mentor_support\n","  except NameError:\n","    print (\"Please answer Mentor support Question\")\n","    return None\n","\n","def getAnswer():\n","  try:\n","    if not Answer:\n","      raise NameError \n","    else: \n","      return Answer\n","  except NameError:\n","    print (\"Please answer Question\")\n","    return None\n","  \n","\n","def getId():\n","  try: \n","    return Id if Id else None\n","  except NameError:\n","    return None\n","\n","def getPassword():\n","  try:\n","    return password if password else None\n","  except NameError:\n","    return None\n","\n","submission_id = None\n","### Setup \n","if getPassword() and getId():\n","  submission_id = submit_notebook()\n","  if submission_id:\n","    setup() \n","else:\n","  print (\"Please complete Id and Password cells before running setup\")\n","\n"],"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/html":["<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId=2100121&recordId=15349\"></script>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Setup completed successfully\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"eBD0M7uyinT3"},"source":["### Importing required  Packages\n","\n","\n"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2018-06-18T17:17:29.299849Z","start_time":"2018-06-18T17:17:28.948111Z"},"id":"UC3lWkPqxxoZ","executionInfo":{"status":"ok","timestamp":1613264451293,"user_tz":300,"elapsed":1609,"user":{"displayName":"Mohammed Hamdan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh7-0C4n1c_uDVPoywaYa91Jx17qP1YRlmJadqc=s64","userId":"00647759825092258022"}}},"source":["from keras.datasets import mnist\n","from keras.models import Model\n","from keras.layers import Dense, LeakyReLU, BatchNormalization, Input\n","from keras.optimizers import Adam\n","\n","import numpy as np\n","import matplotlib.pyplot as plt"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qasiyCseXjrD"},"source":["### Load the Data"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2018-06-18T17:17:31.211621Z","start_time":"2018-06-18T17:17:30.628121Z"},"id":"0oeOE4uhxxod","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1613264453578,"user_tz":300,"elapsed":657,"user":{"displayName":"Mohammed Hamdan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh7-0C4n1c_uDVPoywaYa91Jx17qP1YRlmJadqc=s64","userId":"00647759825092258022"}},"outputId":"ef3a4737-88de-44fc-eb6d-cf00c92c701c"},"source":["# Load dataset\n","(X_train, y_train), (X_test, y_test) = mnist.load_data()"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n","11493376/11490434 [==============================] - 0s 0us/step\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"SVhDIEMiX1bc"},"source":["### Visualize the data"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2018-06-18T17:17:31.673570Z","start_time":"2018-06-18T17:17:31.213616Z"},"id":"RxqRv7-Hxxoe","colab":{"base_uri":"https://localhost:8080/","height":248},"executionInfo":{"status":"ok","timestamp":1613264456526,"user_tz":300,"elapsed":839,"user":{"displayName":"Mohammed Hamdan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh7-0C4n1c_uDVPoywaYa91Jx17qP1YRlmJadqc=s64","userId":"00647759825092258022"}},"outputId":"2d11a3f6-d2e7-4643-c00c-76baac8dd06d"},"source":["fig = plt.figure()\n","for i in range(10):\n","    plt.subplot(2, 5, i+1)\n","    x_y = X_train[y_train == i]\n","    plt.imshow(x_y[0], cmap='gray', interpolation='none')\n","    plt.title(\"Class %d\" % (i))\n","    plt.xticks([])\n","    plt.yticks([])\n","    \n","plt.tight_layout()"],"execution_count":4,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 10 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"_Q9kNOoWfwiw"},"source":["### Data Pre-Processing\n"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2018-06-18T17:17:31.926590Z","start_time":"2018-06-18T17:17:31.675462Z"},"id":"YHbXFwH6xxof","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1613260725760,"user_tz":300,"elapsed":344,"user":{"displayName":"Mohammed Hamdan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh7-0C4n1c_uDVPoywaYa91Jx17qP1YRlmJadqc=s64","userId":"00647759825092258022"}},"outputId":"2dce4ec1-fc6e-4a15-e0fb-a28eac6f74a4"},"source":["print('X_train shape', X_train.shape)\n","\n","# Reshaping the inputs\n","X_train = X_train.reshape(60000, 28*28)\n","# Normalizing the inputs (-1, 1)\n","X_train = (X_train.astype('float32') / 255 - 0.5) * 2\n","\n","print('X_train reshape:', X_train.shape)"],"execution_count":8,"outputs":[{"output_type":"stream","text":["X_train shape (60000, 28, 28)\n","X_train reshape: (60000, 784)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"RSopxRZEZsp4"},"source":["### Build the Generator Model"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2018-06-18T17:17:32.266239Z","start_time":"2018-06-18T17:17:31.929037Z"},"id":"rd7tLDOHxxog","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1613264471524,"user_tz":300,"elapsed":5799,"user":{"displayName":"Mohammed Hamdan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh7-0C4n1c_uDVPoywaYa91Jx17qP1YRlmJadqc=s64","userId":"00647759825092258022"}},"outputId":"8d518c28-4f7e-403c-e97e-73b3165716ca"},"source":["# Get the generator model\n","def build_generator(latent_dim):\n","\n","  # Generator takes in a vector from the latent space\n","  i = Input(shape=(latent_dim,))\n","\n","  # The Leaky ReLU modifies the ReLu function to allow small negative values when the input is less than zero\n","  # below the threshold value of the activation function, the values will be damped or set to zero\n","  # alpha governs the slope for values lower than the threshold \n","  \n","  # YOUR CODE HERE: To dfine the dense layer with the latent dimension as input with 256 hidden nodes\n","  x = Dense(256, activation=LeakyReLU(alpha=0.2))(i)\n","  # YOUR CODE HERE: To define the batch normalization with momentum\n","  x = BatchNormalization(momentum=0.7)(x)\n","  x = Dense(512, activation=LeakyReLU(alpha=0.2))(x)\n","  x = BatchNormalization(momentum=0.7)(x)\n","  x = Dense(1024, activation=LeakyReLU(alpha=0.2))(x)\n","  x = BatchNormalization(momentum=0.7)(x)\n","\n","  # Because our image pixels are censored to be between -1 and +1 use tanh activation function\n","  # The range of the tanh function is from (-1 to 1)\n","\n","  # YOUR CODE HERE: To define the output layer with activation as 'tanh'\n","  x = Dense(784, activation='tanh')(x)\n","\n","  model = Model(i, x)\n","  return model\n","\n","\n","# Dimensionality of the latent space\n","latent_dim = 100 # Can be replaced with any number\n","\n","generator =  build_generator(latent_dim) # YOUR CODE HERE: To call the generator function with latent_dim as input\n","\n","\n","# Display the summary representation of the model\n","generator.summary()"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Model: \"model\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_1 (InputLayer)         [(None, 100)]             0         \n","_________________________________________________________________\n","dense (Dense)                (None, 256)               25856     \n","_________________________________________________________________\n","batch_normalization (BatchNo (None, 256)               1024      \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 512)               131584    \n","_________________________________________________________________\n","batch_normalization_1 (Batch (None, 512)               2048      \n","_________________________________________________________________\n","dense_2 (Dense)              (None, 1024)              525312    \n","_________________________________________________________________\n","batch_normalization_2 (Batch (None, 1024)              4096      \n","_________________________________________________________________\n","dense_3 (Dense)              (None, 784)               803600    \n","=================================================================\n","Total params: 1,493,520\n","Trainable params: 1,489,936\n","Non-trainable params: 3,584\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"9vuIL62FaNht"},"source":["### Build the Discriminator Model"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2018-06-18T17:17:32.326856Z","start_time":"2018-06-18T17:17:32.275681Z"},"id":"j2gtkJGUxxoi","executionInfo":{"status":"ok","timestamp":1613264548746,"user_tz":300,"elapsed":209,"user":{"displayName":"Mohammed Hamdan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh7-0C4n1c_uDVPoywaYa91Jx17qP1YRlmJadqc=s64","userId":"00647759825092258022"}}},"source":["# Get the discriminator model\n","def build_discriminator(img_size):\n","  i = Input(shape=(img_size,))\n","  x = Dense(512, activation=LeakyReLU(alpha=0.2))(i)\n","  x = Dense(256, activation=LeakyReLU(alpha=0.2))(x)\n","  \n","  # YOUR CODE HERE: To define the output layer with sigmoid activation function\n","  x = Dense(1, activation='sigmoid')(x)\n","\n","\n","  model = Model(i, x)\n","  return model\n","\n","discriminator =  build_discriminator(784) # YOUR CODE HERE: To call the discriminator function with img_size as 784"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"h8ULw_8Xxxoi"},"source":["#### Discriminator model visualization"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2018-06-18T17:17:32.332285Z","start_time":"2018-06-18T17:17:32.328654Z"},"id":"6c4uTVUOxxok","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1613264552783,"user_tz":300,"elapsed":277,"user":{"displayName":"Mohammed Hamdan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh7-0C4n1c_uDVPoywaYa91Jx17qP1YRlmJadqc=s64","userId":"00647759825092258022"}},"outputId":"5680936a-cd8c-40cc-8d25-01bf041a766f"},"source":["# Display the summary representation of the model\n","discriminator.summary()"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Model: \"model_2\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_3 (InputLayer)         [(None, 784)]             0         \n","_________________________________________________________________\n","dense_7 (Dense)              (None, 512)               401920    \n","_________________________________________________________________\n","dense_8 (Dense)              (None, 256)               131328    \n","_________________________________________________________________\n","dense_9 (Dense)              (None, 1)                 257       \n","=================================================================\n","Total params: 533,505\n","Trainable params: 533,505\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"3r4SBVvHbStP"},"source":["### Compile the Discriminator Model"]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2018-06-18T17:17:32.385813Z","start_time":"2018-06-18T17:17:32.334488Z"},"id":"19dhxJY0xxok","executionInfo":{"status":"ok","timestamp":1613264677965,"user_tz":300,"elapsed":214,"user":{"displayName":"Mohammed Hamdan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh7-0C4n1c_uDVPoywaYa91Jx17qP1YRlmJadqc=s64","userId":"00647759825092258022"}}},"source":["# Optimizer\n","optimizer = Adam(lr=0.0002, beta_1=0.5)\n","# YOUR CODE HERE: To compile the discriminator Model\n","discriminator.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['binary_accuracy'])"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"DmPfXUHhAq1f","executionInfo":{"status":"ok","timestamp":1613264688166,"user_tz":300,"elapsed":246,"user":{"displayName":"Mohammed Hamdan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh7-0C4n1c_uDVPoywaYa91Jx17qP1YRlmJadqc=s64","userId":"00647759825092258022"}}},"source":["# Create an input to represent noise sample from latent space\r\n","noise = Input(shape=(latent_dim,))\r\n","\r\n","# Pass noise through generator to get an image\r\n","img = generator(noise)\r\n","\r\n","# Freeze the discriminator layers\r\n","# Make sure only the generator is trained\r\n","discriminator.trainable = False"],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"32Z2bddlPJbl"},"source":["Pass output image of the generator as input to the discriminator model"]},{"cell_type":"code","metadata":{"id":"ugaPMht-E7YC","executionInfo":{"status":"ok","timestamp":1613264692176,"user_tz":300,"elapsed":215,"user":{"displayName":"Mohammed Hamdan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh7-0C4n1c_uDVPoywaYa91Jx17qP1YRlmJadqc=s64","userId":"00647759825092258022"}}},"source":["# The true output is fake, but we label them real!\r\n","fake_pred = discriminator(img)"],"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"b0OLZhFdxxol"},"source":["### Combined network"]},{"cell_type":"markdown","metadata":{"id":"8kfxtQJPE-o4"},"source":["#### Build and compile the combined model\r\n","\r\n","Create a new model object called **combined model** which takes input a `noise` sample and produces the output prediction (fake)"]},{"cell_type":"code","metadata":{"id":"nC-vBiQgFDhr","executionInfo":{"status":"ok","timestamp":1613264714358,"user_tz":300,"elapsed":223,"user":{"displayName":"Mohammed Hamdan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh7-0C4n1c_uDVPoywaYa91Jx17qP1YRlmJadqc=s64","userId":"00647759825092258022"}}},"source":["combined_model = Model(noise, fake_pred) # YOUR CODE HERE: To create the combined model by passing noise and fake_pred"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"IYhk16-_FEnd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1613264716403,"user_tz":300,"elapsed":266,"user":{"displayName":"Mohammed Hamdan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh7-0C4n1c_uDVPoywaYa91Jx17qP1YRlmJadqc=s64","userId":"00647759825092258022"}},"outputId":"d3d20f71-1bae-4b44-bca1-a29ad6f2738f"},"source":["# Display the summary representation of the model\r\n","combined_model.summary()"],"execution_count":13,"outputs":[{"output_type":"stream","text":["Model: \"model_3\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_4 (InputLayer)         [(None, 100)]             0         \n","_________________________________________________________________\n","model (Functional)           (None, 784)               1493520   \n","_________________________________________________________________\n","model_2 (Functional)         (None, 1)                 533505    \n","=================================================================\n","Total params: 2,027,025\n","Trainable params: 1,489,936\n","Non-trainable params: 537,089\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"HrEpMsQzBE-t","executionInfo":{"status":"ok","timestamp":1613264741113,"user_tz":300,"elapsed":266,"user":{"displayName":"Mohammed Hamdan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh7-0C4n1c_uDVPoywaYa91Jx17qP1YRlmJadqc=s64","userId":"00647759825092258022"}}},"source":["# YOUR CODE HERE: To compile the combined model\n","combined_model.compile(loss='binary_crossentropy', optimizer = optimizer, metrics=['binary_accuracy'])"],"execution_count":15,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YaPflL5lcwMW"},"source":["### Train the Model\n","\n","\n","We train the discriminator and the generator in turn in a loop as follows:\n","\n","1. Set the discriminator trainable\n","2. Train the discriminator with the real digit images and the images generated by the generator to classify the real and fake images.\n","3. Set the discriminator non-trainable\n","4. Train the generator as part of the GAN. We feed latent samples into the GAN and let the generator to produce digit images and use the discriminator to classify the image."]},{"cell_type":"code","metadata":{"ExecuteTime":{"end_time":"2018-06-18T17:50:25.368754Z","start_time":"2018-06-18T17:17:32.678615Z"},"id":"W1nDkgMyxxom","colab":{"base_uri":"https://localhost:8080/","height":742},"executionInfo":{"status":"error","timestamp":1613265812013,"user_tz":300,"elapsed":289,"user":{"displayName":"Mohammed Hamdan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh7-0C4n1c_uDVPoywaYa91Jx17qP1YRlmJadqc=s64","userId":"00647759825092258022"}},"outputId":"084cf0fd-2887-444a-c125-100102d78f51"},"source":["epochs = 10\n","batch_size = 64\n","\n","# Smoothed labels to the discriminator network\n","# This means we can have decimal values such as 0.9 (true), 0.8 (true), 0.1 (fake), or 0.2 (fake), \n","# instead of labeling every example as either 1 (true) or 0 (fake)\n","smooth = 0.1\n","\n","# Labels for generated and real data while training\n","real = np.ones(shape=(batch_size, 1))\n","fake = np.zeros(shape=(batch_size, 1))\n","\n","d_loss = []\n","combined_model_loss = []\n","\n","for e in range(epochs + 1):\n","    for i in range(len(X_train) // batch_size):\n","        \n","        # YOUR CODE HERE: To train the discriminator weights by setting trainable parameter to True\n","        discriminator.trainable = True\n","\n","        # Real samples\n","        X_batch = X_train[i*batch_size:(i+1)*batch_size]\n","        # Return the real loss \n","        d_loss_real = discriminator.train_on_batch(x=X_batch, y=real * (1 - smooth))\n","        \n","        # Fake Samples\n","        # To train the generator we need fake images only\n","        z = np.random.normal(loc=0, scale=1, size=(batch_size, latent_dim))\n","        X_fake = generator.predict_on_batch(z)\n","        d_loss_fake =  discriminator.train_on_batch(x=X_fake, y=fake) # YOUR CODE HERE: To train the discriminator on the fake data and store the loss\n","         \n","        # Discriminator loss\n","        # In order to calculate the overall loss take the mean of the real and fake losses \n","        d_loss_batch = 0.5 * (d_loss_real[0] + d_loss_fake[0])\n","        \n","        # Train Generator weights\n","        # During the training of gan,\n","        # the weights of discriminator should be fixed.\n","        # We can enforce that by setting the trainable flag\n","        discriminator.trainable = False\n","        \n","        # The input is the noise and the target is our vector of ones\n","        # This is because we are trying to trick the discriminator into thinking that the images from the generator are real\n","        combined_model_loss_batch = combined_model.train_on_batch(x=z, y=real)\n","   \n","        print('epoch = %d/%d, batch = %d/%d, d_loss=%.3f, g_loss=%.3f' % (e + 1, epochs, i, len(X_train) // batch_size, d_loss_batch, combined_model_loss_batch[0]),100*' ',end='\\r')\n","    \n","    # Save the losses (Append both losses to the lists of losses)\n","    d_loss.append(d_loss_batch)\n","    combined_model_loss.append(combined_model_loss_batch[0])\n","    print('epoch = %d/%d, d_loss=%.3f, g_loss=%.3f' % (e + 1, epochs, d_loss[-1], combined_model_loss[-1]), 100*' ')\n","\n","    if e % 10 == 0:\n","        samples = 10\n","        # Generate fake MNIST images from noised input\n","        x_fake = generator.predict(np.random.normal(loc=0, scale=1, size=(samples, latent_dim)))\n","\n","        for k in range(samples):\n","            plt.subplot(2, 5, k+1)\n","            plt.imshow(x_fake[k].reshape(28, 28), cmap='gray')\n","            plt.savefig('outputImages/fake_image'+ str(k) +'.png')\n","            plt.xticks([])\n","            plt.yticks([])\n","\n","        plt.tight_layout()\n","        plt.show()"],"execution_count":22,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-22-ecaafefad8a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mX_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m# Return the real loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0md_loss_real\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreal\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0msmooth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;31m# Fake Samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics, return_dict)\u001b[0m\n\u001b[1;32m   1725\u001b[0m                                                     class_weight)\n\u001b[1;32m   1726\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1727\u001b[0;31m       \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1729\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    860\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 862\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    863\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    864\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2939\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2940\u001b[0m       (graph_function,\n\u001b[0;32m-> 2941\u001b[0;31m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0m\u001b[1;32m   2942\u001b[0m     return graph_function._call_flat(\n\u001b[1;32m   2943\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3356\u001b[0m               call_context_key in self._function_cache.missed):\n\u001b[1;32m   3357\u001b[0m             return self._define_function_with_shape_relaxation(\n\u001b[0;32m-> 3358\u001b[0;31m                 args, kwargs, flat_args, filtered_flat_args, cache_key_context)\n\u001b[0m\u001b[1;32m   3359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3360\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_define_function_with_shape_relaxation\u001b[0;34m(self, args, kwargs, flat_args, filtered_flat_args, cache_key_context)\u001b[0m\n\u001b[1;32m   3278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3279\u001b[0m     graph_function = self._create_graph_function(\n\u001b[0;32m-> 3280\u001b[0;31m         args, kwargs, override_flat_arg_shapes=relaxed_arg_shapes)\n\u001b[0m\u001b[1;32m   3281\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marg_relaxed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mrank_only_cache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3204\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3205\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3206\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3207\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3208\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    988\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 990\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    991\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    635\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    975\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    976\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 977\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    978\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    979\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: in user code:\n\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n        return step_function(self, iterator)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:795 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:1259 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2730 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:3417 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:788 run_step  **\n        outputs = model.train_step(data)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:754 train_step\n        y_pred = self(x, training=True)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py:998 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/input_spec.py:274 assert_input_compatibility\n        ', found shape=' + display_shape(x.shape))\n\n    ValueError: Input 0 is incompatible with layer model_2: expected shape=(None, 784), found shape=(64, 28, 28)\n"]}]},{"cell_type":"markdown","metadata":{"id":"giNdCc9at8-1"},"source":["### Please answer the questions below to complete the experiment:"]},{"cell_type":"code","metadata":{"id":"eMJFxr5YskFs","cellView":"form"},"source":["#@title State True or False: The generator model in GANs takes random noise as input data and generates a new image\n","Answer = \"\" #@param [\"\",\"TRUE\", \"FALSE\"] \n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NMzKSbLIgFzQ"},"source":["#@title How was the experiment? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n","Complexity = \"\" #@param [\"\",\"Too Simple, I am wasting time\", \"Good, But Not Challenging for me\", \"Good and Challenging for me\", \"Was Tough, but I did it\", \"Too Difficult for me\"]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DjcH1VWSFI2l"},"source":["#@title If it was too easy, what more would you have liked to be added? If it was very difficult, what would you have liked to have been removed? { run: \"auto\", display-mode: \"form\" }\n","Additional = \"\" #@param {type:\"string\"}\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4VBk_4VTAxCM"},"source":["#@title Can you identify the concepts from the lecture which this experiment covered? { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n","Concepts = \"\" #@param [\"\",\"Yes\", \"No\"]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"r35isHfTVGKc"},"source":["#@title  Experiment walkthrough video? { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n","Walkthrough = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XH91cL1JWH7m"},"source":["#@title  Text and image description/explanation and code comments within the experiment: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n","Comments = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"z8xLqj7VWIKW"},"source":["#@title Mentor Support: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n","Mentor_support = \"\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"cellView":"form","id":"FzAZHt1zw-Y-"},"source":["#@title Run this cell to submit your notebook for grading { vertical-output: true }\n","try:\n","  if submission_id:\n","      return_id = submit_notebook()\n","      if return_id : submission_id = return_id\n","  else:\n","      print(\"Please complete the setup first.\")\n","except NameError:\n","  print (\"Please complete the setup first.\")"],"execution_count":null,"outputs":[]}]}