{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"BOWTest.ipynb","provenance":[],"authorship_tag":"ABX9TyNebY5DDl4G8kB4H7BJawZw"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"wU5GITzf682Y","executionInfo":{"status":"ok","timestamp":1601351181728,"user_tz":240,"elapsed":870,"user":{"displayName":"Mohammed Hamdan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh7-0C4n1c_uDVPoywaYa91Jx17qP1YRlmJadqc=s64","userId":"00647759825092258022"}},"outputId":"800de8c9-7e3a-4e24-dd45-f90ffebabe0c","colab":{"base_uri":"https://localhost:8080/","height":69}},"source":["import nltk\n","nltk.download('punkt')"],"execution_count":28,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":28}]},{"cell_type":"code","metadata":{"id":"_7LDcEu67Hcd","executionInfo":{"status":"ok","timestamp":1601352192323,"user_tz":240,"elapsed":454,"user":{"displayName":"Mohammed Hamdan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh7-0C4n1c_uDVPoywaYa91Jx17qP1YRlmJadqc=s64","userId":"00647759825092258022"}}},"source":["sentences = [\"Joe waited for the train\", \"The train was late\", \"Mary and Samantha took the bus\", \"I looked for Mary and Samantha at the bus station\" ,\"Mary and Samantha arrived at the bus station early but waited until noon for the bus\"]\n","\n"],"execution_count":66,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KfhZdEd27Vza"},"source":["Step 1: Tokenize a sentence\n","We will start by removing stopwords from the sentences.\n","\n","Tokenization is the act of breaking up a sequence of strings into pieces such as words"]},{"cell_type":"code","metadata":{"id":"3rj-mnJJ7eJm","executionInfo":{"status":"ok","timestamp":1601351972037,"user_tz":240,"elapsed":776,"user":{"displayName":"Mohammed Hamdan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh7-0C4n1c_uDVPoywaYa91Jx17qP1YRlmJadqc=s64","userId":"00647759825092258022"}}},"source":["def word_extraction(sentence):\n","      ignore = ['a', \"the\", \"is\"]    \n","      words = re.sub(\"[^\\w]\", \" \",  sentence).split()    \n","      cleaned_text = [w.lower() \n","      for w in words if w not in ignore]    \n","      return cleaned_text\n"],"execution_count":60,"outputs":[]},{"cell_type":"code","metadata":{"id":"7P2OpSki7usL","executionInfo":{"status":"ok","timestamp":1601352598439,"user_tz":240,"elapsed":1322,"user":{"displayName":"Mohammed Hamdan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh7-0C4n1c_uDVPoywaYa91Jx17qP1YRlmJadqc=s64","userId":"00647759825092258022"}},"outputId":"0ff4a899-bc7b-40d8-f8d2-d017c3033888","colab":{"base_uri":"https://localhost:8080/","height":52}},"source":["from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords \n","nltk.download('stopwords')\n","set(stopwords.words('english'))\n","\n","import spacy\n","nlp = spacy.load(\"en_core_web_sm\")\n","\n","\n","\n","import re\n","\n","#set(stopwords.words('english'))\n","\n"],"execution_count":72,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"dtgwxcYT8In6","executionInfo":{"status":"ok","timestamp":1601352934624,"user_tz":240,"elapsed":420,"user":{"displayName":"Mohammed Hamdan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh7-0C4n1c_uDVPoywaYa91Jx17qP1YRlmJadqc=s64","userId":"00647759825092258022"}}},"source":["def tokenize(sentences):\n","      words = []    \n","      for sentence in sentences:        \n","        w = word_extraction(sentence)        \n","        words.extend(w)            \n","        words = sorted(list(set(words)))    \n","      return words\n"],"execution_count":83,"outputs":[]},{"cell_type":"code","metadata":{"id":"sXxME1R--Mlw","executionInfo":{"status":"ok","timestamp":1601352938218,"user_tz":240,"elapsed":413,"user":{"displayName":"Mohammed Hamdan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh7-0C4n1c_uDVPoywaYa91Jx17qP1YRlmJadqc=s64","userId":"00647759825092258022"}},"outputId":"dea46b67-2fa5-43fb-c548-c45c0f00d1fd","colab":{"base_uri":"https://localhost:8080/","height":380}},"source":["tokenize(sentences)"],"execution_count":84,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['and',\n"," 'arrived',\n"," 'at',\n"," 'bus',\n"," 'but',\n"," 'early',\n"," 'for',\n"," 'i',\n"," 'joe',\n"," 'late',\n"," 'looked',\n"," 'mary',\n"," 'noon',\n"," 'samantha',\n"," 'station',\n"," 'the',\n"," 'took',\n"," 'train',\n"," 'until',\n"," 'waited',\n"," 'was']"]},"metadata":{"tags":[]},"execution_count":84}]},{"cell_type":"markdown","metadata":{"id":"agCQXnaIEyu8"},"source":["Step 3: Build vocabulary and generate vectors\n"]},{"cell_type":"code","metadata":{"id":"ASRWERWzE3d-","executionInfo":{"status":"ok","timestamp":1601353616801,"user_tz":240,"elapsed":449,"user":{"displayName":"Mohammed Hamdan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh7-0C4n1c_uDVPoywaYa91Jx17qP1YRlmJadqc=s64","userId":"00647759825092258022"}}},"source":["import numpy as np\n","def generate_bow(allsentences):\n","          vocab = tokenize(allsentences)\n","          print(\"Word List for Document \\n{0} \\n\".format(vocab));\n","          for sentence in allsentences:\n","            words = word_extraction(sentence)        \n","            bag_vector = np.zeros(len(vocab))        \n","            for w in words:            \n","              for i,word in enumerate(vocab):                \n","                if word == w:                     \n","                  bag_vector[i] += 1                            \n","            print(\"{0}\\n{1}\\n\".format(sentence,np.array(bag_vector)))\n","\n"],"execution_count":102,"outputs":[]},{"cell_type":"code","metadata":{"id":"8EpMDHpPFouh","executionInfo":{"status":"ok","timestamp":1601353617898,"user_tz":240,"elapsed":418,"user":{"displayName":"Mohammed Hamdan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh7-0C4n1c_uDVPoywaYa91Jx17qP1YRlmJadqc=s64","userId":"00647759825092258022"}}},"source":["allsentences = [\"Joe waited for the train train\", \"The train was late\", \"Mary and Samantha took the bus\", \n","                \"I looked for Mary and Samantha at the bus station\",\n","                \"Mary and Samantha arrived at the bus station early but waited until noon for the bus\"]\n","\n","                \n"],"execution_count":103,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yMuHxFIRM2O8"},"source":[" the more similar the words in two documents, the more similar the documents can be."]},{"cell_type":"code","metadata":{"id":"2TK2IQWvGzYP","executionInfo":{"status":"ok","timestamp":1601353620274,"user_tz":240,"elapsed":432,"user":{"displayName":"Mohammed Hamdan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh7-0C4n1c_uDVPoywaYa91Jx17qP1YRlmJadqc=s64","userId":"00647759825092258022"}},"outputId":"acc4213d-7d5c-4f63-e4ae-6c5836da29f7","colab":{"base_uri":"https://localhost:8080/","height":348}},"source":["generate_bow(allsentences)\n"],"execution_count":104,"outputs":[{"output_type":"stream","text":["Word List for Document \n","['and', 'arrived', 'at', 'bus', 'but', 'early', 'for', 'i', 'joe', 'late', 'looked', 'mary', 'noon', 'samantha', 'station', 'the', 'took', 'train', 'until', 'waited', 'was'] \n","\n","Joe waited for the train train\n","[0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 2. 0. 1. 0.]\n","\n","The train was late\n","[0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1.]\n","\n","Mary and Samantha took the bus\n","[1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0.]\n","\n","I looked for Mary and Samantha at the bus station\n","[1. 0. 1. 1. 0. 0. 1. 1. 0. 0. 1. 1. 0. 1. 1. 0. 0. 0. 0. 0. 0.]\n","\n","Mary and Samantha arrived at the bus station early but waited until noon for the bus\n","[1. 1. 1. 2. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 1. 1. 0.]\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"zjmzhcEfNqFn","executionInfo":{"status":"ok","timestamp":1601355590943,"user_tz":240,"elapsed":788,"user":{"displayName":"Mohammed Hamdan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh7-0C4n1c_uDVPoywaYa91Jx17qP1YRlmJadqc=s64","userId":"00647759825092258022"}},"outputId":"f0df0575-3fa6-4339-dc87-e9f2df5ef9c6","colab":{"base_uri":"https://localhost:8080/","height":104}},"source":["from sklearn.feature_extraction.text import CountVectorizer\n","vectorizer = CountVectorizer()\n","X = vectorizer.fit_transform(allsentences)\n","print(X.toarray())\n"],"execution_count":105,"outputs":[{"output_type":"stream","text":["[[0 0 0 0 0 0 1 1 0 0 0 0 0 0 1 0 2 0 1 0]\n"," [0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 1 0 0 1]\n"," [1 0 0 1 0 0 0 0 0 0 1 0 1 0 1 1 0 0 0 0]\n"," [1 0 1 1 0 0 1 0 0 1 1 0 1 1 1 0 0 0 0 0]\n"," [1 1 1 2 1 1 1 0 0 0 1 1 1 1 2 0 0 1 1 0]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"JtUFrFswTrNz"},"source":["Example of the Bag-of-Words Model\n","Step 1: Collect Data\n","let’s treat each line as a separate “document” and the 4 lines as our entire corpus of documents.\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"qsuj1Yi3UGCd"},"source":["Step 2: Design the Vocabulary\n","The unique words here (ignoring case and punctuation) are: That is a vocabulary of 10 words from a corpus containing 24 words."]},{"cell_type":"markdown","metadata":{"id":"2291noP5UCwj"},"source":["Step 3: Create Document Vectors\n"," score the words in each document: turn each document of free text into a vector that we can use as input or output for a machine learning model.\n"," **we can use a fixed-length document representation of vocabluary size with one position in the vector to score each word.The simplest scoring method is to mark the presence of words as a boolean value, 0 for absent, 1 for present.\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"HQn3FC8zVmCk"},"source":["New documents that overlap with the vocabulary of known words, but may contain words outside of the vocabulary, can still be encoded, where only the occurrence of known words are scored and unknown words are ignored.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"7EwGvmQ8VuLE"},"source":["Managing Vocabulary\n","As the vocabulary size increases, so does the vector representation of documents.\n","the length of the document vector is equal to the number of known words.\n","You can imagine that for a very large corpus, such as thousands of books, that the length of the vector might be thousands or millions of positions. Further, each document may contain very few of the known words in the vocabulary.\n","\n","*****\n","This results in a vector with lots of zero scores, called a sparse vector or sparse representation.\n","**Sparse vectors require more memory and computational resources when modeling and the vast number of positions or dimensions can make the modeling process very challenging for traditional algorithms.\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"qusYbHJJWlY5"},"source":["As such, there is pressure to decrease the size of the vocabulary of BOW model using simple text cleaning techniques: \n","\n","1.   Ignoring case\n","2.   Ignoring punctuation\n","\n","\n","\n","\n","Ignoring frequent words that don’t contain much information, called stop words, like “a,” “of,” etc.\n","Fixing misspelled words.\n","Reducing words to their stem (e.g. “play” from “playing”) using stemming algorithms."]},{"cell_type":"markdown","metadata":{"id":"Ox8tuBTCXWYE"},"source":["A more sophisticated approach is to create a vocabulary of grouped words. This both changes the scope of the vocabulary and allows the bag-of-words to capture a little bit more meaning from the document.\n","\n","\n","\n","*    each word or token is called a “gram”. Creating a vocabulary of two-word pairs is, in turn, called a bigram model. \n","Again, only the bigrams that appear in the corpus are modeled, not all possible bigrams.\n","\n","An N-gram is an N-token sequence of words: a 2-gram (more commonly called a bigram) is a two-word sequence of words like “please turn”, “turn your”, or “your homework”, and a 3-gram (more commonly called a trigram) is a three-word sequence of words like “please turn your”, or “turn your homework”.\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"cLpAMrRpYAur"},"source":["For example, the bigrams in the first line of text in the previous section: “It was the best of times” are as follows:\n","\n","“it was”\n","“was the”\n","“the best”\n","“best of”\n","“of times”\n","\n","\n","*  A vocabulary then tracks triplets of words is called a trigram model and the general approach is called the n-gram model, where n refers to the number of grouped words.\n","\n","\n","*   Often a simple bigram approach is better than a 1-gram bag-of-words model for tasks like documentation classification.\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"2BJ51QrsYVdM"},"source":["**Scoring Words**\n","Once a vocabulary has been chosen, the occurrence of words in example documents needs to be scored.\n","\n","In the worked example, we have already seen one very simple approach to scoring: a binary scoring of the presence or absence of words.\n","\n","**Some additional simple scoring methods include:**\n","\n","\n","*   Counts. Count the number of times each word appears in a document.\n","*   Frequencies. Calculate the frequency that each word appears in a document out of all the words in the document.\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"X091vYvMZXff"},"source":["**Word Hashing**\n","a hash function is a bit of math that maps data to a fixed size set of numbers.\n","hash tables when programming where perhaps names are converted to numbers for fast lookup.\n","\n","\n","*   hash representation of known words in our vocabulary. This addresses the problem of having a very large vocabulary for a large text corpus because we can choose the size of the hash space, which is in turn the size of the vector representation of the document.\n","\n","\n","*  Words are hashed deterministically to the same integer index in the target hash space. A binary score or count can then be used to score the word.\n","\n","\n","*   This is called the “hash trick” or “feature hashing“.\n","\n","\n","*   The challenge is to choose a hash space to accommodate the chosen vocabulary size to minimize the probability of collisions and trade-off sparsity.\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"wXOlazgBbyR1"},"source":["**TF-IDF**\n","A problem with scoring word frequency is that highly frequent words start to dominate in the document (e.g. larger score), but may not contain as much “informational content” to the model as rarer but perhaps domain specific words.\n","\n","\n","\n","*   One approach is to rescale the frequency of words by how often **they appear in all documents,** so that the scores for frequent words like “the” that are also frequent across all documents are penalized.\n","\n","\n","*  This approach to scoring is called Term Frequency – Inverse Document Frequency, or TF-IDF for short, where:\n","\n","\n","1.  **Term Frequency**: is a scoring of the frequency of the word in the current document.\n","2.  **Inverse Document Frequency**: is a scoring of how rare the word is across documents.\n","\n","\n","The scores are a weighting where not all words are equally as important or interesting.\n","\n","\n","The scores have the effect of highlighting words that are distinct (contain useful information) in a given document. **Thus the idf of a rare term is high, whereas the idf of a frequent term is likely to be low.**\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"nZpCOuoqd66A"},"source":["**Limitations of Bag-of-Words**\n","\n","\n","1.   **Vocabulary**: The vocabulary requires careful design, most specifically in order to manage the size, which impacts the sparsity of the document representations.\n","2.   **Sparsity**: Sparse representations are harder to model both for computational reasons (space and time complexity) and also for information reasons, where the challenge is for the models to harness so little information in such a large representational space.\n","\n","\n","1.   **Meaning**: Discarding word order ignores the context, and in turn meaning of words in the document (semantics). Context and meaning can offer a lot to the model, that if modeled could tell the difference between the same words differently arranged (“this is interesting” vs “is this interesting”), synonyms (“old bike” vs “used bike”), and much more.\n","\n","2.\n","\n","\n","\n"]}]}