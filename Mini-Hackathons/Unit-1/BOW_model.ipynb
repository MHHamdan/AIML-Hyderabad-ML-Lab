{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"BOW_model.ipynb","provenance":[],"authorship_tag":"ABX9TyOvYB+jAC+VTLHttF0wmoPv"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"yXBJ-lwffFXd"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZRWxH4jpfNm_"},"source":["To perfrom document classification: each document is an “input” and a class label is the “output” for our predictive algorithm.\n","\n","\n","*  Algorithms take vectors of numbers as input, therefore we need to convert documents to fixed-length vectors of numbers.\n","*  A simple and effective model for thinking about text documents in machine learning is called the Bag-of-Words Model, or BoW. Because  it throws away all of the order information in the words and focuses on the occurrence of words in a document.\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"G0vaglOxhFBC"},"source":["This can be done by assigning each word a unique number. Then any document we see can be encoded as a fixed-length vector with the length of the vocabulary of known words. The value in each position in the vector could be filled with a count or frequency of each word in the encoded document.\n","\n","This is the bag of words model, where we are only concerned with encoding schemes that represent what words are present or the degree to which they are present in encoded documents without any information about order.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"aeWaHYZehtzw"},"source":["**Word Counts with CountVectorizer**\n","\n","The CountVectorizer provides a simple way to both **tokenize** a collection of text documents and **build a vocabulary** of known words, but also to encode new documents using that vocabulary.\n","\n","\n","1.  Create an instance of the CountVectorizer class.\n","2.   Call the fit() function in order to learn a vocabulary from one or more documents.\n","1.   Call the transform() function on one or more documents as needed to encode each as a vector.\n","**An encoded vector is returned with a length of the entire vocabulary and an integer count for the number of times each word appeared in the document.**\n","\n","Because these vectors will contain a lot of zeros, we call them sparse. Python provides an efficient way of handling sparse vectors in the scipy.sparse package.\n","\n","The vectors returned from a call to transform() will be sparse vectors, and you can transform them back to numpy arrays to look and better understand what is going on by calling the toarray() function.\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"o9vKRG_pjqxm","executionInfo":{"status":"ok","timestamp":1601361407527,"user_tz":240,"elapsed":1070,"user":{"displayName":"Mohammed Hamdan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh7-0C4n1c_uDVPoywaYa91Jx17qP1YRlmJadqc=s64","userId":"00647759825092258022"}},"outputId":"31a49c5f-1ce0-4ecc-f3fa-fd98f3e7aa40","colab":{"base_uri":"https://localhost:8080/","height":87}},"source":["\n","from sklearn.feature_extraction.text import CountVectorizer\n","# list of text documents\n","text = [\"The quick brown fox jumped over the lazy dog.\"]\n","# create the transform\n","vectorizer = CountVectorizer()\n","# tokenize and build vocab\n","vectorizer.fit(text)\n","# summarize to see what exactly was tokenized \n","print(vectorizer.vocabulary_)\n","# encode document\n","vector = vectorizer.transform(text)\n","# summarize encoded vector\n","print(vector.shape)\n","print(type(vector))\n","print(vector.toarray())"],"execution_count":1,"outputs":[{"output_type":"stream","text":["{'the': 7, 'quick': 6, 'brown': 0, 'fox': 2, 'jumped': 3, 'over': 5, 'lazy': 4, 'dog': 1}\n","(1, 8)\n","<class 'scipy.sparse.csr.csr_matrix'>\n","[[1 1 1 1 1 1 1 2]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"oEV11hxirIdW"},"source":["\n","\n","\n","the same vectorizer can be used on documents that contain words not included in the vocabulary. These words are ignored and no count is given in the resulting vector.\n","**The encoded vectors can then be used directly with a machine learning algorithm.**\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"YhHUa6dFrHeG","executionInfo":{"status":"ok","timestamp":1601363084245,"user_tz":240,"elapsed":635,"user":{"displayName":"Mohammed Hamdan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh7-0C4n1c_uDVPoywaYa91Jx17qP1YRlmJadqc=s64","userId":"00647759825092258022"}},"outputId":"8402dfc9-ba7a-4fc5-94f9-54a184b232cb","colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["\n","# encode another document\n","text2 = [\"the puppy\"]\n","vector = vectorizer.transform(text2)\n","print(vector.toarray())\n"],"execution_count":2,"outputs":[{"output_type":"stream","text":["[[0 0 0 0 0 0 0 1]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"t3Ti2D_0rxGT"},"source":["**Word Frequencies with TfidfVectorizer**\n","\n","One issue with simple counts is that some words like “the” will appear many times and their large counts will not be very meaningful in the encoded vectors."]},{"cell_type":"markdown","metadata":{"id":"TBNOfZYys3Xw"},"source":["An alternative is to calculate word frequencies, and by far the most popular method is called TF-IDF. This is an acronym than stands for “**Term Frequency – Inverse Document” Frequency **which are the components of the resulting scores assigned to each word."]},{"cell_type":"markdown","metadata":{"id":"zJ-wVWjAtHaQ"},"source":["**Term Frequency:** This summarizes how often a given word appears within a document.\n","\n","**Inverse Document Frequency:** This downscales words that appear a lot across documents.\n","\n","Without going into the math, TF-IDF are word frequency scores that try to highlight words that are more interesting, e.g. frequent in a document but not across documents.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"3fhLpJy6ticE"},"source":["The TfidfVectorizer will tokenize documents, learn the vocabulary and inverse document frequency weightings, and allow you to encode new documents. Alternately, if you already have a learned CountVectorizer, you can use it with a TfidfTransformer to just calculate the inverse document frequencies and start encoding documents.\n","The same create, fit, and transform process is used as with the CountVectorizer.\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"tFSfQqnPu0li","executionInfo":{"status":"ok","timestamp":1601364061256,"user_tz":240,"elapsed":811,"user":{"displayName":"Mohammed Hamdan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh7-0C4n1c_uDVPoywaYa91Jx17qP1YRlmJadqc=s64","userId":"00647759825092258022"}},"outputId":"db53f81a-d61c-4b05-d151-9144dfb470c7","colab":{"base_uri":"https://localhost:8080/","height":121}},"source":["\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","# list of text documents\n","text = [\"The quick brown fox jumped over the lazy dog.\",\n","\t\t\"The dog.\",\n","\t\t\"The fox\"]\n","# create the transform\n","vectorizer = TfidfVectorizer()\n","# tokenize and build vocab\n","vectorizer.fit(text)\n","# summarize\n","print(vectorizer.vocabulary_)\n","print(vectorizer.idf_)\n","# encode document\n","vector = vectorizer.transform([text[0]])\n","# summarize encoded vector\n","print(vector.shape)\n","print(vector.toarray())"],"execution_count":3,"outputs":[{"output_type":"stream","text":["{'the': 7, 'quick': 6, 'brown': 0, 'fox': 2, 'jumped': 3, 'over': 5, 'lazy': 4, 'dog': 1}\n","[1.69314718 1.28768207 1.28768207 1.69314718 1.69314718 1.69314718\n"," 1.69314718 1.        ]\n","(1, 8)\n","[[0.36388646 0.27674503 0.27674503 0.36388646 0.36388646 0.36388646\n","  0.36388646 0.42983441]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"kFYZ000yvPgO"},"source":["Above code: A vocabulary of 8 words is learned from the documents and each word is assigned a unique integer index in the output vector.\n","\n","The inverse document frequencies are calculated for each word in the vocabulary,** assigning the lowest score of 1.0 to the most frequently observed word: “the” at index 7.**\n","\n","The scores are normalized to values between 0 and 1 and the encoded document vectors can then be used directly with most machine learning algorithms.\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"W9AOCPVFxG66"},"source":["**Hashing with HashingVectorizer**\n","\n","Counts and frequencies can be very useful, but one limitation of these methods is that the vocabulary can become very large. \n","\n","This, in turn, will require large vectors for encoding documents and impose large requirements on memory and slow down algorithms.\n","\n","Therefore use the following: \n","\n","\n","\n","1.  use a one way hash of words to convert them to integers.\n","2.   no vocabulary is required and you can choose an arbitrary-long fixed length vector.\n","\n","A downside is that the hash is a one-way function so there is no way to convert the encoding back to a word (which may not matter for many supervised learning tasks).\n","\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Emfv4it-yIpi"},"source":["The HashingVectorizer class implements this approach that can be used to consistently hash words, then tokenize and encode documents as needed.\n","\n"]},{"cell_type":"code","metadata":{"id":"G72DZRcOvOsu","executionInfo":{"status":"ok","timestamp":1601365984311,"user_tz":240,"elapsed":662,"user":{"displayName":"Mohammed Hamdan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh7-0C4n1c_uDVPoywaYa91Jx17qP1YRlmJadqc=s64","userId":"00647759825092258022"}},"outputId":"9d948a67-af46-451d-ee39-c3bdde077576","colab":{"base_uri":"https://localhost:8080/","height":104}},"source":["\n","from sklearn.feature_extraction.text import HashingVectorizer\n","# list of text documents\n","text = [\"The quick brown fox jumped over the lazy dog.\"]\n","# create the transform\n","# encodes the sample document as a 20-element sparse array.\n","vectorizer = HashingVectorizer(n_features=20) # An arbitrary fixed-length vector size of 20 \n","#This corresponds to the range of the hash function, where small values (like 20) may result in hash collisions. \n","#heuristics that you can use to pick the hash length and probability of collision based on estimated vocabulary size.\n","\n","# encode document\n","vector = vectorizer.transform(text)\n","# summarize encoded vector\n","print(vector.shape)\n","print(vector.toarray())"],"execution_count":9,"outputs":[{"output_type":"stream","text":["(1, 20)\n","[[ 0.          0.          0.          0.          0.          0.33333333\n","   0.         -0.33333333  0.33333333  0.          0.          0.33333333\n","   0.          0.          0.         -0.33333333  0.          0.\n","  -0.66666667  0.        ]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"9NSExRtLCpsi"},"source":[""]},{"cell_type":"code","metadata":{"id":"sM8_BlCXCqTI"},"source":[""],"execution_count":null,"outputs":[]}]}