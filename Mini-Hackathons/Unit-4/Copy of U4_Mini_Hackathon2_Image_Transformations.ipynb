{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of U4_Mini_Hackathon2_Image_Transformations.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"hVMoZwHL4RTh"},"source":["# Advanced Certification in AIML\n","## A Program by IIIT-H and TalentSprint"]},{"cell_type":"markdown","metadata":{"id":"4EX7dr584we6"},"source":["## Problem Statement"]},{"cell_type":"markdown","metadata":{"id":"MjsV_asO4yL_"},"source":["Prepare a model that is invariant to the transformations and can recognize the varied images."]},{"cell_type":"markdown","metadata":{"id":"JwfwDwfu6MXx"},"source":["## Learning Objectives\n","\n","At the end of the experiment, you will be able to :\n","\n","* Load and prepare images for the model using Pytorch\n","* Load and Finetune a pre-trained model for predicting the labels of the transformed images"]},{"cell_type":"markdown","metadata":{"id":"YnkmRa9Uozvd"},"source":["## Dataset\n","\n","The dataset is comprised of photos of buildings, forests, glaciers, mountains, sea, and streets provided as a subset of photos.\n","\n","The dataset has been divided into folders for training, validation, and testing. The training folder includes around 14,000 images and the validation folder has around 3,000 images. Finally, the  testing folder includes around 10 images."]},{"cell_type":"markdown","metadata":{"id":"gsyDey3To9hE"},"source":["## Grading = 30 Marks"]},{"cell_type":"markdown","metadata":{"id":"0vt5qAcHpBPO"},"source":["## Setup Steps"]},{"cell_type":"code","metadata":{"id":"brp71K6W-_2v","cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1612226799801,"user_tz":300,"elapsed":26782,"user":{"displayName":"Mohammed Hamdan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh7-0C4n1c_uDVPoywaYa91Jx17qP1YRlmJadqc=s64","userId":"00647759825092258022"}},"outputId":"6b05aff8-2ca9-4594-b15e-5829bd29bb42"},"source":["#@title Run this cell to complete the setup for this Notebook\n","\n","from IPython import get_ipython\n","ipython = get_ipython()\n","  \n","notebook= \"U4_Mini_Hackathon2_Image_Transformations\" #name of the notebook\n","Answer = \"This notebook is graded by mentors on the day of mini-hackathon\"\n","def setup():\n","   ipython.magic(\"sx wget https://cdn.iiith.talentsprint.com/aiml/Experiment_related_data/Image_Transformations.zip\")\n","   ipython.magic(\"sx unzip -qq Image_Transformations.zip\")\n","   print (\"Setup completed successfully\")\n","   return\n","\n","setup()"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Setup completed successfully\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"brOxRK3w8DoJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1612226799802,"user_tz":300,"elapsed":25310,"user":{"displayName":"Mohammed Hamdan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh7-0C4n1c_uDVPoywaYa91Jx17qP1YRlmJadqc=s64","userId":"00647759825092258022"}},"outputId":"7375075c-b2ee-4bdf-ca1b-a1593fb0953b"},"source":["%ls"],"execution_count":2,"outputs":[{"output_type":"stream","text":["\u001b[0m\u001b[01;34mImage_Transformations\u001b[0m/  Image_Transformations.zip  \u001b[01;34msample_data\u001b[0m/\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"NLECiqpcdQTQ"},"source":["## Basic Pytorch packages\n","\n","**torchvision:**  This package is used to load and prepare the dataset. Using this package we can perform/apply transformations on the input data.\n","\n","**transforms:**  This package is  used to perform **preprocessing on images** and operations sequentially. \n","\n","**nn:**  This package provides an easy and modular way to build and train simple or complex neural networks.\n","\n","**optim:** This package is used for  implementing various optimization algorithms"]},{"cell_type":"code","metadata":{"id":"geupEQ4mNhO8","executionInfo":{"status":"ok","timestamp":1612226912144,"user_tz":300,"elapsed":330,"user":{"displayName":"Mohammed Hamdan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh7-0C4n1c_uDVPoywaYa91Jx17qP1YRlmJadqc=s64","userId":"00647759825092258022"}}},"source":["#pip install efficientnet-pytorch"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"MXq346qzqCdh","executionInfo":{"status":"ok","timestamp":1612226914920,"user_tz":300,"elapsed":315,"user":{"displayName":"Mohammed Hamdan","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gh7-0C4n1c_uDVPoywaYa91Jx17qP1YRlmJadqc=s64","userId":"00647759825092258022"}}},"source":["# Import Libraries\n","import time\n","import numpy as np # linear algebra\n","import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n","\n","# Input data files are available in the \"../input/\" directory.\n","# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n","from sklearn import metrics\n","from torchvision import transforms\n","from torch.utils.data import DataLoader\n","from torch.utils.data.dataset import Dataset\n","import pandas as pd\n","import numpy as np\n","from torch.utils.data.sampler import SubsetRandomSampler\n","import cv2\n","import os\n","import torchvision\n","import shutil\n","from torch.autograd import Variable\n","import torch\n","from torch import nn\n","import torch.nn.functional as F\n","import torchvision.models as models\n","from torchsummary import summary\n","from efficientnet_pytorch import EfficientNet\n","import torch.optim as optim\n","from tqdm.autonotebook import tqdm"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"J6XWrer9It5R"},"source":["## **Stage 1:** Data Preprocessing"]},{"cell_type":"markdown","metadata":{"id":"fdh-XK6dRw2y"},"source":["### 3 Marks -> Load the training dataset"]},{"cell_type":"code","metadata":{"id":"OxFVpcd4F79T"},"source":["loader =  # YOUR CODE HERE for defining Transformation for an image"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ls6gI08XH2ak"},"source":["# YOUR CODE HERE for the DataLoader\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Rybt6jhAajgX"},"source":["## **Stage 2:** Load and Finetune a pre-trained model\n","\n","Load a pretrained model and finetune the appropriate layers\n"]},{"cell_type":"markdown","metadata":{"id":"rIkiCLiFRpoq"},"source":["\n","###  5 Marks -> Fine-tune the Model and declare the loss function and optimizer. \n","\n","[Hint for Finetuning Models](https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html)"]},{"cell_type":"markdown","metadata":{"id":"jOqy8MZBhDg2"},"source":["Initialize the device to the available runtime type"]},{"cell_type":"code","metadata":{"id":"9j8VCAHuhNiv"},"source":["device = # YOUR CODE HERE\n","print(device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qsCOPyfABc7E"},"source":["# YOUR CODE HERE for the loading and finetuning the Pre-trained model\n","# YOUR CODE HERE for declaring the loss function and optimizer   \n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AC56-s8ZgRsR"},"source":["### 5 Marks -> Train the Model to calculate the loss and accuracy for the dataset across each epoch.\n","\n","Iterate over batch-wise images in the train_loader and perform the following steps. \n","\n","1. First, zero out the gradients using zero_grad()\n","\n","2. Convert the inputs, labels to the device (runtime type: GPU or CPU)\n","\n","3. Pass the input to the model and get the output\n","\n","4. Calculate the loss by comparing output with actual labels using a Loss function\n","\n","5. Perform Backward pass using backward() to update the weights\n","\n","6. Optimize the weights at each epoch and get a high probability prediction using the torch.max()\n","\n","7. Calculate the accuracy of the training dataset using the predictions\n","\n","**Note:** Optimize the CNN model to get better accuracy.\n","\n","## Expected Accuracy > 90%"]},{"cell_type":"code","metadata":{"id":"2Ot89MxKavVy"},"source":["# YOUR CODE HERE \n","# Record loss and accuracy of the training dataset for each epoch"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XYLntvGEPlAz"},"source":["### 5 Marks -> Validate the Model using the validation data\n","\n","Iterate over batch-wise images in the validation_loader and perform the following steps. \n","\n","1. First, set the model in `eval()` mode\n","\n","2. Convert the inputs, labels to the device (runtime type: GPU or CPU)\n","\n","3. Pass the input to the model and get the output\n","\n","4. Get high probability prediction using the torch.max()\n","\n","5. Calculate the accuracy of the validation dataset using the predictions\n","\n","\n","## Expected Accuracy > 90%\n","\n","**Note:** Optimize the CNN model to get better accuracy\n","\n","[Hint for eval()](https://pytorch.org/docs/stable/generated/torch.nn.Module.html#torch.nn.Module.eval)\n"]},{"cell_type":"code","metadata":{"id":"bwQY37IzPlA2"},"source":["# YOUR CODE HERE \n","# Display the accuracy of the validation dataset"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"K5BJIQzgHa0k"},"source":["## **Stage 3:** Test your final architecture on variations of the Test data.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"FqA75CTUS0PU"},"source":["Variations of the Test data mean that you can consider any image transformation such as Brightness, rotation, flip, and so on, as shown in the below example. \n","\n","![alt text](https://cdn.iiith.talentsprint.com/aiml/Experiment_related_data/transformations.png)\n","\n","### 12 Marks-> Define 6 different types of image transformation (variations) and evaluate for all test samples of various classes available.\n","\n","1. Define **6 image transformations** (Eg: Brightness, rotate, flip, and so on). You can use either skimage, PIL, or anything which can give the image transformations.\n","\n","2. Iterate over all **<font size='4.5'>TEN </font>** different test samples available under the testing folder run the below steps\n","   *  Perform 6 different image transformations for the chosen test sample and plot the same.\n","   *  Convert the image type of the transformed images, if required.\n","   *  Call the image_loader function for every transformed image.\n","   *  Pass through the CNN model to predict the label for each transformed test sample.\n","   *  Ensure the transformed test sample gives the correct prediction as an appropriate class name (buildings, forest, glacier, mountain, sea, and street) and **visualize the same**\n","\n","## At least 4 transformed pictures should be predicted similar to the original base prediction\n","\n","[Hint for the image transformations using skimage](https://www.analyticsvidhya.com/blog/2019/09/9-powerful-tricks-for-working-image-data-skimage-python/)\n","\n","[Hint for the image transformations using PIL](https://machinelearningmastery.com/how-to-load-and-manipulate-images-for-deep-learning-in-python-with-pil-pillow/)"]},{"cell_type":"code","metadata":{"id":"YJFCzNRKkkPU"},"source":["# Import required libraries for image transformations"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"veG-mhyJIHmk"},"source":["def image_loader(image):\n","    image = loader(image).to(device)\n","    image = image.unsqueeze(0) # To pass single image through the model\n","    return image "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fPZCUolhaE2v"},"source":["# YOUR CODE HERE for defining 6 different image transformations (eg: Brightness, rotate, flip, and so on)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9G0WSTonVbWj"},"source":["# YOUR CODE HERE for predicting the labels of an image for the 6 image transformations"],"execution_count":null,"outputs":[]}]}